"""
äº¤äº’å¼ä¼˜åŒ–å™¨å¯¹æ¯”å¯è§†åŒ–
"""

import streamlit as st
import numpy as np
import plotly.graph_objects as go
from .base import compute_gradient, get_loss_function
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥commonæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from common.error_handler import safe_render, safe_compute
from common.quiz_system import QuizSystem, QuizTemplates
from common.performance import cache_data, PerformanceMonitor


class InteractiveOptimizer:
    """äº¤äº’å¼ä¼˜åŒ–å™¨å¯¹æ¯”"""
    
    @staticmethod
    @safe_render
    def render():
        st.subheader("ğŸš€ äº¤äº’å¼ä¼˜åŒ–å™¨å¯¹æ¯”")
        st.markdown("""
        **å¸¸ç”¨ä¼˜åŒ–å™¨å¯¹æ¯”**:
        
        1. **SGD**: $\\theta_{t+1} = \\theta_t - \\eta g_t$
        
        2. **Momentum**: 
        $$v_{t+1} = \\beta v_t + g_t$$
        $$\\theta_{t+1} = \\theta_t - \\eta v_{t+1}$$
        
        3. **Adam** (è‡ªé€‚åº”å­¦ä¹ ç‡):
        $$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$
        $$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$
        $$\\theta_{t+1} = \\theta_t - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$
        
        å…¶ä¸­ $g_t = \\nabla_\\theta L(\\theta_t)$ ä¸ºæ¢¯åº¦
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ¯ æŸå¤±å‡½æ•°")
            loss_fn_name = st.selectbox("é€‰æ‹©å‡½æ•°", 
                ["rosenbrock", "rastrigin", "ackley", "beale"])
            
            st.markdown("### ğŸ› ï¸ ä¼˜åŒ–å™¨è®¾ç½®")
            optimizers = st.multiselect(
                "é€‰æ‹©ä¼˜åŒ–å™¨ï¼ˆå¯å¤šé€‰ï¼‰",
                ["SGD", "Momentum", "NAG", "AdaGrad", "RMSprop", "Adam"],
                default=["SGD", "Momentum", "Adam"]
            )
            
            st.markdown("### ğŸ“Š å‚æ•°è®¾ç½®")
            learning_rate = st.slider("å­¦ä¹ ç‡", 0.001, 0.1, 0.01, 0.001)
            iterations = st.slider("è¿­ä»£æ¬¡æ•°", 50, 500, 200, 50)
            
            col1, col2 = st.columns(2)
            with col1:
                start_x = st.number_input("èµ·å§‹ X", -3.0, 3.0, -2.0, 0.1)
            with col2:
                start_y = st.number_input("èµ·å§‹ Y", -3.0, 3.0, 2.0, 0.1)
        
        if not optimizers:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªä¼˜åŒ–å™¨")
            return
        
        # è·å–æŸå¤±å‡½æ•°
        loss_fn, x_range, y_range, title = get_loss_function(loss_fn_name)
        
        # è¿è¡Œæ‰€æœ‰ä¼˜åŒ–å™¨
        results = {}
        for opt_name in optimizers:
            path, loss_hist = InteractiveOptimizer._run_optimizer(
                opt_name, loss_fn, start_x, start_y, learning_rate, iterations
            )
            results[opt_name] = (path, loss_hist)
        
        # å¯è§†åŒ–å¯¹æ¯”
        st.markdown("### ğŸ“ˆ ä¼˜åŒ–è·¯å¾„å¯¹æ¯”")
        
        # 3Då¯è§†åŒ–
        fig_3d = InteractiveOptimizer._create_3d_comparison(
            loss_fn, results, x_range, y_range, title
        )
        st.plotly_chart(fig_3d, use_container_width=True)
        
        # æ”¶æ•›æ›²çº¿å¯¹æ¯”
        st.markdown("### ğŸ“‰ æ”¶æ•›é€Ÿåº¦å¯¹æ¯”")
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig_conv = InteractiveOptimizer._create_convergence_plot(results)
            st.plotly_chart(fig_conv, use_container_width=True)
        
        with col2:
            st.markdown("#### æœ€ç»ˆç»“æœ")
            for opt_name, (path, loss_hist) in results.items():
                final_loss = loss_hist[-1]
                initial_loss = loss_hist[0]
                improvement = (1 - final_loss/initial_loss) * 100
                st.metric(
                    opt_name,
                    f"{final_loss:.4f}",
                    f"{improvement:.1f}% â†“",
                    delta_color="inverse"
                )
    

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        quiz_system = QuizSystem("optimizer")
        quizzes = QuizTemplates.get_optimizer_quizzes()
        quiz_system.render_quiz(quizzes)
    @staticmethod
    def _run_optimizer(opt_name, loss_fn, start_x, start_y, lr, iterations):
        """è¿è¡ŒæŒ‡å®šçš„ä¼˜åŒ–å™¨"""
        path = [np.array([start_x, start_y])]
        loss_hist = [loss_fn(start_x, start_y)]
        current = np.array([start_x, start_y])
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ç‰¹å®šçš„çŠ¶æ€
        velocity = np.zeros(2)  # for Momentum, NAG
        cache = np.zeros(2)  # for AdaGrad, RMSprop
        m = np.zeros(2)  # for Adam
        v = np.zeros(2)  # for Adam
        
        beta1, beta2 = 0.9, 0.999  # Adamå‚æ•°
        epsilon = 1e-8
        
        for t in range(1, iterations + 1):
            grad = compute_gradient(loss_fn, current[0], current[1])
            
            if opt_name == "SGD":
                current = current - lr * grad
            
            elif opt_name == "Momentum":
                velocity = 0.9 * velocity - lr * grad
                current = current + velocity
            
            elif opt_name == "NAG":
                # Nesterov Accelerated Gradient
                look_ahead = current + 0.9 * velocity
                grad_ahead = compute_gradient(loss_fn, look_ahead[0], look_ahead[1])
                velocity = 0.9 * velocity - lr * grad_ahead
                current = current + velocity
            
            elif opt_name == "AdaGrad":
                cache += grad ** 2
                current = current - lr * grad / (np.sqrt(cache) + epsilon)
            
            elif opt_name == "RMSprop":
                cache = 0.9 * cache + 0.1 * grad ** 2
                current = current - lr * grad / (np.sqrt(cache) + epsilon)
            
            elif opt_name == "Adam":
                m = beta1 * m + (1 - beta1) * grad
                v = beta2 * v + (1 - beta2) * grad ** 2
                m_hat = m / (1 - beta1 ** t)
                v_hat = v / (1 - beta2 ** t)
                current = current - lr * m_hat / (np.sqrt(v_hat) + epsilon)
            
            path.append(current.copy())
            loss_hist.append(loss_fn(current[0], current[1]))
        
        return np.array(path), np.array(loss_hist)
    
    @staticmethod
    def _create_3d_comparison(loss_fn, results, x_range, y_range, title):
        """åˆ›å»º3Då¯¹æ¯”å›¾"""
        x = np.linspace(x_range[0], x_range[1], 80)
        y = np.linspace(y_range[0], y_range[1], 80)
        X, Y = np.meshgrid(x, y)
        Z = loss_fn(X, Y)
        Z = np.minimum(Z, np.percentile(Z, 95))
        
        fig = go.Figure()
        
        # æ›²é¢
        fig.add_trace(go.Surface(
            x=X, y=Y, z=Z,
            colorscale='Viridis',
            opacity=0.7,
            showscale=False,
            name='æŸå¤±å‡½æ•°'
        ))
        
        # å„ä¼˜åŒ–å™¨è·¯å¾„
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']
        for i, (opt_name, (path, _)) in enumerate(results.items()):
            path_z = [loss_fn(p[0], p[1]) for p in path]
            fig.add_trace(go.Scatter3d(
                x=path[:, 0], y=path[:, 1], z=path_z,
                mode='lines+markers',
                line=dict(color=colors[i % len(colors)], width=4),
                marker=dict(size=2),
                name=opt_name
            ))
        
        fig.update_layout(
            title=f"{title} - ä¼˜åŒ–å™¨å¯¹æ¯”",
            scene=dict(
                xaxis_title='wâ‚',
                yaxis_title='wâ‚‚',
                zaxis_title='Loss',
                camera=dict(eye=dict(x=1.5, y=1.5, z=1.2))
            ),
            height=600
        )
        
        return fig
    
    @staticmethod
    def _create_convergence_plot(results):
        """åˆ›å»ºæ”¶æ•›æ›²çº¿å¯¹æ¯”å›¾"""
        fig = go.Figure()
        
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']
        for i, (opt_name, (_, loss_hist)) in enumerate(results.items()):
            fig.add_trace(go.Scatter(
                x=list(range(len(loss_hist))),
                y=loss_hist,
                mode='lines',
                line=dict(color=colors[i % len(colors)], width=2),
                name=opt_name
            ))
        
        fig.update_layout(
            title="æŸå¤±å‡½æ•°æ”¶æ•›æ›²çº¿",
            xaxis_title="è¿­ä»£æ¬¡æ•°",
            yaxis_title="æŸå¤±å€¼",
            yaxis_type="log",
            height=400
        )
        
        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        
        return fig
