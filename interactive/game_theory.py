"""
äº¤äº’å¼åšå¼ˆè®ºå¯è§†åŒ–
ä¸¥æ ¼æŒ‰ç…§ 23.GameTheory.md ä¸­çš„ç†è®ºå®ç°

æ ¸å¿ƒå†…å®¹ï¼š
1. çº³ä»€å‡è¡¡åŸºç¡€
2. æå°æå¤§ä¼˜åŒ–ä¸æ—‹è½¬åŠ¨åŠ›å­¦
3. é›…å¯æ¯”çŸ©é˜µåˆ†æ
4. Stackelbergåšå¼ˆ
5. LOLAç®—æ³•
"""

import streamlit as st
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from scipy.linalg import eig


import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥commonæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from common.error_handler import safe_render
from common.quiz_system import QuizSystem, QuizTemplates
from common.smart_cache import cache_medium, cache_heavy, cache_numpy_computation

class InteractiveGameTheory:
    """äº¤äº’å¼åšå¼ˆè®ºå¯è§†åŒ–"""
    
    @staticmethod
    @safe_render

    def render():
        st.subheader("ğŸ® åšå¼ˆè®ºï¼šä»é™æ€ä¼˜åŒ–åˆ°åŠ¨æ€å‡è¡¡")
        
        st.markdown("""
        **æ ¸å¿ƒæ€æƒ³**: å½“ä¼˜åŒ–ç›®æ ‡å–å†³äºå¯¹æ‰‹ç­–ç•¥æ—¶ï¼Œæå°å€¼ç‚¹å˜ä¸ºéç‚¹ï¼Œéœ€è¦åŠ¨åŠ›å­¦åˆ†æ
        
        **å…³é”®æ¦‚å¿µ**:
        - **çº³ä»€å‡è¡¡**: æ²¡æœ‰ç©å®¶æœ‰å•æ–¹é¢åç¦»çš„åŠ¨æœº
        - **æå°æå¤§**: $\\min_x \\max_y f(x,y)$
        - **é›…å¯æ¯”åˆ†æ**: ç‰¹å¾å€¼å†³å®šç³»ç»Ÿç¨³å®šæ€§
        - **åŠ¨åŠ›å­¦ä¿®æ­£**: ä»çº¯æ—‹è½¬åˆ°èºæ—‹æ”¶æ•›
        """)
        
        # ä¾§è¾¹æ é€‰æ‹©
        with st.sidebar:
            st.markdown("### ğŸ“Š é€‰æ‹©å¯è§†åŒ–")
            demo_type = st.selectbox(
                "æ¼”ç¤ºç±»å‹",
                [
                    "çº³ä»€å‡è¡¡åŸºç¡€",
                    "æå°æå¤§åŠ¨åŠ›å­¦",
                    "é›…å¯æ¯”çŸ©é˜µåˆ†æ",
                    "Stackelbergåšå¼ˆ",
                    "LOLAç®—æ³•"
                ]
            )
        
        # æ¸²æŸ“å¯¹åº”çš„å¯è§†åŒ–
        if demo_type == "çº³ä»€å‡è¡¡åŸºç¡€":
            InteractiveGameTheory._render_nash_equilibrium()
        elif demo_type == "æå°æå¤§åŠ¨åŠ›å­¦":
            InteractiveGameTheory._render_minmax_dynamics()
        elif demo_type == "é›…å¯æ¯”çŸ©é˜µåˆ†æ":
            InteractiveGameTheory._render_jacobian_analysis()
        elif demo_type == "Stackelbergåšå¼ˆ":
            InteractiveGameTheory._render_stackelberg()
        elif demo_type == "LOLAç®—æ³•":
            InteractiveGameTheory._render_lola()
    

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        quiz_system = QuizSystem("game_theory")
        quizzes = QuizTemplates.get_game_theory_quizzes()
        quiz_system.render_quiz(quizzes)
    @staticmethod
    def _render_nash_equilibrium():
        """çº³ä»€å‡è¡¡åŸºç¡€æ¼”ç¤º"""
        st.markdown("### ğŸ¯ çº³ä»€å‡è¡¡ï¼šä»ä¼˜åŒ–åˆ°åšå¼ˆ")
        
        st.markdown(r"""
        **å®šä¹‰**: ç­–ç•¥ç»„åˆ $(\theta_1^*, \theta_2^*)$ æ˜¯çº³ä»€å‡è¡¡ï¼Œå¦‚æœï¼š
        """)
        
        st.latex(r"""
        \begin{cases}
        \theta_1^* = \arg\min_{\theta_1} L_1(\theta_1, \theta_2^*) \\
        \theta_2^* = \arg\min_{\theta_2} L_2(\theta_1^*, \theta_2)
        \end{cases}
        """)
        
        st.markdown(r"""
        **å­˜åœ¨æ€§ (Brouwerä¸åŠ¨ç‚¹å®šç†)**: å¦‚æœç­–ç•¥ç©ºé—´æ˜¯ç´§è‡´å‡¸é›†ï¼Œæ”¶ç›Šå‡½æ•°è¿ç»­ï¼Œåˆ™çº³ä»€å‡è¡¡ä¸€å®šå­˜åœ¨ã€‚
        
        **ç›´è§‚ç†è§£**: å°±åƒæŠŠä¸€å¼ æ‰çš±çš„çº¸æ‰”å›æ¡Œä¸Šï¼Œæ€»æœ‰ä¸€ä¸ªç‚¹åœ¨å‚ç›´æ–¹å‘ä¸Šæ²¡æœ‰ä½ç§»ã€‚
        """)
        
        with st.sidebar:
            st.markdown("#### å‚æ•°è®¾ç½®")
            game_type = st.selectbox(
                "åšå¼ˆç±»å‹",
                ["å›šå¾’å›°å¢ƒ", "æ€§åˆ«å¤§æˆ˜", "çŒé¹¿åšå¼ˆ", "å‰ªåˆ€çŸ³å¤´å¸ƒ"]
            )
            show_analysis = st.checkbox("æ˜¾ç¤ºè¯¦ç»†åˆ†æ", value=True)
        
        # å®šä¹‰æ”¶ç›ŠçŸ©é˜µ
        payoff_data = InteractiveGameTheory._get_game_payoffs(game_type)
        payoff_p1 = payoff_data['p1']
        payoff_p2 = payoff_data['p2']
        strategies = payoff_data['strategies']
        nash_eq = payoff_data['nash_eq']
        description = payoff_data['description']
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                "ç©å®¶1æ”¶ç›ŠçŸ©é˜µ",
                "ç©å®¶2æ”¶ç›ŠçŸ©é˜µ",
                "æ”¶ç›Šå¯¹æ¯”",
                "çº³ä»€å‡è¡¡åˆ†æ"
            ),
            specs=[
                [{"type": "heatmap"}, {"type": "heatmap"}],
                [{"type": "bar"}, {"type": "scatter"}]
            ]
        )
        
        # ç©å®¶1æ”¶ç›ŠçŸ©é˜µ
        fig.add_trace(
            go.Heatmap(
                z=payoff_p1,
                x=strategies,
                y=strategies,
                colorscale='RdYlGn',
                text=payoff_p1,
                texttemplate='%{text}',
                textfont={"size": 14},
                showscale=False
            ),
            row=1, col=1
        )
        
        # ç©å®¶2æ”¶ç›ŠçŸ©é˜µ
        fig.add_trace(
            go.Heatmap(
                z=payoff_p2,
                x=strategies,
                y=strategies,
                colorscale='RdYlGn',
                text=payoff_p2,
                texttemplate='%{text}',
                textfont={"size": 14},
                showscale=False
            ),
            row=1, col=2
        )
        
        # æ”¶ç›Šå¯¹æ¯”æŸ±çŠ¶å›¾
        for i, s1 in enumerate(strategies):
            for j, s2 in enumerate(strategies):
                fig.add_trace(
                    go.Bar(
                        x=[f"{s1},{s2}"],
                        y=[payoff_p1[i, j]],
                        name=f"P1",
                        marker_color='blue',
                        showlegend=(i == 0 and j == 0)
                    ),
                    row=2, col=1
                )
                fig.add_trace(
                    go.Bar(
                        x=[f"{s1},{s2}"],
                        y=[payoff_p2[i, j]],
                        name=f"P2",
                        marker_color='red',
                        showlegend=(i == 0 and j == 0)
                    ),
                    row=2, col=1
                )
        
        # çº³ä»€å‡è¡¡æ ‡æ³¨
        nash_labels = []
        nash_x = []
        nash_y = []
        for eq in nash_eq:
            i, j = eq
            nash_labels.append(f"NE: ({strategies[i]}, {strategies[j]})")
            nash_x.append(i)
            nash_y.append(j)
        
        fig.add_trace(
            go.Scatter(
                x=nash_x,
                y=nash_y,
                mode='markers+text',
                marker=dict(size=20, color='gold', symbol='star'),
                text=nash_labels,
                textposition='top center',
                name='çº³ä»€å‡è¡¡'
            ),
            row=2, col=2
        )
        
        # æ·»åŠ æ‰€æœ‰ç­–ç•¥ç‚¹
        for i in range(len(strategies)):
            for j in range(len(strategies)):
                is_nash = (i, j) in nash_eq
                fig.add_trace(
                    go.Scatter(
                        x=[i],
                        y=[j],
                        mode='markers',
                        marker=dict(
                            size=15 if is_nash else 10,
                            color='gold' if is_nash else 'lightblue',
                            symbol='star' if is_nash else 'circle'
                        ),
                        text=f"({strategies[i]}, {strategies[j]})",
                        hoverinfo='text',
                        showlegend=False
                    ),
                    row=2, col=2
                )
        
        fig.update_xaxes(title_text="ç©å®¶2ç­–ç•¥", row=1, col=1)
        fig.update_yaxes(title_text="ç©å®¶1ç­–ç•¥", row=1, col=1)
        fig.update_xaxes(title_text="ç©å®¶2ç­–ç•¥", row=1, col=2)
        fig.update_yaxes(title_text="ç©å®¶1ç­–ç•¥", row=1, col=2)
        fig.update_xaxes(title_text="ç­–ç•¥ç»„åˆ", row=2, col=1)
        fig.update_yaxes(title_text="æ”¶ç›Š", row=2, col=1)
        fig.update_xaxes(title_text="ç©å®¶2ç­–ç•¥ç´¢å¼•", row=2, col=2)
        fig.update_yaxes(title_text="ç©å®¶1ç­–ç•¥ç´¢å¼•", row=2, col=2)
        
        fig.update_layout(
            height=700,
            showlegend=True,
            title_text=f"{game_type} - çº³ä»€å‡è¡¡åˆ†æ"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # è¯¦ç»†åˆ†æ
        st.markdown("### ğŸ“Š åšå¼ˆåˆ†æ")
        st.info(description)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("çº³ä»€å‡è¡¡æ•°é‡", len(nash_eq))
        
        with col2:
            if nash_eq:
                avg_p1 = np.mean([payoff_p1[eq] for eq in nash_eq])
                st.metric("ç©å®¶1å‡è¡¡æ”¶ç›Š", f"{avg_p1:.2f}")
        
        with col3:
            if nash_eq:
                avg_p2 = np.mean([payoff_p2[eq] for eq in nash_eq])
                st.metric("ç©å®¶2å‡è¡¡æ”¶ç›Š", f"{avg_p2:.2f}")
        
        if show_analysis:
            st.markdown("### ğŸ” ä¼˜åŠ¿ç­–ç•¥åˆ†æ")
            
            # æ£€æŸ¥ç©å®¶1çš„ä¼˜åŠ¿ç­–ç•¥
            p1_dominant = InteractiveGameTheory._check_dominant_strategy(payoff_p1, axis=0)
            # æ£€æŸ¥ç©å®¶2çš„ä¼˜åŠ¿ç­–ç•¥
            p2_dominant = InteractiveGameTheory._check_dominant_strategy(payoff_p2.T, axis=0)
            
            if p1_dominant is not None:
                st.success(f"âœ… ç©å®¶1æœ‰ä¼˜åŠ¿ç­–ç•¥: **{strategies[p1_dominant]}**")
            else:
                st.warning("âš ï¸ ç©å®¶1æ²¡æœ‰ä¼˜åŠ¿ç­–ç•¥")
            
            if p2_dominant is not None:
                st.success(f"âœ… ç©å®¶2æœ‰ä¼˜åŠ¿ç­–ç•¥: **{strategies[p2_dominant]}**")
            else:
                st.warning("âš ï¸ ç©å®¶2æ²¡æœ‰ä¼˜åŠ¿ç­–ç•¥")
        
        st.success("""
        **çº³ä»€å‡è¡¡çš„æ ¸å¿ƒæ´å¯Ÿ**:
        - **ç¨³å®šæ€§**: æ²¡æœ‰ç©å®¶æœ‰åŠ¨æœºå•æ–¹é¢åç¦»
        - **éæ•ˆç‡æ€§**: çº³ä»€å‡è¡¡å¯èƒ½ä¸æ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜ï¼ˆå¦‚å›šå¾’å›°å¢ƒï¼‰
        - **å­˜åœ¨æ€§**: åœ¨æœ‰é™åšå¼ˆä¸­æ€»æ˜¯å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯æ··åˆç­–ç•¥ï¼‰
        - **å¤šé‡æ€§**: å¯èƒ½å­˜åœ¨å¤šä¸ªçº³ä»€å‡è¡¡
        """)
    
    @cache_numpy_computation(ttl=1800)
    @staticmethod
    def _get_game_payoffs(game_type):
        """è·å–ä¸åŒåšå¼ˆçš„æ”¶ç›ŠçŸ©é˜µ"""
        if game_type == "å›šå¾’å›°å¢ƒ":
            return {
                'p1': np.array([[-1, -3], [0, -2]]),
                'p2': np.array([[-1, 0], [-3, -2]]),
                'strategies': ['åˆä½œ', 'èƒŒå›'],
                'nash_eq': [(1, 1)],  # (èƒŒå›, èƒŒå›)
                'description': """
                **å›šå¾’å›°å¢ƒ**: ç»å…¸çš„éåˆä½œåšå¼ˆ
                - åŒæ–¹éƒ½é€‰æ‹©èƒŒå›æ˜¯çº³ä»€å‡è¡¡
                - ä½†åŒæ–¹åˆä½œçš„æ”¶ç›Šæ›´é«˜ï¼ˆå¸•ç´¯æ‰˜æœ€ä¼˜ï¼‰
                - è¯´æ˜ä¸ªä½“ç†æ€§å¯èƒ½å¯¼è‡´é›†ä½“éç†æ€§
                """
            }
        elif game_type == "æ€§åˆ«å¤§æˆ˜":
            return {
                'p1': np.array([[2, 0], [0, 1]]),
                'p2': np.array([[1, 0], [0, 2]]),
                'strategies': ['ç”µå½±', 'çƒèµ›'],
                'nash_eq': [(0, 0), (1, 1)],
                'description': """
                **æ€§åˆ«å¤§æˆ˜**: åè°ƒåšå¼ˆ
                - å­˜åœ¨ä¸¤ä¸ªçº¯ç­–ç•¥çº³ä»€å‡è¡¡
                - åŒæ–¹éƒ½æƒ³ä¸€èµ·æ´»åŠ¨ï¼Œä½†åå¥½ä¸åŒ
                - è¯´æ˜åè°ƒé—®é¢˜çš„å¤æ‚æ€§
                """
            }
        elif game_type == "çŒé¹¿åšå¼ˆ":
            return {
                'p1': np.array([[4, 1], [3, 2]]),
                'p2': np.array([[4, 3], [1, 2]]),
                'strategies': ['çŒé¹¿', 'çŒå…”'],
                'nash_eq': [(0, 0), (1, 1)],
                'description': """
                **çŒé¹¿åšå¼ˆ**: ä¿¡ä»»ä¸é£é™©
                - (çŒé¹¿, çŒé¹¿) æ”¶ç›Šé«˜ä½†éœ€è¦åˆä½œ
                - (çŒå…”, çŒå…”) æ”¶ç›Šä½ä½†å®‰å…¨
                - è¯´æ˜ä¿¡ä»»å»ºç«‹çš„å›°éš¾
                """
            }
        else:  # å‰ªåˆ€çŸ³å¤´å¸ƒ
            return {
                'p1': np.array([[0, -1, 1], [1, 0, -1], [-1, 1, 0]]),
                'p2': np.array([[0, 1, -1], [-1, 0, 1], [1, -1, 0]]),
                'strategies': ['çŸ³å¤´', 'å‰ªåˆ€', 'å¸ƒ'],
                'nash_eq': [],  # åªæœ‰æ··åˆç­–ç•¥å‡è¡¡
                'description': """
                **å‰ªåˆ€çŸ³å¤´å¸ƒ**: é›¶å’Œåšå¼ˆ
                - æ²¡æœ‰çº¯ç­–ç•¥çº³ä»€å‡è¡¡
                - å­˜åœ¨æ··åˆç­–ç•¥å‡è¡¡ï¼ˆå„1/3æ¦‚ç‡ï¼‰
                - è¯´æ˜å®Œå…¨ç«äº‰ä¸­çš„éšæœºåŒ–ç­–ç•¥
                """
            }
    
    @cache_numpy_computation(ttl=1800)
    @staticmethod
    def _check_dominant_strategy(payoff_matrix, axis=0):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¼˜åŠ¿ç­–ç•¥"""
        n = payoff_matrix.shape[axis]
        for i in range(n):
            is_dominant = True
            for j in range(n):
                if i == j:
                    continue
                # æ£€æŸ¥ç­–ç•¥iæ˜¯å¦ä¸¥æ ¼ä¼˜äºç­–ç•¥j
                if axis == 0:
                    if not np.all(payoff_matrix[i, :] >= payoff_matrix[j, :]):
                        is_dominant = False
                        break
                else:
                    if not np.all(payoff_matrix[:, i] >= payoff_matrix[:, j]):
                        is_dominant = False
                        break
            if is_dominant:
                return i
        return None
    
    @staticmethod
    def _render_minmax_dynamics():
        """æå°æå¤§åŠ¨åŠ›å­¦æ¼”ç¤º"""
        st.markdown("### ğŸŒ€ æå°æå¤§ä¼˜åŒ–ï¼šä»æ—‹è½¬åˆ°æ”¶æ•›")
        
        st.markdown(r"""
        **ç»å…¸é—®é¢˜**: $\min_x \max_y f(x,y) = xy$
        
        **æ¢¯åº¦ä¸‹é™-ä¸Šå‡ (GDA)**:
        """)
        
        st.latex(r"""
        \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix} = 
        \begin{bmatrix} -y \\ x \end{bmatrix} = 
        \underbrace{\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}}_{J} 
        \begin{bmatrix} x \\ y \end{bmatrix}
        """)
        
        st.markdown(r"""
        **é›…å¯æ¯”çŸ©é˜µ**: $J = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$
        
        **ç‰¹å¾å€¼**: $\lambda = \pm i$ (çº¯è™šæ•°ï¼Œçº¯æ—‹è½¬ï¼)
        """)
        
        with st.sidebar:
            st.markdown("#### å‚æ•°è®¾ç½®")
            dynamics_type = st.selectbox(
                "åŠ¨åŠ›å­¦ç±»å‹",
                ["æœ´ç´ GDA", "æ¢¯åº¦æƒ©ç½š", "è¾›ä¿®æ­£", "å¯¹æ¯”åˆ†æ"]
            )
            learning_rate = st.slider("å­¦ä¹ ç‡ Î·", 0.01, 0.3, 0.1, 0.01)
            lambda_reg = st.slider("æ­£åˆ™åŒ–å¼ºåº¦ Î»", 0.0, 1.0, 0.5, 0.05)
            initial_x = st.slider("åˆå§‹ x", -2.0, 2.0, 1.5, 0.1)
            initial_y = st.slider("åˆå§‹ y", -2.0, 2.0, 0.0, 0.1)
            n_steps = st.slider("è¿­ä»£æ­¥æ•°", 50, 500, 200, 50)
        
        # ç”Ÿæˆè½¨è¿¹
        if dynamics_type == "å¯¹æ¯”åˆ†æ":
            # å¯¹æ¯”å¤šç§æ–¹æ³•
            trajectories = {}
            for dtype in ["æœ´ç´ GDA", "æ¢¯åº¦æƒ©ç½š", "è¾›ä¿®æ­£"]:
                traj = InteractiveGameTheory._simulate_minmax(
                    initial_x, initial_y, learning_rate, lambda_reg, n_steps, dtype
                )
                trajectories[dtype] = traj
        else:
            # å•ä¸€æ–¹æ³•
            trajectory = InteractiveGameTheory._simulate_minmax(
                initial_x, initial_y, learning_rate, lambda_reg, n_steps, dynamics_type
            )
            trajectories = {dynamics_type: trajectory}
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("è½¨è¿¹æ¼”åŒ–", "å‘é‡åœº"),
            specs=[[{"type": "scatter"}, {"type": "scatter"}]]
        )
        
        colors = {'æœ´ç´ GDA': 'red', 'æ¢¯åº¦æƒ©ç½š': 'blue', 'è¾›ä¿®æ­£': 'green'}
        
        # ç»˜åˆ¶è½¨è¿¹
        for name, traj in trajectories.items():
            color = colors.get(name, 'purple')
            
            # è½¨è¿¹çº¿
            fig.add_trace(
                go.Scatter(
                    x=traj[:, 0],
                    y=traj[:, 1],
                    mode='lines+markers',
                    name=name,
                    line=dict(color=color, width=2),
                    marker=dict(size=3),
                    showlegend=True
                ),
                row=1, col=1
            )
            
            # èµ·ç‚¹
            fig.add_trace(
                go.Scatter(
                    x=[traj[0, 0]],
                    y=[traj[0, 1]],
                    mode='markers',
                    marker=dict(size=12, color=color, symbol='circle'),
                    name=f'{name} èµ·ç‚¹',
                    showlegend=False
                ),
                row=1, col=1
            )
            
            # ç»ˆç‚¹
            fig.add_trace(
                go.Scatter(
                    x=[traj[-1, 0]],
                    y=[traj[-1, 1]],
                    mode='markers',
                    marker=dict(size=12, color=color, symbol='square'),
                    name=f'{name} ç»ˆç‚¹',
                    showlegend=False
                ),
                row=1, col=1
            )
        
        # çº³ä»€å‡è¡¡ç‚¹
        fig.add_trace(
            go.Scatter(
                x=[0], y=[0],
                mode='markers',
                marker=dict(size=20, color='gold', symbol='star'),
                name='çº³ä»€å‡è¡¡',
                showlegend=True
            ),
            row=1, col=1
        )
        
        # å‘é‡åœº
        x_range = np.linspace(-2, 2, 20)
        y_range = np.linspace(-2, 2, 20)
        X, Y = np.meshgrid(x_range, y_range)
        
        # è®¡ç®—å‘é‡åœºï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨åŠ›å­¦ç±»å‹ï¼‰
        first_type = list(trajectories.keys())[0]
        U = np.zeros_like(X)
        V = np.zeros_like(Y)
        
        for i in range(len(x_range)):
            for j in range(len(y_range)):
                grad = InteractiveGameTheory._compute_gradient(
                    X[j, i], Y[j, i], lambda_reg, first_type
                )
                U[j, i] = grad[0]
                V[j, i] = grad[1]
        
        # ç»˜åˆ¶å‘é‡åœºï¼ˆä½¿ç”¨ç®­å¤´ï¼‰
        for i in range(0, len(x_range), 2):
            for j in range(0, len(y_range), 2):
                fig.add_annotation(
                    x=X[j, i] + U[j, i] * 0.1,
                    y=Y[j, i] + V[j, i] * 0.1,
                    ax=X[j, i],
                    ay=Y[j, i],
                    xref='x2', yref='y2',
                    axref='x2', ayref='y2',
                    showarrow=True,
                    arrowhead=2,
                    arrowsize=1,
                    arrowwidth=1,
                    arrowcolor='gray',
                    opacity=0.5
                )
        
        # åœ¨å‘é‡åœºå›¾ä¸Šä¹Ÿæ˜¾ç¤ºè½¨è¿¹
        for name, traj in trajectories.items():
            color = colors.get(name, 'purple')
            fig.add_trace(
                go.Scatter(
                    x=traj[:, 0],
                    y=traj[:, 1],
                    mode='lines',
                    line=dict(color=color, width=2),
                    showlegend=False
                ),
                row=1, col=2
            )
        
        fig.update_xaxes(title_text="x (Player 1)", range=[-2.5, 2.5], row=1, col=1)
        fig.update_yaxes(title_text="y (Player 2)", range=[-2.5, 2.5], row=1, col=1)
        fig.update_xaxes(title_text="x (Player 1)", range=[-2.5, 2.5], row=1, col=2)
        fig.update_yaxes(title_text="y (Player 2)", range=[-2.5, 2.5], row=1, col=2)
        
        fig.update_layout(
            height=500,
            showlegend=True,
            title_text=f"æå°æå¤§åŠ¨åŠ›å­¦ - {dynamics_type if dynamics_type != 'å¯¹æ¯”åˆ†æ' else 'å¤šæ–¹æ³•å¯¹æ¯”'}"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ç»Ÿè®¡åˆ†æ
        st.markdown("### ğŸ“Š è½¨è¿¹åˆ†æ")
        
        cols = st.columns(len(trajectories))
        for idx, (name, traj) in enumerate(trajectories.items()):
            with cols[idx]:
                # è®¡ç®—æ”¶æ•›æ€§æŒ‡æ ‡
                final_dist = np.linalg.norm(traj[-1])
                initial_dist = np.linalg.norm(traj[0])
                total_length = np.sum(np.linalg.norm(np.diff(traj, axis=0), axis=1))
                
                st.markdown(f"**{name}**")
                st.metric("æœ€ç»ˆè·ç¦»", f"{final_dist:.4f}")
                st.metric("è½¨è¿¹é•¿åº¦", f"{total_length:.2f}")
                
                if final_dist < 0.1:
                    st.success("âœ… æ”¶æ•›")
                elif final_dist < initial_dist:
                    st.info("ğŸ“‰ æ¥è¿‘ä¸­")
                else:
                    st.error("âŒ å‘æ•£/æ—‹è½¬")
        
        # ç†è®ºè§£é‡Š
        st.markdown("### ğŸ”¬ ç†è®ºåˆ†æ")
        
        st.info("""
        **æœ´ç´ GDAçš„é—®é¢˜**:
        - ç‰¹å¾å€¼ $\\lambda = \\pm i$ (çº¯è™šæ•°)
        - å¯¼è‡´çº¯æ—‹è½¬ï¼Œæ— æ³•æ”¶æ•›
        - èƒ½é‡å®ˆæ’: $\\frac{d}{dt}(x^2 + y^2) = 0$
        
        **ä¿®æ­£ç­–ç•¥**:
        1. **æ¢¯åº¦æƒ©ç½š**: æ·»åŠ  $-\\lambda x$ å’Œ $-\\lambda y$ é¡¹
        2. **è¾›ä¿®æ­£**: è°ƒæ•´æ›´æ–°æ–¹å‘ï¼Œå¼•å…¥"æ‘©æ“¦åŠ›"
        3. **ç›®æ ‡**: ä½¿ç‰¹å¾å€¼å…·æœ‰è´Ÿå®éƒ¨ $\\lambda = -\\alpha \\pm i\\beta$
        """)
        
        st.success("""
        **æ·±åº¦å­¦ä¹ å¯ç¤º**:
        - **GANè®­ç»ƒä¸ç¨³å®š**: æœ¬è´¨æ˜¯çº¯æ—‹è½¬åŠ¨åŠ›å­¦
        - **WGAN-GP**: æ¢¯åº¦æƒ©ç½šå¼•å…¥æ”¶æ•›é¡¹
        - **Spectral Normalization**: æ§åˆ¶é›…å¯æ¯”çŸ©é˜µçš„ç‰¹å¾å€¼
        - **å…³é”®**: å°†æ—‹è½¬åœºå˜ä¸ºæ”¶æ•›åœº
        """)
    
    @cache_numpy_computation(ttl=1800)
    @staticmethod
    def _simulate_minmax(x0, y0, eta, lambda_reg, n_steps, dynamics_type):
        """æ¨¡æ‹Ÿæå°æå¤§åŠ¨åŠ›å­¦"""
        trajectory = np.zeros((n_steps, 2))
        trajectory[0] = [x0, y0]
        
        for t in range(1, n_steps):
            x, y = trajectory[t-1]
            grad = InteractiveGameTheory._compute_gradient(x, y, lambda_reg, dynamics_type)
            trajectory[t] = trajectory[t-1] + eta * grad
        
        return trajectory
    
    @cache_heavy
    @staticmethod
    def _compute_gradient(x, y, lambda_reg, dynamics_type):
        """è®¡ç®—ä¸åŒåŠ¨åŠ›å­¦ä¸‹çš„æ¢¯åº¦"""
        if dynamics_type == "æœ´ç´ GDA":
            # çº¯æ—‹è½¬: dx/dt = -y, dy/dt = x
            return np.array([-y, x])
        
        elif dynamics_type == "æ¢¯åº¦æƒ©ç½š":
            # æ·»åŠ æ­£åˆ™åŒ–é¡¹: dx/dt = -y - Î»x, dy/dt = x - Î»y
            return np.array([-y - lambda_reg * x, x - lambda_reg * y])
        
        elif dynamics_type == "è¾›ä¿®æ­£":
            # è¾›ä¿®æ­£: è°ƒæ•´æ¢¯åº¦æ–¹å‘
            return np.array([-y - lambda_reg * x, x - lambda_reg * y])
        
        else:
            return np.array([-y, x])
    
    @staticmethod
    def _render_jacobian_analysis():
        """é›…å¯æ¯”çŸ©é˜µåˆ†æ"""
        st.markdown("### ğŸ” é›…å¯æ¯”çŸ©é˜µï¼šç³»ç»Ÿç¨³å®šæ€§çš„å…³é”®")
        
        st.markdown(r"""
        **æ ¸å¿ƒæ€æƒ³**: é›…å¯æ¯”çŸ©é˜µçš„ç‰¹å¾å€¼å†³å®šç³»ç»Ÿçš„ç¨³å®šæ€§
        
        **ç‰¹å¾å€¼åˆ†ç±»**:
        - **å®éƒ¨ < 0**: ç¨³å®šæ”¶æ•›ï¼ˆæœ‰æ‘©æ“¦åŠ›ï¼‰
        - **å®éƒ¨ = 0**: ä¸´ç•ŒçŠ¶æ€ï¼ˆçº¯æ—‹è½¬ï¼‰
        - **å®éƒ¨ > 0**: ä¸ç¨³å®šå‘æ•£
        - **è™šéƒ¨ â‰  0**: å­˜åœ¨æ—‹è½¬/éœ‡è¡
        
        **æå°æå¤§é—®é¢˜**: $\min_x \max_y f(x,y) = xy$
        """)
        
        st.latex(r"""
        J = \begin{bmatrix} 
        \frac{\partial \dot{x}}{\partial x} & \frac{\partial \dot{x}}{\partial y} \\ 
        \frac{\partial \dot{y}}{\partial x} & \frac{\partial \dot{y}}{\partial y} 
        \end{bmatrix}
        """)
        
        st.markdown("")
        
        with st.sidebar:
            st.markdown("#### å‚æ•°è®¾ç½®")
            analysis_type = st.selectbox(
                "åˆ†æç±»å‹",
                ["æœ´ç´ GDA", "æ·»åŠ æ­£åˆ™åŒ–", "è‡ªå®šä¹‰çŸ©é˜µ"]
            )
            
            if analysis_type == "æ·»åŠ æ­£åˆ™åŒ–":
                reg_strength = st.slider("æ­£åˆ™åŒ–å¼ºåº¦", 0.0, 2.0, 0.5, 0.1)
            elif analysis_type == "è‡ªå®šä¹‰çŸ©é˜µ":
                st.markdown("é›…å¯æ¯”çŸ©é˜µå…ƒç´ ")
                j11 = st.slider("J[0,0]", -2.0, 2.0, 0.0, 0.1)
                j12 = st.slider("J[0,1]", -2.0, 2.0, -1.0, 0.1)
                j21 = st.slider("J[1,0]", -2.0, 2.0, 1.0, 0.1)
                j22 = st.slider("J[1,1]", -2.0, 2.0, 0.0, 0.1)
        
        # æ„é€ é›…å¯æ¯”çŸ©é˜µ
        if analysis_type == "æœ´ç´ GDA":
            J = np.array([[0, -1], [1, 0]])
            description = "æœ´ç´ GDA: çº¯æ—‹è½¬çŸ©é˜µ"
        elif analysis_type == "æ·»åŠ æ­£åˆ™åŒ–":
            J = np.array([[-reg_strength, -1], [1, -reg_strength]])
            description = f"æ·»åŠ æ­£åˆ™åŒ– (Î»={reg_strength})"
        else:
            J = np.array([[j11, j12], [j21, j22]])
            description = "è‡ªå®šä¹‰é›…å¯æ¯”çŸ©é˜µ"
        
        # è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        eigenvalues, eigenvectors = eig(J)
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                "é›…å¯æ¯”çŸ©é˜µ",
                "ç‰¹å¾å€¼åˆ†å¸ƒ",
                "ç›¸ç©ºé—´è½¨è¿¹",
                "ç¨³å®šæ€§åˆ†æ"
            ),
            specs=[
                [{"type": "heatmap"}, {"type": "scatter"}],
                [{"type": "scatter"}, {"type": "table"}]
            ]
        )
        
        # 1. é›…å¯æ¯”çŸ©é˜µçƒ­å›¾
        fig.add_trace(
            go.Heatmap(
                z=J,
                x=['x', 'y'],
                y=['áº‹', 'áº'],
                colorscale='RdBu',
                zmid=0,
                text=J,
                texttemplate='%{text:.2f}',
                textfont={"size": 16},
                showscale=True
            ),
            row=1, col=1
        )
        
        # 2. ç‰¹å¾å€¼åœ¨å¤å¹³é¢ä¸Šçš„åˆ†å¸ƒ
        for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):
            fig.add_trace(
                go.Scatter(
                    x=[np.real(val)],
                    y=[np.imag(val)],
                    mode='markers+text',
                    marker=dict(size=15, color='red' if np.real(val) > 0 else 'blue'),
                    text=[f'Î»{i+1}'],
                    textposition='top center',
                    name=f'ç‰¹å¾å€¼ {i+1}',
                    showlegend=True
                ),
                row=1, col=2
            )
        
        # æ·»åŠ è™šè½´
        fig.add_vline(x=0, line_dash="dash", line_color="gray", row=1, col=2)
        fig.add_hline(y=0, line_dash="dash", line_color="gray", row=1, col=2)
        
        # æ·»åŠ ç¨³å®šåŒºåŸŸç€è‰²
        fig.add_vrect(
            x0=-3, x1=0,
            fillcolor="green", opacity=0.1,
            annotation_text="ç¨³å®šåŒº", annotation_position="top left",
            row=1, col=2
        )
        fig.add_vrect(
            x0=0, x1=3,
            fillcolor="red", opacity=0.1,
            annotation_text="ä¸ç¨³å®šåŒº", annotation_position="top right",
            row=1, col=2
        )
        
        # 3. ç›¸ç©ºé—´è½¨è¿¹
        # æ¨¡æ‹Ÿå¤šæ¡è½¨è¿¹
        initial_points = [
            [1.5, 0.0], [0.0, 1.5], [-1.5, 0.0], [0.0, -1.5],
            [1.0, 1.0], [-1.0, 1.0], [1.0, -1.0], [-1.0, -1.0]
        ]
        
        for init_point in initial_points:
            # ç®€å•çš„æ¬§æ‹‰æ–¹æ³•æ¨¡æ‹Ÿ
            trajectory = [init_point]
            for _ in range(100):
                current = trajectory[-1]
                update = J @ current
                next_point = current + 0.05 * update
                trajectory.append(next_point)
                # é˜²æ­¢å‘æ•£åˆ°æ— ç©·
                if np.linalg.norm(next_point) > 5:
                    break
            
            trajectory = np.array(trajectory)
            fig.add_trace(
                go.Scatter(
                    x=trajectory[:, 0],
                    y=trajectory[:, 1],
                    mode='lines',
                    line=dict(width=1),
                    opacity=0.6,
                    showlegend=False
                ),
                row=2, col=1
            )
        
        # çº³ä»€å‡è¡¡ç‚¹
        fig.add_trace(
            go.Scatter(
                x=[0], y=[0],
                mode='markers',
                marker=dict(size=15, color='gold', symbol='star'),
                name='å‡è¡¡ç‚¹',
                showlegend=False
            ),
            row=2, col=1
        )
        
        # 4. ç¨³å®šæ€§åˆ†æè¡¨æ ¼
        stability_data = []
        for i, val in enumerate(eigenvalues):
            real_part = np.real(val)
            imag_part = np.imag(val)
            magnitude = np.abs(val)
            
            if real_part < -0.01:
                stability = "ç¨³å®š"
            elif real_part > 0.01:
                stability = "ä¸ç¨³å®š"
            else:
                stability = "ä¸´ç•Œ"
            
            if abs(imag_part) > 0.01:
                behavior = "æ—‹è½¬"
            else:
                behavior = "çº¯æŒ‡æ•°"
            
            stability_data.append([
                f"Î»{i+1}",
                f"{real_part:.3f}",
                f"{imag_part:.3f}",
                f"{magnitude:.3f}",
                behavior,
                stability
            ])
        
        fig.add_trace(
            go.Table(
                header=dict(
                    values=["ç‰¹å¾å€¼", "å®éƒ¨", "è™šéƒ¨", "æ¨¡", "è¡Œä¸º", "ç¨³å®šæ€§"],
                    fill_color='paleturquoise',
                    align='center'
                ),
                cells=dict(
                    values=list(zip(*stability_data)),
                    fill_color='lavender',
                    align='center'
                )
            ),
            row=2, col=2
        )
        
        fig.update_xaxes(title_text="âˆ‚/âˆ‚x", row=1, col=1)
        fig.update_yaxes(title_text="d/dt", row=1, col=1)
        fig.update_xaxes(title_text="å®éƒ¨ Re(Î»)", range=[-2, 2], row=1, col=2)
        fig.update_yaxes(title_text="è™šéƒ¨ Im(Î»)", range=[-2, 2], row=1, col=2)
        fig.update_xaxes(title_text="x", range=[-3, 3], row=2, col=1)
        fig.update_yaxes(title_text="y", range=[-3, 3], row=2, col=1)
        
        fig.update_layout(
            height=800,
            showlegend=True,
            title_text=f"é›…å¯æ¯”çŸ©é˜µåˆ†æ - {description}"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # è¯¦ç»†åˆ†æ
        st.markdown("### ğŸ“Š ç³»ç»Ÿè¯Šæ–­")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            max_real = max(np.real(eigenvalues))
            if max_real < -0.01:
                stability_status = "ç¨³å®š"
                color = "green"
            elif max_real > 0.01:
                stability_status = "ä¸ç¨³å®š"
                color = "red"
            else:
                stability_status = "ä¸´ç•Œ"
                color = "orange"
            st.metric("ç³»ç»Ÿç¨³å®šæ€§", stability_status)
        
        with col2:
            has_rotation = any(abs(np.imag(val)) > 0.01 for val in eigenvalues)
            st.metric("æ˜¯å¦æ—‹è½¬", "æ˜¯" if has_rotation else "å¦")
        
        with col3:
            max_magnitude = max(np.abs(eigenvalues))
            st.metric("æœ€å¤§ç‰¹å¾å€¼æ¨¡", f"{max_magnitude:.3f}")
        
        with col4:
            trace = np.trace(J)
            st.metric("è¿¹ Tr(J)", f"{trace:.3f}")
        
        # ç†è®ºè§£é‡Š
        st.markdown("### ğŸ“ ç†è®ºè¦ç‚¹")
        
        st.info(f"""
        **é›…å¯æ¯”çŸ©é˜µ**:
        ```
        J = {J}
        ```
        
        **è¡Œåˆ—å¼**: det(J) = {np.linalg.det(J):.3f}
        
        **è¿¹**: Tr(J) = {np.trace(J):.3f}
        
        **ç‰¹å¾æ–¹ç¨‹**: det(J - Î»I) = 0
        """)
        
        st.success("""
        **åˆ¤æ–­å‡†åˆ™**:
        
        1. **Tr(J) < 0**: ç³»ç»Ÿæœ‰æ”¶æ•›è¶‹åŠ¿
        2. **det(J) > 0**: ç‰¹å¾å€¼åŒå·æˆ–å…±è½­
        3. **Re(Î») < 0**: æ‰€æœ‰ç‰¹å¾å€¼å®éƒ¨ä¸ºè´Ÿ â†’ æ¸è¿‘ç¨³å®š
        4. **Re(Î») = 0**: ä¸´ç•Œç¨³å®šï¼ˆå¦‚çº¯GDAï¼‰
        5. **Re(Î») > 0**: è‡³å°‘ä¸€ä¸ªç‰¹å¾å€¼å®éƒ¨ä¸ºæ­£ â†’ ä¸ç¨³å®š
        
        **åº”ç”¨**:
        - **GANè®­ç»ƒ**: éœ€è¦ä½¿ Re(Î») < 0
        - **å¼ºåŒ–å­¦ä¹ **: ç­–ç•¥æ¢¯åº¦çš„ç¨³å®šæ€§åˆ†æ
        - **å¯¹æŠ—è®­ç»ƒ**: ç¡®ä¿æ”¶æ•›è€Œééœ‡è¡
        """)
    
    @staticmethod
    def _render_stackelberg():
        """Stackelbergåšå¼ˆæ¼”ç¤º"""
        st.markdown("### ğŸ‘‘ Stackelbergåšå¼ˆï¼šLeader-FolloweråŠ¨æ€")
        
        st.markdown(r"""
        **æ ¸å¿ƒæ¦‚å¿µ**: é¢†å¯¼è€…è€ƒè™‘è·Ÿéšè€…çš„æœ€ä¼˜ååº”
        
        **æ•°å­¦è¡¨è¾¾**:
        
        - Leaderä¼˜åŒ–: $\min_{\theta_1} U(\theta_1, \theta_2^*(\theta_1))$
        - Followerä¼˜åŒ–: $\theta_2^* = \arg\min_{\theta_2} L_{follower}(\theta_1, \theta_2)$
        
        **éšå‡½æ•°æ¢¯åº¦**:
        """)
        
        st.latex(r"""
        \frac{d\theta_2^*}{d\theta_1} = -[\nabla_{\theta_2\theta_2}^2 L]^{-1} \nabla_{\theta_1\theta_2}^2 L
        """)
        
        st.markdown(r"""
        **Total Gradient**:
        """)
        
        st.latex(r"""
        \nabla_{\theta_1}^{Total} = \frac{\partial U}{\partial \theta_1} + 
        \frac{\partial U}{\partial \theta_2} \cdot \frac{d\theta_2^*}{d\theta_1}
        """)
        
        st.markdown("")
        
        with st.sidebar:
            st.markdown("#### å‚æ•°è®¾ç½®")
            scenario = st.selectbox(
                "åšå¼ˆåœºæ™¯",
                ["å®šä»·ç«äº‰", "è´¨é‡ç«äº‰", "å¹¿å‘ŠæŠ•å…¥"]
            )
            leader_strategy = st.slider("Leaderåˆå§‹ç­–ç•¥", 0.0, 10.0, 5.0, 0.5)
            show_reaction = st.checkbox("æ˜¾ç¤ºååº”å‡½æ•°", value=True)
        
        # å®šä¹‰Stackelbergåšå¼ˆ
        if scenario == "å®šä»·ç«äº‰":
            # Leaderä»·æ ¼ï¼ŒFolloweræœ€ä¼˜ååº”
            def follower_best_response(p1):
                # å‡è®¾çº¿æ€§éœ€æ±‚: p2 = (10 - p1) / 2
                return (10 - p1) / 2
            
            def leader_profit(p1):
                p2 = follower_best_response(p1)
                # Leaderåˆ©æ¶¦: (p1 - c1) * q1, q1 = 10 - p1 - 0.5*p2
                c1 = 2  # Leaderæˆæœ¬
                q1 = 10 - p1 - 0.5 * p2
                return (p1 - c1) * q1
            
            x_label = "ä»·æ ¼"
            y_label = "æ•°é‡/åˆ©æ¶¦"
            
        elif scenario == "è´¨é‡ç«äº‰":
            def follower_best_response(q1):
                return 0.8 * q1  # Followerè´¨é‡ç•¥ä½
            
            def leader_profit(q1):
                q2 = follower_best_response(q1)
                # å‡è®¾åˆ©æ¶¦ä¸è´¨é‡å·®ç›¸å…³
                return q1 * (10 - q1) - 0.5 * (q1 - q2)**2
            
            x_label = "è´¨é‡"
            y_label = "åˆ©æ¶¦"
            
        else:  # å¹¿å‘ŠæŠ•å…¥
            def follower_best_response(a1):
                return 0.5 * np.sqrt(a1)  # è·Ÿéšè€…ç­–ç•¥
            
            def leader_profit(a1):
                a2 = follower_best_response(a1)
                # åˆ©æ¶¦: æ”¶ç›Š - æˆæœ¬
                return np.sqrt(a1) * 10 - a1 - 0.3 * a2**2
            
            x_label = "å¹¿å‘ŠæŠ•å…¥"
            y_label = "åˆ©æ¶¦"
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("ååº”å‡½æ•°", "Leaderåˆ©æ¶¦æœ€å¤§åŒ–"),
            specs=[[{"type": "scatter"}, {"type": "scatter"}]]
        )
        
        # 1. ååº”å‡½æ•°æ›²çº¿
        x_range = np.linspace(0.1, 10, 100)
        follower_responses = [follower_best_response(x) for x in x_range]
        
        fig.add_trace(
            go.Scatter(
                x=x_range,
                y=follower_responses,
                mode='lines',
                name='Followerååº”å‡½æ•°',
                line=dict(color='blue', width=3)
            ),
            row=1, col=1
        )
        
        # å½“å‰Leaderç­–ç•¥ç‚¹
        current_response = follower_best_response(leader_strategy)
        fig.add_trace(
            go.Scatter(
                x=[leader_strategy],
                y=[current_response],
                mode='markers',
                marker=dict(size=15, color='red', symbol='star'),
                name='å½“å‰ç­–ç•¥',
                showlegend=True
            ),
            row=1, col=1
        )
        
        # 2. Leaderåˆ©æ¶¦æ›²çº¿
        leader_profits = [leader_profit(x) for x in x_range]
        optimal_idx = np.argmax(leader_profits)
        optimal_strategy = x_range[optimal_idx]
        
        fig.add_trace(
            go.Scatter(
                x=x_range,
                y=leader_profits,
                mode='lines',
                name='Leaderåˆ©æ¶¦',
                line=dict(color='green', width=3)
            ),
            row=1, col=2
        )
        
        # æœ€ä¼˜ç‚¹
        fig.add_trace(
            go.Scatter(
                x=[optimal_strategy],
                y=[leader_profits[optimal_idx]],
                mode='markers',
                marker=dict(size=15, color='gold', symbol='star'),
                name='æœ€ä¼˜ç­–ç•¥',
                showlegend=True
            ),
            row=1, col=2
        )
        
        # å½“å‰ç‚¹
        fig.add_trace(
            go.Scatter(
                x=[leader_strategy],
                y=[leader_profit(leader_strategy)],
                mode='markers',
                marker=dict(size=12, color='red', symbol='circle'),
                name='å½“å‰åˆ©æ¶¦',
                showlegend=True
            ),
            row=1, col=2
        )
        
        fig.update_xaxes(title_text=f"Leader {x_label}", row=1, col=1)
        fig.update_yaxes(title_text=f"Follower {x_label}", row=1, col=1)
        fig.update_xaxes(title_text=f"Leader {x_label}", row=1, col=2)
        fig.update_yaxes(title_text=y_label, row=1, col=2)
        
        fig.update_layout(
            height=500,
            showlegend=True,
            title_text=f"Stackelbergåšå¼ˆ - {scenario}"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # åˆ†æ
        st.markdown("### ğŸ“Š åšå¼ˆåˆ†æ")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æœ€ä¼˜Leaderç­–ç•¥", f"{optimal_strategy:.2f}")
        
        with col2:
            optimal_follower = follower_best_response(optimal_strategy)
            st.metric("å¯¹åº”Followerç­–ç•¥", f"{optimal_follower:.2f}")
        
        with col3:
            max_profit = leader_profits[optimal_idx]
            st.metric("æœ€å¤§Leaderåˆ©æ¶¦", f"{max_profit:.2f}")
        
        with col4:
            current_profit = leader_profit(leader_strategy)
            improvement = max_profit - current_profit
            st.metric("å¯æ”¹è¿›ç©ºé—´", f"{improvement:.2f}")
        
        st.success("""
        **Stackelbergåšå¼ˆçš„å…³é”®**:
        
        1. **å…ˆåŠ¨ä¼˜åŠ¿**: Leaderé€šè¿‡æ‰¿è¯ºè·å¾—ä¼˜åŠ¿
        2. **éšå‡½æ•°æ¢¯åº¦**: è€ƒè™‘Followerçš„ååº”
        3. **è®¡ç®—å¤æ‚åº¦**: éœ€è¦è®¡ç®—Hessiané€†ï¼Œ$O(n^3)$
        4. **å·¥ç¨‹è¿‘ä¼¼**: ä½¿ç”¨Neumannçº§æ•°æˆ–å…±è½­æ¢¯åº¦
        
        **åº”ç”¨åœºæ™¯**:
        - **å…ƒå­¦ä¹  (MAML)**: Outer loopæ˜¯Leader
        - **ç¥ç»æ¶æ„æœç´¢ (DARTS)**: æ¶æ„å‚æ•°æ˜¯Leader
        - **å¯¹æŠ—è®­ç»ƒ**: é˜²å¾¡è€…æ˜¯Leader
        """)
    
    @staticmethod
    def _render_lola():
        """LOLAç®—æ³•æ¼”ç¤º"""
        st.markdown("### ğŸ¤ LOLAï¼šå¯¹æ‰‹æ„ŸçŸ¥å­¦ä¹ ")
        
        st.markdown(r"""
        **Learning with Opponent-Learning Awareness (LOLA)**
        
        **æ ¸å¿ƒæ€æƒ³**: æ™ºèƒ½ä½“ä¸ä»…ä¼˜åŒ–è‡ªå·±çš„æ”¶ç›Šï¼Œè¿˜è¦è€ƒè™‘å¯¹æ‰‹çš„å­¦ä¹ è¿‡ç¨‹
        
        **æ ‡å‡†æ›´æ–°** (æœ´ç´ å­¦ä¹ ):
        """)
        
        st.latex(r"""
        \theta_1 \leftarrow \theta_1 - \eta \nabla_{\theta_1} L_1(\theta_1, \theta_2)
        """)
        
        st.markdown(r"""
        **LOLAæ›´æ–°** (å¯¹æ‰‹æ„ŸçŸ¥):
        """)
        
        st.latex(r"""
        \theta_1 \leftarrow \theta_1 - \eta \left[\nabla_{\theta_1} L_1 + 
        \nabla_{\theta_2} L_1 \cdot \frac{d\theta_2}{d\theta_1}\right]
        """)
        
        st.markdown(r"""
        **æ•ˆæœ**: åœ¨é‡å¤å›šå¾’å›°å¢ƒä¸­è‡ªå‘æ¶Œç°åˆä½œè¡Œä¸ºï¼
        """)
        
        with st.sidebar:
            st.markdown("#### å‚æ•°è®¾ç½®")
            algorithm = st.selectbox(
                "å­¦ä¹ ç®—æ³•",
                ["LOLA", "æ ‡å‡†æ¢¯åº¦", "å¯¹æ¯”åˆ†æ"]
            )
            learning_rate = st.slider("å­¦ä¹ ç‡", 0.01, 0.5, 0.1, 0.01)
            n_episodes = st.slider("è®­ç»ƒè½®æ•°", 50, 500, 200, 50)
            game_scenario = st.selectbox(
                "åšå¼ˆåœºæ™¯",
                ["å›šå¾’å›°å¢ƒ", "åè°ƒåšå¼ˆ", "æ··åˆç­–ç•¥"]
            )
        
        # å®šä¹‰æ”¶ç›ŠçŸ©é˜µ
        if game_scenario == "å›šå¾’å›°å¢ƒ":
            payoff_matrix = {
                'p1': np.array([[-1, -3], [0, -2]]),
                'p2': np.array([[-1, 0], [-3, -2]])
            }
            optimal_action = "åˆä½œ"
        elif game_scenario == "åè°ƒåšå¼ˆ":
            payoff_matrix = {
                'p1': np.array([[2, 0], [0, 1]]),
                'p2': np.array([[2, 0], [0, 1]])
            }
            optimal_action = "åè°ƒ"
        else:  # æ··åˆç­–ç•¥
            payoff_matrix = {
                'p1': np.array([[1, -1], [-1, 1]]),
                'p2': np.array([[-1, 1], [1, -1]])
            }
            optimal_action = "æ··åˆ"
        
        # è¿è¡Œæ¨¡æ‹Ÿ
        if algorithm == "å¯¹æ¯”åˆ†æ":
            results = {}
            for alg in ["LOLA", "æ ‡å‡†æ¢¯åº¦"]:
                results[alg] = InteractiveGameTheory._simulate_lola(
                    payoff_matrix, alg, learning_rate, n_episodes
                )
        else:
            results = {
                algorithm: InteractiveGameTheory._simulate_lola(
                    payoff_matrix, algorithm, learning_rate, n_episodes
                )
            }
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                "ç­–ç•¥æ¼”åŒ–",
                "ç´¯ç§¯æ”¶ç›Š",
                "åˆä½œç‡/åè°ƒç‡",
                "ç­–ç•¥ç©ºé—´è½¨è¿¹"
            ),
            specs=[
                [{"type": "scatter"}, {"type": "scatter"}],
                [{"type": "scatter"}, {"type": "scatter"}]
            ]
        )
        
        colors = {'LOLA': 'blue', 'æ ‡å‡†æ¢¯åº¦': 'red'}
        
        for alg_name, result in results.items():
            color = colors[alg_name]
            
            # 1. ç­–ç•¥æ¼”åŒ–ï¼ˆç©å®¶1é€‰æ‹©åˆä½œçš„æ¦‚ç‡ï¼‰
            fig.add_trace(
                go.Scatter(
                    x=list(range(n_episodes)),
                    y=result['p1_strategy'][:, 0],  # é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
                    mode='lines',
                    name=f'{alg_name} - P1',
                    line=dict(color=color, width=2)
                ),
                row=1, col=1
            )
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(n_episodes)),
                    y=result['p2_strategy'][:, 0],
                    mode='lines',
                    name=f'{alg_name} - P2',
                    line=dict(color=color, width=2, dash='dash')
                ),
                row=1, col=1
            )
            
            # 2. ç´¯ç§¯æ”¶ç›Š
            cumulative_p1 = np.cumsum(result['p1_rewards'])
            cumulative_p2 = np.cumsum(result['p2_rewards'])
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(n_episodes)),
                    y=cumulative_p1,
                    mode='lines',
                    name=f'{alg_name} - P1æ”¶ç›Š',
                    line=dict(color=color, width=2)
                ),
                row=1, col=2
            )
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(n_episodes)),
                    y=cumulative_p2,
                    mode='lines',
                    name=f'{alg_name} - P2æ”¶ç›Š',
                    line=dict(color=color, width=2, dash='dash')
                ),
                row=1, col=2
            )
            
            # 3. åˆä½œç‡ï¼ˆåŒæ–¹éƒ½é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼‰
            cooperation_rate = result['p1_strategy'][:, 0] * result['p2_strategy'][:, 0]
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(n_episodes)),
                    y=cooperation_rate,
                    mode='lines',
                    name=f'{alg_name} - åˆä½œç‡',
                    line=dict(color=color, width=3),
                    fill='tozeroy',
                    fillcolor=f'rgba({255 if alg_name == "æ ‡å‡†æ¢¯åº¦" else 0}, {0}, {255 if alg_name == "LOLA" else 0}, 0.2)'
                ),
                row=2, col=1
            )
            
            # 4. ç­–ç•¥ç©ºé—´è½¨è¿¹
            fig.add_trace(
                go.Scatter(
                    x=result['p1_strategy'][:, 0],
                    y=result['p2_strategy'][:, 0],
                    mode='lines+markers',
                    name=f'{alg_name} è½¨è¿¹',
                    line=dict(color=color, width=2),
                    marker=dict(size=3)
                ),
                row=2, col=2
            )
            
            # èµ·ç‚¹å’Œç»ˆç‚¹
            fig.add_trace(
                go.Scatter(
                    x=[result['p1_strategy'][0, 0]],
                    y=[result['p2_strategy'][0, 0]],
                    mode='markers',
                    marker=dict(size=12, color=color, symbol='circle'),
                    name=f'{alg_name} èµ·ç‚¹',
                    showlegend=False
                ),
                row=2, col=2
            )
            
            fig.add_trace(
                go.Scatter(
                    x=[result['p1_strategy'][-1, 0]],
                    y=[result['p2_strategy'][-1, 0]],
                    mode='markers',
                    marker=dict(size=12, color=color, symbol='star'),
                    name=f'{alg_name} ç»ˆç‚¹',
                    showlegend=False
                ),
                row=2, col=2
            )
        
        # æ·»åŠ çº³ä»€å‡è¡¡ç‚¹ï¼ˆå¦‚æœæ˜¯å›šå¾’å›°å¢ƒï¼‰
        if game_scenario == "å›šå¾’å›°å¢ƒ":
            # (èƒŒå›, èƒŒå›) = (0, 0) in probability space
            fig.add_trace(
                go.Scatter(
                    x=[0], y=[0],
                    mode='markers+text',
                    marker=dict(size=15, color='black', symbol='x'),
                    text=['çº³ä»€å‡è¡¡<br>(èƒŒå›,èƒŒå›)'],
                    textposition='bottom center',
                    name='çº³ä»€å‡è¡¡',
                    showlegend=True
                ),
                row=2, col=2
            )
            
            # å¸•ç´¯æ‰˜æœ€ä¼˜ç‚¹
            fig.add_trace(
                go.Scatter(
                    x=[1], y=[1],
                    mode='markers+text',
                    marker=dict(size=15, color='gold', symbol='star'),
                    text=['å¸•ç´¯æ‰˜æœ€ä¼˜<br>(åˆä½œ,åˆä½œ)'],
                    textposition='top center',
                    name='å¸•ç´¯æ‰˜æœ€ä¼˜',
                    showlegend=True
                ),
                row=2, col=2
            )
        
        fig.update_xaxes(title_text="è½®æ•°", row=1, col=1)
        fig.update_yaxes(title_text="åŠ¨ä½œ1æ¦‚ç‡", row=1, col=1)
        fig.update_xaxes(title_text="è½®æ•°", row=1, col=2)
        fig.update_yaxes(title_text="ç´¯ç§¯æ”¶ç›Š", row=1, col=2)
        fig.update_xaxes(title_text="è½®æ•°", row=2, col=1)
        fig.update_yaxes(title_text="åˆä½œç‡", row=2, col=1)
        fig.update_xaxes(title_text="P1åŠ¨ä½œ1æ¦‚ç‡", range=[0, 1], row=2, col=2)
        fig.update_yaxes(title_text="P2åŠ¨ä½œ1æ¦‚ç‡", range=[0, 1], row=2, col=2)
        
        fig.update_layout(
            height=800,
            showlegend=True,
            title_text=f"LOLAç®—æ³•æ¼”ç¤º - {game_scenario}"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ç»Ÿè®¡åˆ†æ
        st.markdown("### ğŸ“Š å­¦ä¹ ç»“æœåˆ†æ")
        
        cols = st.columns(len(results))
        for idx, (alg_name, result) in enumerate(results.items()):
            with cols[idx]:
                st.markdown(f"**{alg_name}**")
                
                # æœ€ç»ˆç­–ç•¥
                final_p1 = result['p1_strategy'][-1, 0]
                final_p2 = result['p2_strategy'][-1, 0]
                st.metric("P1æœ€ç»ˆç­–ç•¥", f"{final_p1:.3f}")
                st.metric("P2æœ€ç»ˆç­–ç•¥", f"{final_p2:.3f}")
                
                # æ€»æ”¶ç›Š
                total_p1 = np.sum(result['p1_rewards'])
                total_p2 = np.sum(result['p2_rewards'])
                st.metric("P1æ€»æ”¶ç›Š", f"{total_p1:.1f}")
                st.metric("P2æ€»æ”¶ç›Š", f"{total_p2:.1f}")
                
                # æœ€ç»ˆåˆä½œç‡
                final_coop = final_p1 * final_p2
                st.metric("æœ€ç»ˆåˆä½œç‡", f"{final_coop:.3f}")
                
                # åˆ¤æ–­æ”¶æ•›æƒ…å†µ
                if game_scenario == "å›šå¾’å›°å¢ƒ":
                    if final_coop > 0.7:
                        st.success("âœ… æˆåŠŸæ¶Œç°åˆä½œ")
                    elif final_coop > 0.3:
                        st.info("ğŸ“Š éƒ¨åˆ†åˆä½œ")
                    else:
                        st.warning("âš ï¸ é™·å…¥èƒŒå›")
        
        # ç†è®ºè§£é‡Š
        st.markdown("### ğŸ“ ç†è®ºæ´å¯Ÿ")
        
        st.success("""
        **LOLAçš„ä¼˜åŠ¿**:
        
        1. **å¯¹æ‰‹å»ºæ¨¡**: æ˜¾å¼è€ƒè™‘å¯¹æ‰‹çš„å­¦ä¹ åŠ¨æ€
           - $\\frac{d\\theta_2}{d\\theta_1}$ æ•æ‰å¯¹æ‰‹å¦‚ä½•å“åº”
        
        2. **é•¿æœŸè§†è§’**: ä¼˜åŒ–é•¿æœŸç´¯ç§¯æ”¶ç›Šè€ŒéçŸ­æœŸ
           - æ„¿æ„çŸ­æœŸç‰ºç‰²æ¢å–é•¿æœŸåˆä½œ
        
        3. **åˆä½œæ¶Œç°**: åœ¨é‡å¤å›šå¾’å›°å¢ƒä¸­è‡ªå‘äº§ç”Ÿ"é’ˆé”‹ç›¸å¯¹"
           - æ— éœ€æ˜¾å¼ç¼–ç¨‹åˆä½œç­–ç•¥
        
        4. **ç­–ç•¥å¡‘é€ **: é€šè¿‡å½±å“å¯¹æ‰‹çš„å­¦ä¹ å¼•å¯¼å‘æœ‰åˆ©æ–¹å‘
           - ç±»ä¼¼Stackelbergçš„Leaderæ€ç»´
        """)
        
        st.info("""
        **æ ‡å‡†æ¢¯åº¦çš„å±€é™**:
        
        1. **çŸ­è§†**: åªçœ‹å½“å‰è½®æ¬¡çš„æ”¶ç›Š
        2. **ç‹¬ç«‹å­¦ä¹ **: å¿½ç•¥å¯¹æ‰‹çš„å­¦ä¹ è¿‡ç¨‹
        3. **é™·å…¥å±€éƒ¨**: å®¹æ˜“é™·å…¥æ¬¡ä¼˜çº³ä»€å‡è¡¡
        4. **æ— åˆä½œ**: åœ¨å›šå¾’å›°å¢ƒä¸­æ€»æ˜¯èƒŒå›
        """)
        
        st.warning("""
        **åº”ç”¨åœºæ™¯**:
        
        - **GANè®­ç»ƒ**: ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„å¯¹æŠ—å­¦ä¹ 
        - **å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ **: åˆä½œä»»åŠ¡ä¸­çš„ç­–ç•¥å­¦ä¹ 
        - **å¯¹æŠ—æ”»é˜²**: æ”»å‡»è€…å’Œé˜²å¾¡è€…çš„åšå¼ˆ
        - **æ‹å–æœºåˆ¶è®¾è®¡**: å¤šä¸ªç«ä»·è€…çš„ç­–ç•¥ä¼˜åŒ–
        - **äº¤é€šæ§åˆ¶**: å¤šä¸ªè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„åè°ƒ
        """)
    
    @cache_numpy_computation(ttl=1800)
    @staticmethod
    def _simulate_lola(payoff_matrix, algorithm, lr, n_episodes):
        """æ¨¡æ‹ŸLOLAæˆ–æ ‡å‡†æ¢¯åº¦å­¦ä¹ """
        # åˆå§‹åŒ–ç­–ç•¥ï¼ˆsoftmaxå‚æ•°ï¼‰
        theta1 = np.zeros(2)
        theta2 = np.zeros(2)
        
        # è®°å½•å†å²
        history = {
            'p1_strategy': [],
            'p2_strategy': [],
            'p1_rewards': [],
            'p2_rewards': []
        }
        
        for episode in range(n_episodes):
            # Softmaxç­–ç•¥
            p1_probs = InteractiveGameTheory._softmax(theta1)
            p2_probs = InteractiveGameTheory._softmax(theta2)
            
            history['p1_strategy'].append(p1_probs.copy())
            history['p2_strategy'].append(p2_probs.copy())
            
            # é‡‡æ ·åŠ¨ä½œ
            a1 = np.random.choice(2, p=p1_probs)
            a2 = np.random.choice(2, p=p2_probs)
            
            # è·å¾—æ”¶ç›Š
            r1 = payoff_matrix['p1'][a1, a2]
            r2 = payoff_matrix['p2'][a1, a2]
            
            history['p1_rewards'].append(r1)
            history['p2_rewards'].append(r2)
            
            # è®¡ç®—æ¢¯åº¦
            if algorithm == "LOLA":
                # LOLA: è€ƒè™‘å¯¹æ‰‹å­¦ä¹ 
                grad1 = InteractiveGameTheory._compute_lola_gradient(
                    theta1, theta2, a1, a2, r1, payoff_matrix['p1']
                )
                grad2 = InteractiveGameTheory._compute_lola_gradient(
                    theta2, theta1, a2, a1, r2, payoff_matrix['p2']
                )
            else:
                # æ ‡å‡†æ¢¯åº¦: ç­–ç•¥æ¢¯åº¦
                grad1 = InteractiveGameTheory._compute_policy_gradient(
                    theta1, a1, r1
                )
                grad2 = InteractiveGameTheory._compute_policy_gradient(
                    theta2, a2, r2
                )
            
            # æ›´æ–°å‚æ•°
            theta1 += lr * grad1
            theta2 += lr * grad2
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        history['p1_strategy'] = np.array(history['p1_strategy'])
        history['p2_strategy'] = np.array(history['p2_strategy'])
        history['p1_rewards'] = np.array(history['p1_rewards'])
        history['p2_rewards'] = np.array(history['p2_rewards'])
        
        return history
    
    @cache_numpy_computation(ttl=1800)
    @staticmethod
    def _softmax(theta):
        """Softmaxå‡½æ•°"""
        exp_theta = np.exp(theta - np.max(theta))  # æ•°å€¼ç¨³å®š
        return exp_theta / np.sum(exp_theta)
    
    @cache_heavy
    @staticmethod
    def _compute_policy_gradient(theta, action, reward):
        """è®¡ç®—æ ‡å‡†ç­–ç•¥æ¢¯åº¦"""
        probs = InteractiveGameTheory._softmax(theta)
        grad = np.zeros_like(theta)
        grad[action] = reward * (1 - probs[action])
        for a in range(len(theta)):
            if a != action:
                grad[a] = -reward * probs[a]
        return grad
    
    @cache_heavy
    @staticmethod
    def _compute_lola_gradient(theta_self, theta_opp, action_self, action_opp, reward, payoff_matrix):
        """è®¡ç®—LOLAæ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # æ ‡å‡†ç­–ç•¥æ¢¯åº¦
        base_grad = InteractiveGameTheory._compute_policy_gradient(theta_self, action_self, reward)
        
        # LOLAä¿®æ­£é¡¹ï¼ˆç®€åŒ–ï¼šå‡è®¾å¯¹æ‰‹ä¹Ÿåœ¨åšç­–ç•¥æ¢¯åº¦ï¼‰
        # å®é™…LOLAéœ€è¦è®¡ç®— d(theta_opp)/d(theta_self)ï¼Œè¿™é‡Œç”¨å¯å‘å¼è¿‘ä¼¼
        probs_opp = InteractiveGameTheory._softmax(theta_opp)
        
        # è€ƒè™‘å¦‚æœå¯¹æ‰‹æ”¹å˜ç­–ç•¥ï¼Œå¯¹è‡ªå·±çš„å½±å“
        correction = np.zeros_like(theta_self)
        for a_self in range(len(theta_self)):
            for a_opp in range(len(theta_opp)):
                future_reward = payoff_matrix[a_self, a_opp]
                correction[a_self] += 0.1 * future_reward * probs_opp[a_opp]
        
        return base_grad + correction

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
