"""
äº¤äº’å¼æ¦‚ç‡ä¸ä¿¡æ¯è®ºå¯è§†åŒ–
ä¸¥æ ¼æŒ‰ç…§ 0.3.Probability_Information.md ä¸­çš„å…¬å¼å®ç°
"""

import streamlit as st
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import scipy.stats as stats


import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥commonæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from common.error_handler import safe_render
from common.quiz_system import QuizSystem, QuizTemplates

class InteractiveProbability:
    """äº¤äº’å¼æ¦‚ç‡ä¸ä¿¡æ¯è®ºå¯è§†åŒ–"""
    
    @staticmethod
    @safe_render

    def render():
        st.subheader("ğŸ² äº¤äº’å¼æ¦‚ç‡ä¸ä¿¡æ¯è®º")
        st.markdown("""
        **æ¦‚ç‡è®º**: ç ”ç©¶éšæœºç°è±¡çš„æ•°å­¦åˆ†æ”¯
        
        **ä¿¡æ¯è®º**: é‡åŒ–ä¿¡æ¯çš„æ•°å­¦ç†è®ºï¼Œç”±Claude Shannonåˆ›ç«‹
        
        **æ ¸å¿ƒæ¦‚å¿µ**:
        - ç†µ (Entropy): ä¸ç¡®å®šæ€§çš„åº¦é‡
        - KLæ•£åº¦ (KL Divergence): ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚
        - äº’ä¿¡æ¯ (Mutual Information): å˜é‡é—´çš„ä¾èµ–å…³ç³»
        - äº¤å‰ç†µ (Cross Entropy): æœºå™¨å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ“Š é€‰æ‹©ä¸»é¢˜")
            topic = st.selectbox("ä¸»é¢˜", [
                "æ¦‚ç‡åˆ†å¸ƒ",
                "ç†µ (Entropy)",
                "KLæ•£åº¦",
                "äº¤å‰ç†µä¸æŸå¤±å‡½æ•°",
                "äº’ä¿¡æ¯",
                "è´å¶æ–¯æ¨æ–­"
            ])
        
        if topic == "æ¦‚ç‡åˆ†å¸ƒ":
            InteractiveProbability._render_distributions()
        elif topic == "ç†µ (Entropy)":
            InteractiveProbability._render_entropy()
        elif topic == "KLæ•£åº¦":
            InteractiveProbability._render_kl_divergence()
        elif topic == "äº¤å‰ç†µä¸æŸå¤±å‡½æ•°":
            InteractiveProbability._render_cross_entropy()
        elif topic == "äº’ä¿¡æ¯":
            InteractiveProbability._render_mutual_information()
        elif topic == "è´å¶æ–¯æ¨æ–­":
            InteractiveProbability._render_bayes()
    

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        quiz_system = QuizSystem("probability")
        quizzes = QuizTemplates.get_probability_quizzes()
        quiz_system.render_quiz(quizzes)
    @staticmethod
    def _render_distributions():
        """æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–"""
        st.markdown("### ğŸ“Š å¸¸è§æ¦‚ç‡åˆ†å¸ƒ")
        
        with st.sidebar:
            st.markdown("### ğŸ›ï¸ åˆ†å¸ƒç±»å‹")
            dist_type = st.selectbox("åˆ†å¸ƒ", [
                "æ­£æ€åˆ†å¸ƒ (Gaussian)",
                "ä¼¯åŠªåˆ©åˆ†å¸ƒ (Bernoulli)",
                "äºŒé¡¹åˆ†å¸ƒ (Binomial)",
                "æ³Šæ¾åˆ†å¸ƒ (Poisson)",
                "æŒ‡æ•°åˆ†å¸ƒ (Exponential)",
                "Betaåˆ†å¸ƒ"
            ])
        
        if dist_type == "æ­£æ€åˆ†å¸ƒ (Gaussian)":
            st.latex(r"p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)")
            
            mu = st.sidebar.slider("å‡å€¼ Î¼", -5.0, 5.0, 0.0, 0.1)
            sigma = st.sidebar.slider("æ ‡å‡†å·® Ïƒ", 0.1, 3.0, 1.0, 0.1)
            
            x = np.linspace(-10, 10, 1000)
            y = stats.norm.pdf(x, mu, sigma)
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=x, y=y, fill='tozeroy', name='PDF'))
            
            # æ ‡æ³¨å…³é”®ç‚¹
            fig.add_vline(x=mu, line_dash="dash", line_color="red",
                         annotation_text=f"Î¼ = {mu}")
            fig.add_vline(x=mu-sigma, line_dash="dot", line_color="orange",
                         annotation_text=f"Î¼-Ïƒ")
            fig.add_vline(x=mu+sigma, line_dash="dot", line_color="orange",
                         annotation_text=f"Î¼+Ïƒ")
            
            fig.update_layout(
                title=f"æ­£æ€åˆ†å¸ƒ N({mu}, {sigma}Â²)",
                xaxis_title="x",
                yaxis_title="æ¦‚ç‡å¯†åº¦ p(x)",
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            st.markdown(f"""
            **æ­£æ€åˆ†å¸ƒæ€§è´¨**:
            - å‡å€¼ = ä¸­ä½æ•° = ä¼—æ•° = {mu}
            - 68.27% çš„æ•°æ®åœ¨ [Î¼-Ïƒ, Î¼+Ïƒ] = [{mu-sigma:.2f}, {mu+sigma:.2f}]
            - 95.45% çš„æ•°æ®åœ¨ [Î¼-2Ïƒ, Î¼+2Ïƒ] = [{mu-2*sigma:.2f}, {mu+2*sigma:.2f}]
            - ç†µ: $H = \\frac{1}{2}\\log(2\\pi e \\sigma^2) = {0.5 * np.log(2*np.pi*np.e*sigma**2):.3f}$ nats
            """)
        
        elif dist_type == "ä¼¯åŠªåˆ©åˆ†å¸ƒ (Bernoulli)":
            st.latex(r"P(X=1) = p, \quad P(X=0) = 1-p")
            
            p = st.sidebar.slider("æˆåŠŸæ¦‚ç‡ p", 0.0, 1.0, 0.5, 0.01)
            
            fig = go.Figure()
            fig.add_trace(go.Bar(x=[0, 1], y=[1-p, p],
                                marker_color=['blue', 'red'],
                                text=[f'{1-p:.3f}', f'{p:.3f}'],
                                textposition='outside'))
            
            fig.update_layout(
                title=f"ä¼¯åŠªåˆ©åˆ†å¸ƒ Bernoulli({p})",
                xaxis_title="X",
                yaxis_title="æ¦‚ç‡ P(X)",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # ç†µ
            if p > 0 and p < 1:
                entropy = -p * np.log2(p) - (1-p) * np.log2(1-p)
            else:
                entropy = 0
            
            st.markdown(f"""
            **ä¼¯åŠªåˆ©åˆ†å¸ƒæ€§è´¨**:
            - æœŸæœ›: $E[X] = p = {p}$
            - æ–¹å·®: $Var[X] = p(1-p) = {p*(1-p):.4f}$
            - ç†µ: $H(X) = -p\\log_2 p - (1-p)\\log_2(1-p) = {entropy:.4f}$ bits
            - ç†µåœ¨ $p=0.5$ æ—¶æœ€å¤§ = 1 bit (æœ€ä¸ç¡®å®š)
            """)
        
        elif dist_type == "äºŒé¡¹åˆ†å¸ƒ (Binomial)":
            st.latex(r"P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}")
            
            n = st.sidebar.slider("è¯•éªŒæ¬¡æ•° n", 1, 50, 10)
            p = st.sidebar.slider("æˆåŠŸæ¦‚ç‡ p", 0.0, 1.0, 0.5, 0.01)
            
            x = np.arange(0, n+1)
            y = stats.binom.pmf(x, n, p)
            
            fig = go.Figure()
            fig.add_trace(go.Bar(x=x, y=y, name='PMF'))
            
            fig.update_layout(
                title=f"äºŒé¡¹åˆ†å¸ƒ Binomial(n={n}, p={p})",
                xaxis_title="æˆåŠŸæ¬¡æ•° k",
                yaxis_title="æ¦‚ç‡ P(X=k)",
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            mean = n * p
            var = n * p * (1-p)
            
            st.markdown(f"""
            **äºŒé¡¹åˆ†å¸ƒæ€§è´¨**:
            - æœŸæœ›: $E[X] = np = {mean:.2f}$
            - æ–¹å·®: $Var[X] = np(1-p) = {var:.2f}$
            - æ ‡å‡†å·®: $\\sigma = \\sqrt{{np(1-p)}} = {np.sqrt(var):.2f}$
            - å½“ $n$ å¾ˆå¤§æ—¶ï¼Œè¿‘ä¼¼æ­£æ€åˆ†å¸ƒ $N(np, np(1-p))$
            """)
        
        elif dist_type == "æ³Šæ¾åˆ†å¸ƒ (Poisson)":
            st.latex(r"P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}")
            
            lambda_val = st.sidebar.slider("é€Ÿç‡å‚æ•° Î»", 0.1, 20.0, 3.0, 0.1)
            
            x = np.arange(0, int(lambda_val * 3 + 10))
            y = stats.poisson.pmf(x, lambda_val)
            
            fig = go.Figure()
            fig.add_trace(go.Bar(x=x, y=y, name='PMF'))
            
            fig.update_layout(
                title=f"æ³Šæ¾åˆ†å¸ƒ Poisson(Î»={lambda_val})",
                xaxis_title="äº‹ä»¶æ¬¡æ•° k",
                yaxis_title="æ¦‚ç‡ P(X=k)",
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            st.markdown(f"""
            **æ³Šæ¾åˆ†å¸ƒæ€§è´¨**:
            - æœŸæœ›: $E[X] = \\lambda = {lambda_val}$
            - æ–¹å·®: $Var[X] = \\lambda = {lambda_val}$
            - ç”¨äºå»ºæ¨¡å•ä½æ—¶é—´å†…äº‹ä»¶å‘ç”Ÿæ¬¡æ•°
            - ä¾‹å¦‚: ç½‘ç«™è®¿é—®é‡ã€æ”¾å°„æ€§è¡°å˜ã€ç”µè¯å‘¼å«
            """)
        
        elif dist_type == "æŒ‡æ•°åˆ†å¸ƒ (Exponential)":
            st.latex(r"p(x) = \lambda e^{-\lambda x}, \quad x \geq 0")
            
            lambda_val = st.sidebar.slider("é€Ÿç‡å‚æ•° Î»", 0.1, 5.0, 1.0, 0.1)
            
            x = np.linspace(0, 10, 1000)
            y = stats.expon.pdf(x, scale=1/lambda_val)
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=x, y=y, fill='tozeroy', name='PDF'))
            
            fig.update_layout(
                title=f"æŒ‡æ•°åˆ†å¸ƒ Exponential(Î»={lambda_val})",
                xaxis_title="x",
                yaxis_title="æ¦‚ç‡å¯†åº¦ p(x)",
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            st.markdown(f"""
            **æŒ‡æ•°åˆ†å¸ƒæ€§è´¨**:
            - æœŸæœ›: $E[X] = \\frac{{1}}{{\\lambda}} = {1/lambda_val:.3f}$
            - æ–¹å·®: $Var[X] = \\frac{{1}}{{\\lambda^2}} = {1/lambda_val**2:.3f}$
            - æ— è®°å¿†æ€§: $P(X > s+t | X > s) = P(X > t)$
            - ç”¨äºå»ºæ¨¡ç­‰å¾…æ—¶é—´ã€å¯¿å‘½åˆ†å¸ƒ
            """)
        
        else:  # Betaåˆ†å¸ƒ
            st.latex(r"p(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}, \quad x \in [0,1]")
            
            alpha = st.sidebar.slider("å‚æ•° Î±", 0.1, 5.0, 2.0, 0.1)
            beta = st.sidebar.slider("å‚æ•° Î²", 0.1, 5.0, 2.0, 0.1)
            
            x = np.linspace(0, 1, 1000)
            y = stats.beta.pdf(x, alpha, beta)
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=x, y=y, fill='tozeroy', name='PDF'))
            
            fig.update_layout(
                title=f"Betaåˆ†å¸ƒ Beta(Î±={alpha}, Î²={beta})",
                xaxis_title="x",
                yaxis_title="æ¦‚ç‡å¯†åº¦ p(x)",
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            mean = alpha / (alpha + beta)
            mode = (alpha - 1) / (alpha + beta - 2) if alpha > 1 and beta > 1 else None
            
            st.markdown(f"""
            **Betaåˆ†å¸ƒæ€§è´¨**:
            - æœŸæœ›: $E[X] = \\frac{{\\alpha}}{{\\alpha + \\beta}} = {mean:.3f}$
            - ä¼—æ•°: $\\text{{Mode}} = \\frac{{\\alpha - 1}}{{\\alpha + \\beta - 2}} = {mode if mode else 'N/A'}$
            - Betaåˆ†å¸ƒæ˜¯[0,1]åŒºé—´ä¸Šçš„å…±è½­å…ˆéªŒ
            - ç”¨äºè´å¶æ–¯æ¨æ–­ä¸­çš„æ¦‚ç‡å»ºæ¨¡
            - Î±=Î²=1 æ—¶ä¸ºå‡åŒ€åˆ†å¸ƒ
            """)
    
    @staticmethod
    def _render_entropy():
        """ç†µçš„å¯è§†åŒ–"""
        st.markdown("### ğŸ“ ç†µ (Entropy): ä¸ç¡®å®šæ€§çš„åº¦é‡")
        
        st.latex(r"""
        H(X) = -\sum_{i} p(x_i) \log p(x_i)
        """)
        
        st.markdown("""
        **ç†µçš„ç›´è§‰**:
        - é¦™å†œç†µé‡åŒ–äº†éšæœºå˜é‡çš„"å¹³å‡æƒŠå¥‡åº¦"
        - ç†µè¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šå‡åŒ€ï¼Œè¶Šä¸ç¡®å®š
        - ç†µè¶Šå°ï¼Œåˆ†å¸ƒè¶Šé›†ä¸­ï¼Œè¶Šç¡®å®š
        
        **å•ä½**:
        - ä»¥2ä¸ºåº• (logâ‚‚): bits
        - ä»¥eä¸ºåº• (ln): nats
        - ä»¥10ä¸ºåº• (logâ‚â‚€): dits
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ›ï¸ åˆ†å¸ƒè®¾ç½®")
            n_categories = st.slider("ç±»åˆ«æ•°", 2, 10, 4)
            dist_type = st.radio("åˆ†å¸ƒç±»å‹", ["è‡ªå®šä¹‰", "å‡åŒ€åˆ†å¸ƒ", "å•å³°åˆ†å¸ƒ", "åŒå³°åˆ†å¸ƒ"])
        
        # ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ
        if dist_type == "è‡ªå®šä¹‰":
            st.markdown("#### è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ")
            probs = []
            for i in range(n_categories):
                p = st.slider(f"P(X={i})", 0.0, 1.0, 1.0/n_categories, 0.01, key=f"p_{i}")
                probs.append(p)
            probs = np.array(probs)
            probs = probs / probs.sum()  # å½’ä¸€åŒ–
        elif dist_type == "å‡åŒ€åˆ†å¸ƒ":
            probs = np.ones(n_categories) / n_categories
        elif dist_type == "å•å³°åˆ†å¸ƒ":
            probs = np.random.dirichlet(np.ones(n_categories) * 5)
            peak = np.argmax(probs)
            probs = np.exp(-((np.arange(n_categories) - peak)**2) / 2)
            probs = probs / probs.sum()
        else:  # åŒå³°åˆ†å¸ƒ
            probs = np.zeros(n_categories)
            if n_categories >= 2:
                probs[0] = 0.4
                probs[-1] = 0.4
                if n_categories > 2:
                    probs[1:-1] = 0.2 / (n_categories - 2)
        
        # è®¡ç®—ç†µ
        entropy_bits = -np.sum(probs * np.log2(probs + 1e-10))
        entropy_nats = -np.sum(probs * np.log(probs + 1e-10))
        max_entropy = np.log2(n_categories)
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("æ¦‚ç‡åˆ†å¸ƒ", "ä¿¡æ¯é‡")
        )
        
        # æ¦‚ç‡åˆ†å¸ƒ
        fig.add_trace(
            go.Bar(x=list(range(n_categories)), y=probs,
                  name='æ¦‚ç‡', marker_color='blue',
                  text=[f'{p:.3f}' for p in probs],
                  textposition='outside'),
            row=1, col=1
        )
        
        # ä¿¡æ¯é‡ -log(p)
        information = -np.log2(probs + 1e-10)
        fig.add_trace(
            go.Bar(x=list(range(n_categories)), y=information,
                  name='ä¿¡æ¯é‡ -logâ‚‚(p)', marker_color='red',
                  text=[f'{i:.2f}' for i in information],
                  textposition='outside'),
            row=1, col=2
        )
        
        fig.update_xaxes(title_text="ç±»åˆ«", row=1, col=1)
        fig.update_xaxes(title_text="ç±»åˆ«", row=1, col=2)
        fig.update_yaxes(title_text="æ¦‚ç‡ p(x)", row=1, col=1)
        fig.update_yaxes(title_text="ä¿¡æ¯é‡ (bits)", row=1, col=2)
        
        fig.update_layout(height=400, showlegend=False)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºç†µ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç†µ H(X)", f"{entropy_bits:.4f} bits")
        with col2:
            st.metric("æœ€å¤§ç†µ", f"{max_entropy:.4f} bits")
        with col3:
            st.metric("ç†µ/æœ€å¤§ç†µ", f"{entropy_bits/max_entropy:.2%}")
        
        st.markdown(f"""
        ### ğŸ“Š ç†µçš„è§£é‡Š
        
        - **å½“å‰ç†µ**: {entropy_bits:.4f} bits = {entropy_nats:.4f} nats
        - **æœ€å¤§ç†µ**: {max_entropy:.4f} bits (å‡åŒ€åˆ†å¸ƒæ—¶è¾¾åˆ°)
        - **å½’ä¸€åŒ–ç†µ**: {entropy_bits/max_entropy:.2%}
        
        **å«ä¹‰**:
        - å¹³å‡éœ€è¦ {entropy_bits:.2f} bits æ¥ç¼–ç ä¸€ä¸ªæ ·æœ¬
        - å¦‚æœåˆ†å¸ƒå®Œå…¨ç¡®å®š (æŸä¸ªæ¦‚ç‡=1)ï¼Œç†µ=0
        - å¦‚æœåˆ†å¸ƒå®Œå…¨å‡åŒ€ï¼Œç†µ=logâ‚‚({n_categories})={max_entropy:.2f}
        
        **åœ¨æœºå™¨å­¦ä¹ ä¸­**:
        - å†³ç­–æ ‘: é€‰æ‹©ä½¿ä¿¡æ¯å¢ç›Šæœ€å¤§çš„ç‰¹å¾
        - äº¤å‰ç†µæŸå¤±: æœ€å°åŒ–é¢„æµ‹åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„äº¤å‰ç†µ
        - ç”Ÿæˆæ¨¡å‹: æœ€å¤§åŒ–æ•°æ®çš„ç†µï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
        """)
    
    @staticmethod
    def _render_kl_divergence():
        """KLæ•£åº¦å¯è§†åŒ–"""
        st.markdown("### ğŸ“ KLæ•£åº¦: åˆ†å¸ƒå·®å¼‚çš„åº¦é‡")
        
        st.latex(r"""
        D_{KL}(P \| Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)}
        """)
        
        st.markdown("""
        **KLæ•£åº¦çš„æ€§è´¨**:
        - âœ… éè´Ÿæ€§: $D_{KL}(P \\| Q) \\geq 0$
        - âœ… å½“ä¸”ä»…å½“ $P = Q$ æ—¶ç­‰äº0
        - âŒ ä¸å¯¹ç§°: $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$
        - âŒ ä¸æ»¡è¶³ä¸‰è§’ä¸ç­‰å¼ï¼ˆä¸æ˜¯çœŸæ­£çš„è·ç¦»åº¦é‡ï¼‰
        
        **ç‰©ç†æ„ä¹‰**: 
        - ç”¨åˆ†å¸ƒQæ¥è¿‘ä¼¼Pæ—¶çš„"é¢å¤–ä¿¡æ¯é‡"
        - VAEä¸­çš„æ­£åˆ™åŒ–é¡¹
        - å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥æ›´æ–°çº¦æŸ
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ›ï¸ åˆ†å¸ƒè®¾ç½®")
            n_categories = st.slider("ç±»åˆ«æ•°", 3, 10, 5)
            
            st.markdown("#### åˆ†å¸ƒP (çœŸå®)")
            p_type = st.selectbox("Pç±»å‹", ["å‡åŒ€", "å•å³°", "åŒå³°", "è‡ªå®šä¹‰"])
            
            st.markdown("#### åˆ†å¸ƒQ (è¿‘ä¼¼)")
            q_type = st.selectbox("Qç±»å‹", ["å‡åŒ€", "å•å³°", "åŒå³°", "è‡ªå®šä¹‰"])
        
        # ç”ŸæˆPåˆ†å¸ƒ
        if p_type == "å‡åŒ€":
            P = np.ones(n_categories) / n_categories
        elif p_type == "å•å³°":
            peak = n_categories // 2
            P = np.exp(-((np.arange(n_categories) - peak)**2) / 2)
            P = P / P.sum()
        elif p_type == "åŒå³°":
            P = np.zeros(n_categories)
            P[0] = 0.4
            P[-1] = 0.4
            if n_categories > 2:
                P[1:-1] = 0.2 / (n_categories - 2)
        else:  # è‡ªå®šä¹‰
            st.markdown("##### Påˆ†å¸ƒ:")
            P = np.array([st.slider(f"P({i})", 0.0, 1.0, 1.0/n_categories, 0.01, 
                                   key=f"p_dist_{i}") for i in range(n_categories)])
            P = P / P.sum()
        
        # ç”ŸæˆQåˆ†å¸ƒ
        if q_type == "å‡åŒ€":
            Q = np.ones(n_categories) / n_categories
        elif q_type == "å•å³°":
            peak = n_categories // 2
            Q = np.exp(-((np.arange(n_categories) - peak)**2) / 2)
            Q = Q / Q.sum()
        elif q_type == "åŒå³°":
            Q = np.zeros(n_categories)
            Q[0] = 0.4
            Q[-1] = 0.4
            if n_categories > 2:
                Q[1:-1] = 0.2 / (n_categories - 2)
        else:  # è‡ªå®šä¹‰
            st.markdown("##### Qåˆ†å¸ƒ:")
            Q = np.array([st.slider(f"Q({i})", 0.0, 1.0, 1.0/n_categories, 0.01,
                                   key=f"q_dist_{i}") for i in range(n_categories)])
            Q = Q / Q.sum()
        
        # è®¡ç®—KLæ•£åº¦
        kl_pq = np.sum(P * np.log((P + 1e-10) / (Q + 1e-10)))
        kl_qp = np.sum(Q * np.log((Q + 1e-10) / (P + 1e-10)))
        
        # JSæ•£åº¦ (å¯¹ç§°ç‰ˆæœ¬)
        M = (P + Q) / 2
        js_divergence = 0.5 * np.sum(P * np.log((P + 1e-10) / (M + 1e-10))) + \
                       0.5 * np.sum(Q * np.log((Q + 1e-10) / (M + 1e-10)))
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("åˆ†å¸ƒå¯¹æ¯”", "é€ç‚¹è´¡çŒ®")
        )
        
        # åˆ†å¸ƒå¯¹æ¯”
        x = list(range(n_categories))
        fig.add_trace(go.Bar(x=x, y=P, name='P (çœŸå®)', marker_color='blue', opacity=0.7),
                     row=1, col=1)
        fig.add_trace(go.Bar(x=x, y=Q, name='Q (è¿‘ä¼¼)', marker_color='red', opacity=0.7),
                     row=1, col=1)
        
        # KLæ•£åº¦çš„é€ç‚¹è´¡çŒ®
        pointwise_kl = P * np.log((P + 1e-10) / (Q + 1e-10))
        fig.add_trace(go.Bar(x=x, y=pointwise_kl, name='P log(P/Q)', marker_color='green'),
                     row=1, col=2)
        
        fig.update_xaxes(title_text="ç±»åˆ«", row=1, col=1)
        fig.update_xaxes(title_text="ç±»åˆ«", row=1, col=2)
        fig.update_yaxes(title_text="æ¦‚ç‡", row=1, col=1)
        fig.update_yaxes(title_text="KLè´¡çŒ®", row=1, col=2)
        
        fig.update_layout(height=500, barmode='overlay')
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºç»“æœ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("D_KL(P || Q)", f"{kl_pq:.4f} nats")
        with col2:
            st.metric("D_KL(Q || P)", f"{kl_qp:.4f} nats")
        with col3:
            st.metric("JSæ•£åº¦", f"{js_divergence:.4f} nats")
        
        # éå¯¹ç§°æ€§æ¼”ç¤º
        if abs(kl_pq - kl_qp) > 0.01:
            st.warning(f"âš ï¸ **ä¸å¯¹ç§°æ€§**: D_KL(P||Q) â‰  D_KL(Q||P), å·®å¼‚ = {abs(kl_pq - kl_qp):.4f}")
        
        st.markdown("""
        ### ğŸ” KLæ•£åº¦çš„åº”ç”¨
        
        **1. VAE (å˜åˆ†è‡ªç¼–ç å™¨)**:
        $$\\mathcal{L} = \\mathbb{E}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))$$
        - ç¬¬ä¸€é¡¹: é‡æ„æŸå¤±
        - ç¬¬äºŒé¡¹: KLæ•£åº¦æ­£åˆ™åŒ–ï¼ˆä½¿ç¼–ç æ¥è¿‘å…ˆéªŒï¼‰
        
        **2. å¼ºåŒ–å­¦ä¹  (TRPO, PPO)**:
        $$D_{KL}(\\pi_{old} \\| \\pi_{new}) < \\delta$$
        - çº¦æŸç­–ç•¥æ›´æ–°ä¸è¦å¤ªæ¿€è¿›
        
        **3. çŸ¥è¯†è’¸é¦**:
        $$\\mathcal{L} = D_{KL}(P_{teacher} \\| P_{student})$$
        - è®©å°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒ
        
        **4. è´å¶æ–¯æ¨æ–­**:
        - ç”¨å˜åˆ†åˆ†å¸ƒq(z)è¿‘ä¼¼åéªŒp(z|x)
        - æœ€å°åŒ– D_KL(q(z) || p(z|x))
        """)
    
    @staticmethod
    def _render_cross_entropy():
        """äº¤å‰ç†µä¸æŸå¤±å‡½æ•°"""
        st.markdown("### ğŸ¯ äº¤å‰ç†µ: æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæŸå¤±å‡½æ•°")
        
        st.latex(r"""
        H(P, Q) = -\sum_i P(i) \log Q(i)
        """)
        
        st.markdown("""
        **äº¤å‰ç†µä¸KLæ•£åº¦çš„å…³ç³»**:
        $$H(P, Q) = H(P) + D_{KL}(P \\| Q)$$
        
        åœ¨åˆ†ç±»é—®é¢˜ä¸­:
        - P: çœŸå®æ ‡ç­¾åˆ†å¸ƒ (one-hot)
        - Q: æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ (softmaxè¾“å‡º)
        - æœ€å°åŒ–äº¤å‰ç†µ = æœ€å°åŒ–KLæ•£åº¦
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ›ï¸ åˆ†ç±»ä»»åŠ¡")
            n_classes = st.slider("ç±»åˆ«æ•°", 2, 10, 3)
            true_class = st.selectbox("çœŸå®ç±»åˆ«", list(range(n_classes)))
            
            st.markdown("#### æ¨¡å‹é¢„æµ‹")
            prediction_quality = st.slider("é¢„æµ‹è´¨é‡", 0.0, 1.0, 0.7, 0.05,
                                          help="1.0=å®Œç¾é¢„æµ‹, 0.0=éšæœºçŒœæµ‹")
        
        # çœŸå®åˆ†å¸ƒ (one-hot)
        P = np.zeros(n_classes)
        P[true_class] = 1.0
        
        # æ¨¡å‹é¢„æµ‹ (å¸¦å™ªå£°çš„softmax)
        Q = np.random.rand(n_classes)
        Q[true_class] = Q[true_class] + prediction_quality * 10
        Q = np.exp(Q) / np.sum(np.exp(Q))  # softmax
        
        # è®¡ç®—æŸå¤±
        cross_entropy = -np.sum(P * np.log(Q + 1e-10))
        entropy_p = 0  # one-hotçš„ç†µä¸º0
        kl_div = cross_entropy - entropy_p
        
        # é¢„æµ‹å‡†ç¡®æ€§
        predicted_class = np.argmax(Q)
        is_correct = (predicted_class == true_class)
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("çœŸå® vs é¢„æµ‹åˆ†å¸ƒ", "äº¤å‰ç†µåˆ†è§£")
        )
        
        x = list(range(n_classes))
        
        # åˆ†å¸ƒå¯¹æ¯”
        fig.add_trace(go.Bar(x=x, y=P, name='çœŸå® (P)', marker_color='blue', opacity=0.7),
                     row=1, col=1)
        fig.add_trace(go.Bar(x=x, y=Q, name='é¢„æµ‹ (Q)', marker_color='red', opacity=0.7),
                     row=1, col=1)
        
        # äº¤å‰ç†µåˆ†è§£
        components = [entropy_p, kl_div]
        labels = ['H(P)', 'D_KL(P||Q)']
        colors = ['blue', 'orange']
        
        fig.add_trace(go.Bar(x=labels, y=components, marker_color=colors,
                            text=[f'{c:.3f}' for c in components],
                            textposition='outside'),
                     row=1, col=2)
        
        fig.update_xaxes(title_text="ç±»åˆ«", row=1, col=1)
        fig.update_xaxes(title_text="ç»„æˆéƒ¨åˆ†", row=1, col=2)
        fig.update_yaxes(title_text="æ¦‚ç‡", row=1, col=1)
        fig.update_yaxes(title_text="å€¼ (nats)", row=1, col=2)
        
        fig.update_layout(height=500, barmode='group')
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºç»“æœ
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("äº¤å‰ç†µæŸå¤±", f"{cross_entropy:.4f}")
        with col2:
            st.metric("é¢„æµ‹æ¦‚ç‡", f"{Q[true_class]:.2%}")
        with col3:
            st.metric("é¢„æµ‹ç±»åˆ«", f"{predicted_class}")
        with col4:
            if is_correct:
                st.metric("åˆ¤æ–­", "âœ… æ­£ç¡®", delta="å‡†ç¡®")
            else:
                st.metric("åˆ¤æ–­", "âŒ é”™è¯¯", delta="å¤±è´¥", delta_color="inverse")
        
        st.markdown(f"""
        ### ğŸ“Š æŸå¤±åˆ†æ
        
        **å½“å‰çŠ¶æ€**:
        - çœŸå®ç±»åˆ«: {true_class}
        - é¢„æµ‹ç±»åˆ«: {predicted_class}
        - é¢„æµ‹ç½®ä¿¡åº¦: {Q[true_class]:.2%}
        - äº¤å‰ç†µæŸå¤±: {cross_entropy:.4f}
        
        **æŸå¤±çš„å«ä¹‰**:
        - å¦‚æœæ¨¡å‹é¢„æµ‹å®Œå…¨æ­£ç¡® (Q[{true_class}]=1): æŸå¤±=0
        - å¦‚æœæ¨¡å‹é¢„æµ‹å®Œå…¨é”™è¯¯ (Q[{true_class}]â†’0): æŸå¤±â†’âˆ
        - å½“å‰æŸå¤± {cross_entropy:.4f} è¡¨ç¤ºæ¨¡å‹éœ€è¦ {cross_entropy:.2f} nats çš„"æƒŠå¥‡"
        
        **æ¢¯åº¦æ–¹å‘**:
        $$\\frac{{\\partial L}}{{\\partial Q_i}} = -\\frac{{P_i}}{{Q_i}}$$
        
        å¯¹äºæ­£ç¡®ç±»åˆ« (i={true_class}):
        - æ¢¯åº¦ = -1/{Q[true_class]:.3f} = {-1/Q[true_class]:.3f}
        - ä¼˜åŒ–å™¨ä¼šå¢å¤§ Q[{true_class}]
        """)
        
        # ä¸åŒé¢„æµ‹çš„æŸå¤±å¯¹æ¯”
        st.markdown("### ğŸ“‰ é¢„æµ‹è´¨é‡ä¸æŸå¤±çš„å…³ç³»")
        
        confidences = np.linspace(0.01, 0.99, 100)
        losses = -np.log(confidences)
        
        fig_loss = go.Figure()
        fig_loss.add_trace(go.Scatter(x=confidences, y=losses, mode='lines',
                                      line=dict(color='red', width=3),
                                      name='äº¤å‰ç†µæŸå¤±'))
        
        # æ ‡æ³¨å½“å‰ç‚¹
        fig_loss.add_trace(go.Scatter(x=[Q[true_class]], y=[cross_entropy],
                                      mode='markers',
                                      marker=dict(size=15, color='blue'),
                                      name='å½“å‰çŠ¶æ€'))
        
        fig_loss.update_layout(
            title="é¢„æµ‹æ¦‚ç‡ vs äº¤å‰ç†µæŸå¤±",
            xaxis_title="å¯¹æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡",
            yaxis_title="äº¤å‰ç†µæŸå¤±",
            height=400
        )
        
        st.plotly_chart(fig_loss, use_container_width=True)
        
        st.markdown("""
        **è§‚å¯Ÿ**:
        - æŸå¤±éšç€é¢„æµ‹æ¦‚ç‡å¢åŠ è€Œå¿«é€Ÿä¸‹é™
        - åœ¨ä½æ¦‚ç‡åŒºåŸŸï¼ŒæŸå¤±çš„æ¢¯åº¦å¾ˆå¤§ï¼ˆå­¦ä¹ å¿«ï¼‰
        - åœ¨é«˜æ¦‚ç‡åŒºåŸŸï¼ŒæŸå¤±çš„æ¢¯åº¦å˜å°ï¼ˆå­¦ä¹ æ…¢ï¼‰
        - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ¨¡å‹åœ¨"æ¥è¿‘æ­£ç¡®"æ—¶æ”¶æ•›å˜æ…¢
        """)
    
    @staticmethod
    def _render_mutual_information():
        """äº’ä¿¡æ¯å¯è§†åŒ–"""
        st.markdown("### ğŸ”— äº’ä¿¡æ¯: å˜é‡ä¾èµ–æ€§çš„åº¦é‡")
        
        st.latex(r"""
        I(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
        """)
        
        st.markdown("""
        **äº’ä¿¡æ¯çš„ç›´è§‰**:
        - æµ‹é‡çŸ¥é“Xåï¼Œå¯¹Yçš„ä¸ç¡®å®šæ€§å‡å°‘äº†å¤šå°‘
        - $I(X; Y) = H(Y) - H(Y|X)$
        - $I(X; Y) = H(X) + H(Y) - H(X,Y)$
        - å¯¹ç§°: $I(X; Y) = I(Y; X)$
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ›ï¸ ç›¸å…³æ€§è®¾ç½®")
            correlation = st.slider("ç›¸å…³ç¨‹åº¦", 0.0, 1.0, 0.7, 0.05,
                                   help="0=ç‹¬ç«‹, 1=å®Œå…¨ç›¸å…³")
        
        # ç”ŸæˆäºŒç»´æ•°æ®
        n = 1000
        X = np.random.randn(n)
        Y = correlation * X + np.sqrt(1 - correlation**2) * np.random.randn(n)
        
        # ç¦»æ•£åŒ–è®¡ç®—äº’ä¿¡æ¯
        n_bins = 10
        H_X, edges_X = np.histogram(X, bins=n_bins, density=True)
        H_Y, edges_Y = np.histogram(Y, bins=n_bins, density=True)
        H_X = H_X / H_X.sum()
        H_Y = H_Y / H_Y.sum()
        
        # è”åˆåˆ†å¸ƒ
        H_XY, _, _ = np.histogram2d(X, Y, bins=n_bins, density=True)
        H_XY = H_XY / H_XY.sum()
        
        # è®¡ç®—ç†µå’Œäº’ä¿¡æ¯
        entropy_X = -np.sum(H_X * np.log(H_X + 1e-10))
        entropy_Y = -np.sum(H_Y * np.log(H_Y + 1e-10))
        entropy_XY = -np.sum(H_XY * np.log(H_XY + 1e-10))
        mutual_info = entropy_X + entropy_Y - entropy_XY
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("è”åˆåˆ†å¸ƒ", "ä¿¡æ¯å…³ç³»")
        )
        
        # è”åˆåˆ†å¸ƒçƒ­åŠ›å›¾
        fig.add_trace(go.Heatmap(z=H_XY, colorscale='Blues', showscale=True),
                     row=1, col=1)
        
        # ä¿¡æ¯å…³ç³»ï¼ˆæ–‡æ°å›¾é£æ ¼ï¼‰
        info_data = {
            'H(X)': entropy_X,
            'H(Y)': entropy_Y,
            'H(X,Y)': entropy_XY,
            'I(X;Y)': mutual_info
        }
        
        fig.add_trace(go.Bar(
            x=list(info_data.keys()),
            y=list(info_data.values()),
            marker_color=['blue', 'red', 'purple', 'green'],
            text=[f'{v:.3f}' for v in info_data.values()],
            textposition='outside'
        ), row=1, col=2)
        
        fig.update_xaxes(title_text="X", row=1, col=1)
        fig.update_yaxes(title_text="Y", row=1, col=1)
        fig.update_xaxes(title_text="ä¿¡æ¯é‡", row=1, col=2)
        fig.update_yaxes(title_text="ç†µ (nats)", row=1, col=2)
        
        fig.update_layout(height=500, showlegend=False)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºç»“æœ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("H(X)", f"{entropy_X:.3f} nats")
        with col2:
            st.metric("H(Y)", f"{entropy_Y:.3f} nats")
        with col3:
            st.metric("I(X; Y)", f"{mutual_info:.3f} nats")
        
        # å½’ä¸€åŒ–äº’ä¿¡æ¯
        normalized_mi = mutual_info / min(entropy_X, entropy_Y) if min(entropy_X, entropy_Y) > 0 else 0
        
        st.markdown(f"""
        ### ğŸ“Š äº’ä¿¡æ¯åˆ†æ
        
        **è®¡ç®—ç»“æœ**:
        - è”åˆç†µ: H(X,Y) = {entropy_XY:.3f}
        - äº’ä¿¡æ¯: I(X;Y) = {mutual_info:.3f}
        - å½’ä¸€åŒ–äº’ä¿¡æ¯: {normalized_mi:.2%}
        
        **å…³ç³»éªŒè¯**:
        - H(X) + H(Y) = {entropy_X + entropy_Y:.3f}
        - H(X,Y) + I(X;Y) = {entropy_XY + mutual_info:.3f}
        - åº”è¯¥ç›¸ç­‰: {'âœ…' if abs((entropy_X + entropy_Y) - (entropy_XY + mutual_info)) < 0.01 else 'âŒ'}
        
        **åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨**:
        - **ç‰¹å¾é€‰æ‹©**: é€‰æ‹©ä¸æ ‡ç­¾äº’ä¿¡æ¯é«˜çš„ç‰¹å¾
        - **ä¿¡æ¯ç“¶é¢ˆç†è®º**: ç¥ç»ç½‘ç»œå±‚é—´çš„äº’ä¿¡æ¯æ¼”åŒ–
        - **å¯¹æ¯”å­¦ä¹ **: æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹çš„äº’ä¿¡æ¯
        - **ç”Ÿæˆæ¨¡å‹**: æœ€å¤§åŒ–ç”Ÿæˆæ•°æ®ä¸çœŸå®æ•°æ®çš„äº’ä¿¡æ¯
        """)
    
    @staticmethod
    def _render_bayes():
        """è´å¶æ–¯æ¨æ–­å¯è§†åŒ–"""
        st.markdown("### ğŸ² è´å¶æ–¯æ¨æ–­: ä»å…ˆéªŒåˆ°åéªŒ")
        
        st.latex(r"""
        P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}
        """)
        
        st.markdown("""
        **è´å¶æ–¯å®šç†çš„ç»„æˆ**:
        - $P(\\theta)$: å…ˆéªŒ (Prior) - è§‚æµ‹æ•°æ®å‰çš„ä¿¡å¿µ
        - $P(D | \\theta)$: ä¼¼ç„¶ (Likelihood) - æ•°æ®åœ¨å‚æ•°ä¸‹çš„æ¦‚ç‡
        - $P(\\theta | D)$: åéªŒ (Posterior) - è§‚æµ‹æ•°æ®åçš„æ›´æ–°ä¿¡å¿µ
        - $P(D)$: è¯æ® (Evidence) - å½’ä¸€åŒ–å¸¸æ•°
        """)
        
        st.markdown("#### ç¤ºä¾‹: ç¡¬å¸æŠ›æ·çš„è´å¶æ–¯æ¨æ–­")
        
        with st.sidebar:
            st.markdown("### ğŸ›ï¸ å…ˆéªŒè®¾ç½®")
            prior_alpha = st.slider("å…ˆéªŒ Î±", 0.5, 10.0, 1.0, 0.5)
            prior_beta = st.slider("å…ˆéªŒ Î²", 0.5, 10.0, 1.0, 0.5)
            
            st.markdown("### ğŸ² è§‚æµ‹æ•°æ®")
            n_heads = st.slider("æ­£é¢æ¬¡æ•°", 0, 100, 7, 1)
            n_tails = st.slider("åé¢æ¬¡æ•°", 0, 100, 3, 1)
        
        # å…ˆéªŒ: Betaåˆ†å¸ƒ
        theta = np.linspace(0, 1, 500)
        prior = stats.beta.pdf(theta, prior_alpha, prior_beta)
        
        # ä¼¼ç„¶: äºŒé¡¹åˆ†å¸ƒ
        likelihood = stats.binom.pmf(n_heads, n_heads + n_tails, theta)
        
        # åéªŒ: Betaåˆ†å¸ƒ (å…±è½­å…ˆéªŒ)
        posterior_alpha = prior_alpha + n_heads
        posterior_beta = prior_beta + n_tails
        posterior = stats.beta.pdf(theta, posterior_alpha, posterior_beta)
        
        # å¯è§†åŒ–
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(x=theta, y=prior, mode='lines',
                                name='å…ˆéªŒ P(Î¸)', line=dict(color='blue', width=2)))
        
        fig.add_trace(go.Scatter(x=theta, y=likelihood / likelihood.max() * prior.max(),
                                name='ä¼¼ç„¶ P(D|Î¸) (å½’ä¸€åŒ–)', 
                                line=dict(color='orange', width=2, dash='dash')))
        
        fig.add_trace(go.Scatter(x=theta, y=posterior, mode='lines',
                                name='åéªŒ P(Î¸|D)', line=dict(color='red', width=3)))
        
        # åéªŒå‡å€¼
        posterior_mean = posterior_alpha / (posterior_alpha + posterior_beta)
        fig.add_vline(x=posterior_mean, line_dash="dot", line_color="red",
                     annotation_text=f"åéªŒå‡å€¼ = {posterior_mean:.3f}")
        
        fig.update_layout(
            title=f"è´å¶æ–¯æ›´æ–°: è§‚æµ‹åˆ° {n_heads} æ­£é¢, {n_tails} åé¢",
            xaxis_title="Î¸ (æ­£é¢æ¦‚ç‡)",
            yaxis_title="å¯†åº¦",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºç»Ÿè®¡é‡
        prior_mean = prior_alpha / (prior_alpha + prior_beta)
        mle = n_heads / (n_heads + n_tails) if (n_heads + n_tails) > 0 else 0.5
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å…ˆéªŒå‡å€¼", f"{prior_mean:.3f}")
        with col2:
            st.metric("MLEä¼°è®¡", f"{mle:.3f}")
        with col3:
            st.metric("åéªŒå‡å€¼", f"{posterior_mean:.3f}")
        
        st.markdown(f"""
        ### ğŸ” è´å¶æ–¯ vs é¢‘ç‡æ´¾
        
        **é¢‘ç‡æ´¾ (MLE)**:
        - å‚æ•°æ˜¯å›ºå®šä½†æœªçŸ¥çš„å¸¸æ•°
        - ä¼°è®¡: $\\hat{{\\theta}}_{{MLE}} = \\frac{{{n_heads}}}{{{n_heads + n_tails}}} = {mle:.3f}$
        - åªä¾èµ–æ•°æ®ï¼Œå¿½ç•¥å…ˆéªŒçŸ¥è¯†
        
        **è´å¶æ–¯æ´¾**:
        - å‚æ•°æ˜¯éšæœºå˜é‡ï¼Œæœ‰åˆ†å¸ƒ
        - ä¼°è®¡: $\\mathbb{{E}}[\\theta|D] = {posterior_mean:.3f}$
        - ç»“åˆå…ˆéªŒçŸ¥è¯†å’Œè§‚æµ‹æ•°æ®
        - ç»™å‡ºå®Œæ•´çš„åéªŒåˆ†å¸ƒï¼Œè€Œä¸åªæ˜¯ç‚¹ä¼°è®¡
        
        **åéªŒæ›´æ–°è§„åˆ™ (Beta-Binomialå…±è½­)**:
        $$\\text{{Beta}}(\\alpha, \\beta) + \\text{{Data}}(h, t) \\to \\text{{Beta}}(\\alpha+h, \\beta+t)$$
        
        **è§‚å¯Ÿ**:
        - æ•°æ®é‡å°‘æ—¶ï¼Œå…ˆéªŒå½±å“å¤§
        - æ•°æ®é‡å¤šæ—¶ï¼Œä¼¼ç„¶ä¸»å¯¼ï¼Œè´å¶æ–¯â†’MLE
        - å½“å‰: å…ˆéªŒ({prior_alpha}, {prior_beta}) + æ•°æ®({n_heads}, {n_tails}) â†’ åéªŒ({posterior_alpha}, {posterior_beta})
        """)
