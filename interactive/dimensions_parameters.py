"""
å·¥ç¨‹é€ŸæŸ¥ï¼šç»´åº¦åˆ†æä¸å‚æ•°ä¼°ç®— - äº¤äº’å¼å¯è§†åŒ–
åŸºäº AppxD_Dimensions_Parameters.md

æ ¸å¿ƒå†…å®¹ï¼š
1. Transformerå‚æ•°è®¡ç®—å™¨
2. CNNå‚æ•°è®¡ç®—å™¨
3. æ˜¾å­˜å ç”¨ä¼°ç®—
4. æ¶æ„å¯¹æ¯”ï¼ˆGPT vs BERTï¼‰
5. è®­ç»ƒæ˜¾å­˜è§£å‰–
6. æ··åˆç²¾åº¦ä¸é‡åŒ–
"""

import streamlit as st
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import pandas as pd


import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥commonæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from common.error_handler import safe_render
from common.quiz_system import QuizSystem, QuizTemplates
from common.smart_cache import cache_medium, cache_heavy, cache_numpy_computation

class InteractiveDimensionsParameters:
    """äº¤äº’å¼ç»´åº¦åˆ†æä¸å‚æ•°ä¼°ç®—"""
    
    @staticmethod
    @safe_render

    def render():
        st.subheader("ğŸ”§ å·¥ç¨‹é€ŸæŸ¥ï¼šç»´åº¦åˆ†æä¸å‚æ•°ä¼°ç®—")
        
        st.markdown(r"""
        **å·¥ç¨‹å¸ˆå¿…å¤‡å·¥å…·**ï¼šå¿«é€Ÿè®¡ç®—æ¨¡å‹å‚æ•°é‡ã€æ˜¾å­˜å ç”¨ã€è®­ç»ƒæˆæœ¬
        
        æœ¬æ¨¡å—å¸®åŠ©ä½ ï¼š
        - ğŸ“Š ä¼°ç®—æ¨¡å‹å‚æ•°é‡
        - ğŸ’¾ è®¡ç®—æ˜¾å­˜éœ€æ±‚
        - âš¡ ä¼˜åŒ–èµ„æºé…ç½®
        - ğŸ¯ ç†è§£æ¶æ„å·®å¼‚
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ“Š é€‰æ‹©å·¥å…·")
            tool_type = st.selectbox(
                "å·¥å…·ç±»å‹",
                [
                    "Transformerå‚æ•°è®¡ç®—å™¨",
                    "CNNå‚æ•°è®¡ç®—å™¨",
                    "æ˜¾å­˜å ç”¨ä¼°ç®—",
                    "æ¶æ„å¯¹æ¯” (GPT vs BERT)",
                    "è®­ç»ƒæ˜¾å­˜è§£å‰–",
                    "æ··åˆç²¾åº¦ä¸é‡åŒ–"
                ]
            )
        
        if tool_type == "Transformerå‚æ•°è®¡ç®—å™¨":
            InteractiveDimensionsParameters._render_transformer_calculator()
        elif tool_type == "CNNå‚æ•°è®¡ç®—å™¨":
            InteractiveDimensionsParameters._render_cnn_calculator()
        elif tool_type == "æ˜¾å­˜å ç”¨ä¼°ç®—":
            InteractiveDimensionsParameters._render_memory_calculator()
        elif tool_type == "æ¶æ„å¯¹æ¯” (GPT vs BERT)":
            InteractiveDimensionsParameters._render_architecture_comparison()
        elif tool_type == "è®­ç»ƒæ˜¾å­˜è§£å‰–":
            InteractiveDimensionsParameters._render_memory_anatomy()
        elif tool_type == "æ··åˆç²¾åº¦ä¸é‡åŒ–":
            InteractiveDimensionsParameters._render_precision_quantization()
    

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        quiz_system = QuizSystem("dimensions_parameters")
        quizzes = QuizTemplates.get_dimensions_parameters_quizzes()
        quiz_system.render_quiz(quizzes)
    @staticmethod
    def _render_transformer_calculator():
        """Transformerå‚æ•°è®¡ç®—å™¨"""
        st.markdown("### ğŸ¤– Transformerå‚æ•°è®¡ç®—å™¨")
        
        st.markdown(r"""
        **æ ¸å¿ƒå…¬å¼**ï¼š
        """)
        
        st.latex(r"""
        \begin{align}
        \text{Embedding} &= V \times d_{model} \\
        \text{Self-Attention} &= 4 \times d_{model}^2 \\
        \text{FFN} &= 8 \times d_{model}^2 \\
        \text{One Layer} &\approx 12 \times d_{model}^2 \\
        \text{Total} &\approx V \times d_{model} + L \times 12 \times d_{model}^2
        \end{align}
        """)
        
        with st.sidebar:
            st.markdown("#### æ¨¡å‹é…ç½®")
            vocab_size = st.number_input("è¯è¡¨å¤§å° (V)", 10000, 100000, 50257, 1000)
            d_model = st.number_input("éšè—ç»´åº¦ (d_model)", 256, 8192, 768, 64)
            n_layers = st.number_input("å±‚æ•° (L)", 1, 96, 12, 1)
            n_heads = st.number_input("æ³¨æ„åŠ›å¤´æ•°", 1, 128, 12, 1)
            d_ff = st.number_input("FFNç»´åº¦", 256, 32768, d_model * 4, 256)
            
            st.markdown("#### é¢å¤–é€‰é¡¹")
            include_bias = st.checkbox("åŒ…å«bias", value=True)
            include_ln = st.checkbox("åŒ…å«LayerNorm", value=True)
        
        # è®¡ç®—å„éƒ¨åˆ†å‚æ•°
        # 1. Embedding
        embedding_params = vocab_size * d_model
        
        # 2. æ¯å±‚çš„å‚æ•°
        # Self-Attention: Q, K, V, O å„æœ‰ d_model x d_model
        attn_params = 4 * d_model * d_model
        if include_bias:
            attn_params += 4 * d_model
        
        # FFN: W1, W2
        ffn_params = d_model * d_ff + d_ff * d_model
        if include_bias:
            ffn_params += d_ff + d_model
        
        # LayerNorm: gamma, beta (ä¸¤ä¸ªLN)
        ln_params = 0
        if include_ln:
            ln_params = 2 * (2 * d_model)
        
        # æ¯å±‚æ€»å‚æ•°
        params_per_layer = attn_params + ffn_params + ln_params
        
        # æ‰€æœ‰å±‚
        all_layers_params = n_layers * params_per_layer
        
        # è¾“å‡ºå±‚ï¼ˆå¯é€‰ï¼‰
        output_params = d_model * vocab_size

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        
        # æ€»å‚æ•°
        total_params = embedding_params + all_layers_params + output_params
        
        # æ˜¾ç¤ºç»“æœ
        st.markdown("### ğŸ“Š å‚æ•°é‡åˆ†è§£")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ€»å‚æ•°é‡", f"{total_params/1e6:.1f}M")
        with col2:
            st.metric("å±‚æ•°", n_layers)
        with col3:
            st.metric("d_model", d_model)
        
        # è¯¦ç»†åˆ†è§£
        breakdown = {
            "ç»„ä»¶": [
                "Embeddingå±‚",
                f"Transformerå±‚ (Ã—{n_layers})",
                "  - Self-Attention",
                "  - FFN",
                "  - LayerNorm" if include_ln else None,
                "è¾“å‡ºå±‚",
                "æ€»è®¡"
            ],
            "å‚æ•°é‡": [
                f"{embedding_params/1e6:.2f}M",
                f"{all_layers_params/1e6:.2f}M",
                f"{attn_params/1e6:.2f}M (æ¯å±‚)",
                f"{ffn_params/1e6:.2f}M (æ¯å±‚)",
                f"{ln_params/1e3:.2f}K (æ¯å±‚)" if include_ln else None,
                f"{output_params/1e6:.2f}M",
                f"{total_params/1e6:.2f}M"
            ],
            "å æ¯”": [
                f"{embedding_params/total_params*100:.1f}%",
                f"{all_layers_params/total_params*100:.1f}%",
                f"{attn_params/params_per_layer*100:.1f}%",
                f"{ffn_params/params_per_layer*100:.1f}%",
                f"{ln_params/params_per_layer*100:.2f}%" if include_ln else None,
                f"{output_params/total_params*100:.1f}%",
                "100.0%"
            ]
        }
        
        # è¿‡æ»¤None
        breakdown = {k: [v for v in vals if v is not None] 
                    for k, vals in breakdown.items()}
        
        df = pd.DataFrame(breakdown)
        st.dataframe(df, use_container_width=True)
        
        # å¯è§†åŒ–
        fig = go.Figure()
        
        # é¥¼å›¾ï¼šå‚æ•°åˆ†å¸ƒ
        labels = ["Embedding", "Transformerå±‚", "è¾“å‡ºå±‚"]
        values = [embedding_params, all_layers_params, output_params]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        fig.add_trace(go.Pie(
            labels=labels,
            values=values,
            marker=dict(colors=colors),
            textinfo='label+percent',
            hovertemplate='%{label}<br>%{value:.2f}M å‚æ•°<br>%{percent}<extra></extra>'
        ))
        
        fig.update_layout(
            title="å‚æ•°åˆ†å¸ƒ",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å±‚å‚æ•°è¯¦ç»†åˆ†è§£
        st.markdown("### ğŸ” å•å±‚å‚æ•°è¯¦ç»†åˆ†è§£")
        
        layer_breakdown = {
            "ç»„ä»¶": ["QçŸ©é˜µ", "KçŸ©é˜µ", "VçŸ©é˜µ", "OçŸ©é˜µ", "FFN-W1", "FFN-W2"],
            "å½¢çŠ¶": [
                f"({d_model}, {d_model})",
                f"({d_model}, {d_model})",
                f"({d_model}, {d_model})",
                f"({d_model}, {d_model})",
                f"({d_model}, {d_ff})",
                f"({d_ff}, {d_model})"
            ],
            "å‚æ•°é‡": [
                f"{d_model * d_model:,}",
                f"{d_model * d_model:,}",
                f"{d_model * d_model:,}",
                f"{d_model * d_model:,}",
                f"{d_model * d_ff:,}",
                f"{d_ff * d_model:,}"
            ]
        }
        
        df_layer = pd.DataFrame(layer_breakdown)
        st.dataframe(df_layer, use_container_width=True)
        
        # å®é™…æ¨¡å‹å¯¹æ¯”
        st.markdown("### ğŸ¯ ä¸å®é™…æ¨¡å‹å¯¹æ¯”")
        
        real_models = {
            "æ¨¡å‹": ["GPT-2 Small", "GPT-2 Medium", "GPT-2 Large", "BERT-Base", "BERT-Large", "æ‚¨çš„é…ç½®"],
            "å±‚æ•°": [12, 24, 36, 12, 24, n_layers],
            "d_model": [768, 1024, 1280, 768, 1024, d_model],
            "å‚æ•°é‡ (M)": [117, 345, 774, 110, 340, total_params/1e6]
        }
        
        df_models = pd.DataFrame(real_models)
        
        # é«˜äº®æ‚¨çš„é…ç½®
        def highlight_yours(row):
            if row["æ¨¡å‹"] == "æ‚¨çš„é…ç½®":
                return ['background-color: #FFF3CD'] * len(row)
            return [''] * len(row)
        
        st.dataframe(df_models.style.apply(highlight_yours, axis=1), 
                    use_container_width=True)
        
        st.info(r"""
        **å¿«é€Ÿä¼°ç®—å…¬å¼**:
        
        å¯¹äºæ ‡å‡†Transformerï¼ˆd_ff = 4 Ã— d_modelï¼‰ï¼š
        
        $$\text{Total Params} \approx V \times d + L \times 12 \times d^2$$
        
        å…¶ä¸­ï¼š
        - V: è¯è¡¨å¤§å°
        - d: d_model
        - L: å±‚æ•°
        
        **è®°å¿†æŠ€å·§**ï¼šæ¯å±‚çº¦ 12dÂ² å‚æ•°ï¼ˆ4dÂ² attention + 8dÂ² FFNï¼‰
        """)
    
    @staticmethod
    def _render_cnn_calculator():
        """CNNå‚æ•°è®¡ç®—å™¨"""
        st.markdown("### ğŸ–¼ï¸ CNNå‚æ•°è®¡ç®—å™¨")
        
        st.markdown(r"""
        **å·ç§¯å±‚å‚æ•°å…¬å¼**ï¼š
        """)
        
        st.latex(r"""
        \text{Conv Params} = (K_h \times K_w \times C_{in} + 1) \times C_{out}
        """)
        
        st.markdown(r"""
        å…¶ä¸­ï¼š
        - $K_h, K_w$: å·ç§¯æ ¸é«˜åº¦å’Œå®½åº¦
        - $C_{in}$: è¾“å…¥é€šé“æ•°
        - $C_{out}$: è¾“å‡ºé€šé“æ•°
        - +1: biasé¡¹
        """)
        
        with st.sidebar:
            st.markdown("#### ç½‘ç»œé…ç½®")
            network_type = st.selectbox(
                "ç½‘ç»œç±»å‹",
                ["è‡ªå®šä¹‰", "LeNet-5", "AlexNet", "VGG-16", "ResNet-50"]
            )
            
            if network_type == "è‡ªå®šä¹‰":
                n_conv_layers = st.slider("å·ç§¯å±‚æ•°é‡", 1, 10, 3)
        
        if network_type == "è‡ªå®šä¹‰":
            # è‡ªå®šä¹‰é…ç½®
            st.markdown("#### ğŸ”§ è‡ªå®šä¹‰CNNé…ç½®")
            
            layers_config = []
            total_params = 0
            
            for i in range(n_conv_layers):
                with st.expander(f"ç¬¬ {i+1} å±‚"):
                    col1, col2 = st.columns(2)
                    with col1:
                        c_in = st.number_input(f"è¾“å…¥é€šé“", 1, 2048, 
                                              3 if i == 0 else 64, 
                                              key=f"cin_{i}")
                        k_h = st.number_input(f"å·ç§¯æ ¸é«˜åº¦", 1, 11, 3, key=f"kh_{i}")
                    with col2:
                        c_out = st.number_input(f"è¾“å‡ºé€šé“", 1, 2048, 64, key=f"cout_{i}")
                        k_w = st.number_input(f"å·ç§¯æ ¸å®½åº¦", 1, 11, 3, key=f"kw_{i}")
                    
                    include_bias = st.checkbox(f"åŒ…å«bias", value=True, key=f"bias_{i}")
                    
                    # è®¡ç®—å‚æ•°
                    params = k_h * k_w * c_in * c_out
                    if include_bias:
                        params += c_out
                    
                    layers_config.append({
                        "å±‚": f"Conv{i+1}",
                        "è¾“å…¥é€šé“": c_in,
                        "è¾“å‡ºé€šé“": c_out,
                        "å·ç§¯æ ¸": f"{k_h}Ã—{k_w}",
                        "å‚æ•°é‡": f"{params:,}",
                        "å‚æ•°é‡(æ•°å€¼)": params
                    })
                    
                    total_params += params
            
            # æ˜¾ç¤ºç»“æœ
            st.markdown("### ğŸ“Š å‚æ•°ç»Ÿè®¡")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("æ€»å‚æ•°é‡", f"{total_params/1e6:.2f}M")
            with col2:
                st.metric("å·ç§¯å±‚æ•°", n_conv_layers)
            
            df = pd.DataFrame(layers_config)
            st.dataframe(df.drop('å‚æ•°é‡(æ•°å€¼)', axis=1), use_container_width=True)
            
            # å¯è§†åŒ–
            fig = go.Figure(data=[
                go.Bar(
                    x=[layer["å±‚"] for layer in layers_config],
                    y=[layer["å‚æ•°é‡(æ•°å€¼)"] for layer in layers_config],
                    text=[f"{layer['å‚æ•°é‡(æ•°å€¼)']/1e3:.1f}K" for layer in layers_config],
                    textposition='outside',
                    marker_color='steelblue'
                )
            ])
            
            fig.update_layout(
                title="å„å±‚å‚æ•°é‡åˆ†å¸ƒ",
                xaxis_title="å±‚",
                yaxis_title="å‚æ•°é‡",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
        else:
            # é¢„è®¾ç½‘ç»œ
            preset_configs = {
                "LeNet-5": [
                    {"å±‚": "Conv1", "è¾“å…¥": 1, "è¾“å‡º": 6, "å·ç§¯æ ¸": "5Ã—5", "å‚æ•°": 5*5*1*6 + 6},
                    {"å±‚": "Conv2", "è¾“å…¥": 6, "è¾“å‡º": 16, "å·ç§¯æ ¸": "5Ã—5", "å‚æ•°": 5*5*6*16 + 16},
                    {"å±‚": "FC1", "è¾“å…¥": 400, "è¾“å‡º": 120, "å·ç§¯æ ¸": "-", "å‚æ•°": 400*120 + 120},
                    {"å±‚": "FC2", "è¾“å…¥": 120, "è¾“å‡º": 84, "å·ç§¯æ ¸": "-", "å‚æ•°": 120*84 + 84},
                    {"å±‚": "FC3", "è¾“å…¥": 84, "è¾“å‡º": 10, "å·ç§¯æ ¸": "-", "å‚æ•°": 84*10 + 10},
                ],
                "AlexNet": [
                    {"å±‚": "Conv1", "è¾“å…¥": 3, "è¾“å‡º": 96, "å·ç§¯æ ¸": "11Ã—11", "å‚æ•°": 11*11*3*96 + 96},
                    {"å±‚": "Conv2", "è¾“å…¥": 96, "è¾“å‡º": 256, "å·ç§¯æ ¸": "5Ã—5", "å‚æ•°": 5*5*96*256 + 256},
                    {"å±‚": "Conv3", "è¾“å…¥": 256, "è¾“å‡º": 384, "å·ç§¯æ ¸": "3Ã—3", "å‚æ•°": 3*3*256*384 + 384},
                    {"å±‚": "Conv4", "è¾“å…¥": 384, "è¾“å‡º": 384, "å·ç§¯æ ¸": "3Ã—3", "å‚æ•°": 3*3*384*384 + 384},
                    {"å±‚": "Conv5", "è¾“å…¥": 384, "è¾“å‡º": 256, "å·ç§¯æ ¸": "3Ã—3", "å‚æ•°": 3*3*384*256 + 256},
                    {"å±‚": "FC1", "è¾“å…¥": 9216, "è¾“å‡º": 4096, "å·ç§¯æ ¸": "-", "å‚æ•°": 9216*4096 + 4096},
                    {"å±‚": "FC2", "è¾“å…¥": 4096, "è¾“å‡º": 4096, "å·ç§¯æ ¸": "-", "å‚æ•°": 4096*4096 + 4096},
                    {"å±‚": "FC3", "è¾“å…¥": 4096, "è¾“å‡º": 1000, "å·ç§¯æ ¸": "-", "å‚æ•°": 4096*1000 + 1000},
                ],
                "VGG-16": [
                    {"å±‚": "Conv1-1", "è¾“å…¥": 3, "è¾“å‡º": 64, "å·ç§¯æ ¸": "3Ã—3", "å‚æ•°": 3*3*3*64 + 64},
                    {"å±‚": "Conv1-2", "è¾“å…¥": 64, "è¾“å‡º": 64, "å·ç§¯æ ¸": "3Ã—3", "å‚æ•°": 3*3*64*64 + 64},
                    {"å±‚": "Conv2-1", "è¾“å…¥": 64, "è¾“å‡º": 128, "å·ç§¯æ ¸": "3Ã—3", "å‚æ•°": 3*3*64*128 + 128},
                    {"å±‚": "Conv2-2", "è¾“å…¥": 128, "è¾“å‡º": 128, "å·ç§¯æ ¸": "3Ã—3", "å‚æ•°": 3*3*128*128 + 128},
                    {"å±‚": "...", "è¾“å…¥": "...", "è¾“å‡º": "...", "å·ç§¯æ ¸": "...", "å‚æ•°": 0},
                    {"å±‚": "FC", "è¾“å…¥": 25088, "è¾“å‡º": 4096, "å·ç§¯æ ¸": "-", "å‚æ•°": 25088*4096 + 4096},
                ],
                "ResNet-50": [
                    {"å±‚": "Conv1", "è¾“å…¥": 3, "è¾“å‡º": 64, "å·ç§¯æ ¸": "7Ã—7", "å‚æ•°": 7*7*3*64 + 64},
                    {"å±‚": "BottleneckÃ—3", "è¾“å…¥": 64, "è¾“å‡º": 256, "å·ç§¯æ ¸": "mix", "å‚æ•°": 3*(64*64 + 64*64*3*3 + 64*256)*3},
                    {"å±‚": "BottleneckÃ—4", "è¾“å…¥": 256, "è¾“å‡º": 512, "å·ç§¯æ ¸": "mix", "å‚æ•°": 4*(128*256 + 128*128*3*3 + 128*512)},
                    {"å±‚": "...", "è¾“å…¥": "...", "è¾“å‡º": "...", "å·ç§¯æ ¸": "...", "å‚æ•°": 0},
                    {"å±‚": "FC", "è¾“å…¥": 2048, "è¾“å‡º": 1000, "å·ç§¯æ ¸": "-", "å‚æ•°": 2048*1000 + 1000},
                ]
            }
            
            config = preset_configs[network_type]
            total = sum(layer["å‚æ•°"] for layer in config)
            
            st.markdown(f"### ğŸ“Š {network_type} å‚æ•°ç»Ÿè®¡")
            st.metric("æ€»å‚æ•°é‡", f"{total/1e6:.1f}M")
            
            df = pd.DataFrame([
                {
                    "å±‚": layer["å±‚"],
                    "è¾“å…¥é€šé“": layer["è¾“å…¥"],
                    "è¾“å‡ºé€šé“": layer["è¾“å‡º"],
                    "å·ç§¯æ ¸": layer["å·ç§¯æ ¸"],
                    "å‚æ•°é‡": f"{layer['å‚æ•°']:,}" if isinstance(layer['å‚æ•°'], int) else "..."
                }
                for layer in config
            ])
            
            st.dataframe(df, use_container_width=True)
        
        st.success(r"""
        **CNNå‚æ•°çš„ç‰¹ç‚¹**:
        
        1. **å·ç§¯å±‚å‚æ•°å°‘**ï¼šæƒé‡å…±äº«ï¼Œå‚æ•°é‡ä¸è¾“å…¥å¤§å°æ— å…³
        2. **å…¨è¿æ¥å±‚å‚æ•°å¤š**ï¼šé€šå¸¸å 90%ä»¥ä¸Šå‚æ•°
        3. **ä¼˜åŒ–ç­–ç•¥**ï¼š
           - ç”¨Global Average Poolingæ›¿ä»£FC
           - æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆDepthwise Separableï¼‰
           - 1Ã—1å·ç§¯é™ç»´
        
        **ç»éªŒæ³•åˆ™**ï¼š
        - 3Ã—3å·ç§¯ï¼š9 Ã— C_in Ã— C_out
        - 1Ã—1å·ç§¯ï¼šC_in Ã— C_outï¼ˆé™ç»´åˆ©å™¨ï¼‰
        - FCå±‚ï¼šInput_dim Ã— Output_dimï¼ˆå¤§å¤´æ‰€åœ¨ï¼‰
        """)

    
    @staticmethod
    def _render_memory_calculator():
        """æ˜¾å­˜å ç”¨ä¼°ç®—"""
        st.markdown("### ğŸ’¾ æ˜¾å­˜å ç”¨ä¼°ç®—å™¨")
        
        st.markdown(r"""
        **è®­ç»ƒæ˜¾å­˜ç»„æˆ**ï¼š
        
        $$\text{Total Memory} = \text{Model} + \text{Optimizer} + \text{Gradients} + \text{Activations}$$
        """)
        
        with st.sidebar:
            st.markdown("#### æ¨¡å‹é…ç½®")
            param_count = st.number_input("å‚æ•°é‡ (M)", 1, 200000, 1000, 100)
            batch_size = st.number_input("Batch Size", 1, 256, 8, 1)
            seq_length = st.number_input("åºåˆ—é•¿åº¦", 128, 8192, 512, 128)
            
            st.markdown("#### è®­ç»ƒé…ç½®")
            optimizer = st.selectbox("ä¼˜åŒ–å™¨", ["Adam", "SGD", "AdamW"])
            precision = st.selectbox("ç²¾åº¦", ["FP32", "FP16", "BF16", "INT8"])
            gradient_checkpointing = st.checkbox("æ¢¯åº¦æ£€æŸ¥ç‚¹", value=False)
        
        # ç²¾åº¦å¯¹åº”çš„å­—èŠ‚æ•°
        precision_bytes = {
            "FP32": 4,
            "FP16": 2,
            "BF16": 2,
            "INT8": 1
        }
        
        bytes_per_param = precision_bytes[precision]
        param_count_actual = param_count * 1e6
        
        # 1. æ¨¡å‹å‚æ•°
        model_memory = param_count_actual * bytes_per_param
        
        # 2. æ¢¯åº¦
        gradient_memory = param_count_actual * bytes_per_param
        
        # 3. ä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer == "SGD":
            optimizer_memory = 0  # SGDæ— é¢å¤–çŠ¶æ€
        elif optimizer in ["Adam", "AdamW"]:
            # Adaméœ€è¦ä¸¤ä¸ªçŠ¶æ€ï¼šmomentumå’Œvarianceï¼Œéƒ½æ˜¯FP32
            optimizer_memory = param_count_actual * 4 * 2
        
        # 4. æ¿€æ´»å€¼ï¼ˆä¼°ç®—ï¼‰
        # å¯¹äºTransformerï¼Œæ¿€æ´»å€¼çº¦ä¸ºï¼šbatch_size Ã— seq_length Ã— hidden_dim Ã— n_layers Ã— å¸¸æ•°
        # è¿™é‡Œç®€åŒ–ä¼°ç®—
        activation_per_token = param_count_actual / 100  # ç²—ç•¥ä¼°è®¡
        activation_memory = batch_size * seq_length * activation_per_token * bytes_per_param
        
        if gradient_checkpointing:
            activation_memory *= 0.3  # æ¢¯åº¦æ£€æŸ¥ç‚¹å¯èŠ‚çº¦çº¦70%æ¿€æ´»å€¼å†…å­˜
        
        # æ€»æ˜¾å­˜
        total_memory = model_memory + gradient_memory + optimizer_memory + activation_memory
        
        # æ˜¾ç¤ºç»“æœ
        st.markdown("### ğŸ“Š æ˜¾å­˜åˆ†è§£")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»æ˜¾å­˜", f"{total_memory/1e9:.2f} GB")
        with col2:
            st.metric("æ¨¡å‹å‚æ•°", f"{model_memory/1e9:.2f} GB")
        with col3:
            st.metric("ä¼˜åŒ–å™¨", f"{optimizer_memory/1e9:.2f} GB")
        with col4:
            st.metric("æ¿€æ´»å€¼", f"{activation_memory/1e9:.2f} GB")
        
        # è¯¦ç»†åˆ†è§£è¡¨æ ¼
        breakdown_data = {
            "ç»„ä»¶": ["æ¨¡å‹å‚æ•°", "æ¢¯åº¦", "ä¼˜åŒ–å™¨çŠ¶æ€", "æ¿€æ´»å€¼", "æ€»è®¡"],
            "å¤§å° (GB)": [
                f"{model_memory/1e9:.2f}",
                f"{gradient_memory/1e9:.2f}",
                f"{optimizer_memory/1e9:.2f}",
                f"{activation_memory/1e9:.2f}",
                f"{total_memory/1e9:.2f}"
            ],
            "å æ¯”": [
                f"{model_memory/total_memory*100:.1f}%",
                f"{gradient_memory/total_memory*100:.1f}%",
                f"{optimizer_memory/total_memory*100:.1f}%",
                f"{activation_memory/total_memory*100:.1f}%",
                "100.0%"
            ]
        }
        
        df_breakdown = pd.DataFrame(breakdown_data)
        st.dataframe(df_breakdown, use_container_width=True)
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            specs=[[{"type": "pie"}, {"type": "bar"}]],
            subplot_titles=("æ˜¾å­˜å æ¯”", "å„ç»„ä»¶å¤§å°")
        )
        
        # é¥¼å›¾
        labels = ["æ¨¡å‹å‚æ•°", "æ¢¯åº¦", "ä¼˜åŒ–å™¨", "æ¿€æ´»å€¼"]
        values = [model_memory, gradient_memory, optimizer_memory, activation_memory]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        
        fig.add_trace(
            go.Pie(
                labels=labels,
                values=values,
                marker=dict(colors=colors),
                textinfo='label+percent'
            ),
            row=1, col=1
        )
        
        # æŸ±çŠ¶å›¾
        fig.add_trace(
            go.Bar(
                x=labels,
                y=[v/1e9 for v in values],
                marker_color=colors,
                text=[f"{v/1e9:.2f}GB" for v in values],
                textposition='outside'
            ),
            row=1, col=2
        )
        
        fig.update_layout(height=400, showlegend=False)
        fig.update_yaxes(title_text="æ˜¾å­˜ (GB)", row=1, col=2)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # GPUå»ºè®®
        st.markdown("### ğŸ¯ GPUé€‰å‹å»ºè®®")
        
        gpu_options = {
            "GPU": ["RTX 3090", "RTX 4090", "A100 40GB", "A100 80GB", "H100 80GB"],
            "æ˜¾å­˜": ["24 GB", "24 GB", "40 GB", "80 GB", "80 GB"],
            "æ˜¯å¦è¶³å¤Ÿ": [
                "âœ…" if total_memory/1e9 < 24 else "âŒ",
                "âœ…" if total_memory/1e9 < 24 else "âŒ",
                "âœ…" if total_memory/1e9 < 40 else "âŒ",
                "âœ…" if total_memory/1e9 < 80 else "âŒ",
                "âœ…" if total_memory/1e9 < 80 else "âŒ"
            ],
            "å»ºè®®": [
                "æ¶ˆè´¹çº§ï¼Œæ€§ä»·æ¯”é«˜" if total_memory/1e9 < 20 else "æ˜¾å­˜ä¸è¶³",
                "æœ€æ–°æ¶ˆè´¹çº§" if total_memory/1e9 < 20 else "æ˜¾å­˜ä¸è¶³",
                "ä¼ä¸šçº§ï¼Œç¨³å®š" if total_memory/1e9 < 35 else "æ˜¾å­˜ä¸è¶³",
                "å¤§æ¨¡å‹è®­ç»ƒ" if total_memory/1e9 < 75 else "æ˜¾å­˜ä¸è¶³",
                "æœ€å¼ºç®—åŠ›" if total_memory/1e9 < 75 else "è€ƒè™‘æ¨¡å‹å¹¶è¡Œ"
            ]
        }
        
        df_gpu = pd.DataFrame(gpu_options)
        st.dataframe(df_gpu, use_container_width=True)
        
        # ä¼˜åŒ–å»ºè®®
        st.markdown("### ğŸ’¡ æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### å‡å°‘æ¨¡å‹å‚æ•°æ˜¾å­˜")
            savings_fp16 = (model_memory - param_count_actual * 2) / 1e9
            savings_int8 = (model_memory - param_count_actual * 1) / 1e9
            
            st.write(f"- **FP16/BF16**: èŠ‚çœ {savings_fp16:.2f} GB")
            st.write(f"- **INT8é‡åŒ–**: èŠ‚çœ {savings_int8:.2f} GB")
            st.write(f"- **LoRAå¾®è°ƒ**: åªè®­ç»ƒå°‘é‡å‚æ•°")
        
        with col2:
            st.markdown("#### å‡å°‘æ¿€æ´»å€¼æ˜¾å­˜")
            savings_checkpoint = activation_memory * 0.7 / 1e9
            savings_batch = activation_memory * 0.5 / 1e9
            
            st.write(f"- **æ¢¯åº¦æ£€æŸ¥ç‚¹**: èŠ‚çœ {savings_checkpoint:.2f} GB")
            st.write(f"- **å‡å°batch_size**: èŠ‚çœ {savings_batch:.2f} GB")
            st.write(f"- **æ¢¯åº¦ç´¯ç§¯**: ä¿æŒæœ‰æ•ˆbatch_size")
        
        st.success(r"""
        **æ˜¾å­˜è®¡ç®—å…¬å¼ (FP32, Adam)**:
        
        $$\text{Memory} \approx \text{Params} \times (4 + 4 + 8) = 16 \times \text{Params}$$
        
        - 4 bytes: æ¨¡å‹å‚æ•°
        - 4 bytes: æ¢¯åº¦
        - 8 bytes: AdamçŠ¶æ€ (må’Œv)
        
        **é™ä½åˆ°FP16 + æ¢¯åº¦æ£€æŸ¥ç‚¹**:
        
        $$\text{Memory} \approx \text{Params} \times (2 + 2 + 8) = 12 \times \text{Params}$$
        
        **è®°å¿†æŠ€å·§**: FP32+Adam â‰ˆ 16Ã—å‚æ•°é‡ï¼ŒFP16 â‰ˆ ä¸€åŠ
        """)
    
    @staticmethod
    def _render_architecture_comparison():
        """æ¶æ„å¯¹æ¯”ï¼ˆGPT vs BERTï¼‰"""
        st.markdown("### âš”ï¸ æ¶æ„å¯¹æ¯”ï¼šGPT vs BERT")
        
        st.markdown("""
        **æ ¸å¿ƒåŒºåˆ«**ï¼š
        - **GPT**: å•å‘è¯­è¨€æ¨¡å‹ï¼ˆCausal/Decoder-onlyï¼‰
        - **BERT**: åŒå‘ç¼–ç å™¨ï¼ˆMasked/Encoder-onlyï¼‰
        """)
        
        with st.sidebar:
            st.markdown("#### æ¨¡å‹è§„æ¨¡")
            d_model = st.slider("d_model", 256, 2048, 768, 64)
            n_layers = st.slider("å±‚æ•°", 6, 48, 12, 1)
            vocab_size = st.slider("è¯è¡¨å¤§å°", 10000, 100000, 50000, 1000)
        
        # GPTè®¡ç®—
        gpt_params = vocab_size * d_model + n_layers * 12 * d_model**2 + d_model * vocab_size
        
        # BERTè®¡ç®—ï¼ˆé¢å¤–çš„segment embeddingå’Œposition embeddingï¼‰
        bert_params = (vocab_size + 512 + 2) * d_model + n_layers * 12 * d_model**2 + d_model * vocab_size
        
        st.markdown("### ğŸ“Š å‚æ•°é‡å¯¹æ¯”")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("GPT", f"{gpt_params/1e6:.1f}M")
        with col2:
            st.metric("BERT", f"{bert_params/1e6:.1f}M")
        with col3:
            diff_pct = (bert_params - gpt_params) / gpt_params * 100
            st.metric("å·®å¼‚", f"{diff_pct:.1f}%")
        
        # è¯¦ç»†å¯¹æ¯”è¡¨
        comparison_data = {
            "ç‰¹æ€§": [
                "æ³¨æ„åŠ›ç±»å‹",
                "è®­ç»ƒç›®æ ‡",
                "Embedding",
                "ä½ç½®ç¼–ç ",
                "é¢„è®­ç»ƒä»»åŠ¡",
                "ä¸‹æ¸¸ä»»åŠ¡",
                "å‚æ•°é‡",
                "å…¸å‹åº”ç”¨"
            ],
            "GPT (Decoder)": [
                "Causal (å•å‘)",
                "ä¸‹ä¸€ä¸ªtokené¢„æµ‹",
                "Token + Position",
                "å­¦ä¹ å¼",
                "Language Modeling",
                "ç”Ÿæˆä»»åŠ¡",
                f"{gpt_params/1e6:.1f}M",
                "æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯"
            ],
            "BERT (Encoder)": [
                "Bidirectional (åŒå‘)",
                "MLM + NSP",
                "Token + Segment + Position",
                "å­¦ä¹ å¼",
                "Masked LM + Next Sentence",
                "ç†è§£ä»»åŠ¡",
                f"{bert_params/1e6:.1f}M",
                "åˆ†ç±»ã€NERã€QA"
            ]
        }
        
        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison, use_container_width=True)
        
        # å¯è§†åŒ–attentionæ¨¡å¼
        st.markdown("### ğŸ‘ï¸ Attentionæ¨¡å¼å¯¹æ¯”")
        
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("GPT: Causal Attention", "BERT: Full Attention"),
            specs=[[{"type": "heatmap"}, {"type": "heatmap"}]]
        )
        
        # GPTçš„causal mask
        seq_len = 8
        gpt_mask = np.tril(np.ones((seq_len, seq_len)))
        
        # BERTçš„full attention
        bert_mask = np.ones((seq_len, seq_len))
        
        fig.add_trace(
            go.Heatmap(
                z=gpt_mask,
                colorscale=[[0, 'white'], [1, 'blue']],
                showscale=False,
                text=[[f"T{i+1}" for i in range(seq_len)] for _ in range(seq_len)],
                texttemplate="%{text}",
                hovertemplate='Query: %{y}<br>Key: %{x}<extra></extra>'
            ),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Heatmap(
                z=bert_mask,
                colorscale=[[0, 'white'], [1, 'green']],
                showscale=False,
                text=[[f"T{i+1}" for i in range(seq_len)] for _ in range(seq_len)],
                texttemplate="%{text}",
                hovertemplate='Query: %{y}<br>Key: %{x}<extra></extra>'
            ),
            row=1, col=2
        )
        
        fig.update_xaxes(title_text="Key Position", row=1, col=1)
        fig.update_yaxes(title_text="Query Position", row=1, col=1)
        fig.update_xaxes(title_text="Key Position", row=1, col=2)
        fig.update_yaxes(title_text="Query Position", row=1, col=2)
        fig.update_layout(height=400)
        
        st.plotly_chart(fig, use_container_width=True)
        
        st.info("""
        **Attentionæ¨¡å¼å«ä¹‰**ï¼š
        
        - **GPT (Causal)**: è“è‰²è¡¨ç¤ºå¯ä»¥attendï¼Œç™½è‰²è¡¨ç¤ºmaskæ‰
          - åªèƒ½çœ‹åˆ°å½“å‰å’Œä¹‹å‰çš„token
          - ä¿è¯ç”Ÿæˆçš„è‡ªå›å½’æ€§è´¨
        
        - **BERT (Full)**: å…¨ç»¿ï¼Œå¯ä»¥çœ‹åˆ°æ‰€æœ‰token
          - åŒå‘ä¸Šä¸‹æ–‡ä¿¡æ¯
          - æ›´é€‚åˆç†è§£ä»»åŠ¡
        """)
        
        st.success(r"""
        **é€‰æ‹©å»ºè®®**:
        
        | ä»»åŠ¡ç±»å‹ | æ¨èæ¶æ„ | åŸå›  |
        |---------|---------|------|
        | æ–‡æœ¬ç”Ÿæˆ | GPT | è‡ªå›å½’ç”Ÿæˆ |
        | æ–‡æœ¬åˆ†ç±» | BERT | åŒå‘ç†è§£ |
        | é—®ç­”ç³»ç»Ÿ | BERT | éœ€è¦å…¨å±€ä¿¡æ¯ |
        | å¯¹è¯ç³»ç»Ÿ | GPT | ç”Ÿæˆè¿è´¯å›å¤ |
        | å®ä½“è¯†åˆ« | BERT | éœ€è¦ä¸Šä¸‹æ–‡ |
        | ä»£ç ç”Ÿæˆ | GPT | è‡ªå›å½’ç”Ÿæˆ |
        
        **ç°ä»£è¶‹åŠ¿**: Decoder-only (GPTé£æ ¼) ç»Ÿä¸€ç”Ÿæˆå’Œç†è§£ä»»åŠ¡
        """)

    
    @staticmethod
    def _render_memory_anatomy():
        """è®­ç»ƒæ˜¾å­˜è§£å‰–"""
        st.markdown("### ğŸ”¬ è®­ç»ƒæ˜¾å­˜è§£å‰–å­¦")
        
        st.markdown(r"""
        **æ·±å…¥å‰–æ**: è®­ç»ƒä¸€ä¸ª7Bæ¨¡å‹åˆ°åº•éœ€è¦å¤šå°‘æ˜¾å­˜ï¼Ÿ
        
        ä»¥ **LLaMA 7B** ä¸ºä¾‹ï¼š
        """)
        
        with st.sidebar:
            st.markdown("#### æ¨¡å‹å‚æ•°")
            model_size_b = st.selectbox(
                "æ¨¡å‹å¤§å°",
                ["7B (LLaMA)", "13B (LLaMA)", "70B (LLaMA)", "è‡ªå®šä¹‰"],
                index=0
            )
            
            if model_size_b == "è‡ªå®šä¹‰":
                param_count = st.number_input("å‚æ•°é‡ (B)", 1, 200, 7)
            else:
                param_count = int(model_size_b.split('B')[0])
            
            st.markdown("#### è®­ç»ƒé…ç½®")
            training_mode = st.selectbox(
                "è®­ç»ƒæ¨¡å¼",
                ["å…¨å‚æ•°å¾®è°ƒ", "LoRA", "QLoRA", "ä»…æ¨ç†"]
            )
            precision = st.selectbox("ç²¾åº¦", ["FP32", "FP16", "BF16"], index=1)
            batch_size = st.number_input("Batch Size", 1, 64, 4, 1)
            seq_length = st.number_input("åºåˆ—é•¿åº¦", 512, 4096, 2048, 512)
        
        param_count_actual = param_count * 1e9
        
        # ç²¾åº¦å­—èŠ‚æ•°
        bytes_map = {"FP32": 4, "FP16": 2, "BF16": 2}
        bytes_per_param = bytes_map[precision]
        
        # 1. æ¨¡å‹å‚æ•°
        model_weights = param_count_actual * bytes_per_param / 1e9
        
        # 2. æ¢¯åº¦
        if training_mode == "ä»…æ¨ç†":
            gradients = 0
            optimizer_states = 0
        elif training_mode == "LoRA":
            # LoRAåªè®­ç»ƒå¾ˆå°‘çš„å‚æ•°ï¼ˆå‡è®¾1%ï¼‰
            trainable_params = param_count_actual * 0.01
            gradients = trainable_params * bytes_per_param / 1e9
            optimizer_states = trainable_params * 8 / 1e9  # AdamçŠ¶æ€
        elif training_mode == "QLoRA":
            # QLoRA + 4bité‡åŒ–
            model_weights = param_count_actual * 0.5 / 1e9  # 4bit â‰ˆ 0.5 bytes
            trainable_params = param_count_actual * 0.01
            gradients = trainable_params * bytes_per_param / 1e9
            optimizer_states = trainable_params * 8 / 1e9
        else:  # å…¨å‚æ•°
            gradients = param_count_actual * bytes_per_param / 1e9
            optimizer_states = param_count_actual * 8 / 1e9  # Adam: m + v
        
        # 3. æ¿€æ´»å€¼ï¼ˆä¸batch sizeæˆæ­£æ¯”ï¼‰
        # ç²—ç•¥ä¼°è®¡ï¼šæ¯ä¸ªtokençº¦éœ€ hidden_dim * n_layers * å¸¸æ•° çš„æ¿€æ´»å€¼
        hidden_dim = 4096  # LLaMA 7Bçš„éšè—ç»´åº¦
        n_layers = 32  # LLaMA 7Bçš„å±‚æ•°
        activation_per_token = hidden_dim * n_layers * 20 * bytes_per_param / 1e9
        activations = batch_size * seq_length * activation_per_token
        
        # KV Cache (æ¨ç†æ—¶)
        if training_mode == "ä»…æ¨ç†":
            kv_cache = 2 * n_layers * hidden_dim * seq_length * bytes_per_param * batch_size / 1e9
        else:
            kv_cache = 0
        
        # æ€»æ˜¾å­˜
        total_memory = model_weights + gradients + optimizer_states + activations + kv_cache
        
        # æ˜¾ç¤ºç»“æœ
        st.markdown("### ğŸ“Š æ˜¾å­˜è¯¦ç»†åˆ†è§£")
        
        cols = st.columns(5)
        with cols[0]:
            st.metric("æ€»è®¡", f"{total_memory:.1f} GB")
        with cols[1]:
            st.metric("æ¨¡å‹", f"{model_weights:.1f} GB")
        with cols[2]:
            st.metric("æ¢¯åº¦", f"{gradients:.1f} GB")
        with cols[3]:
            st.metric("ä¼˜åŒ–å™¨", f"{optimizer_states:.1f} GB")
        with cols[4]:
            st.metric("æ¿€æ´»å€¼", f"{activations:.1f} GB")
        
        # è¯¦ç»†è¡¨æ ¼
        breakdown = []
        
        if model_weights > 0:
            breakdown.append(["æ¨¡å‹æƒé‡", f"{model_weights:.2f}", f"{model_weights/total_memory*100:.1f}%", 
                            f"{param_count}B Ã— {bytes_per_param} bytes"])
        if gradients > 0:
            breakdown.append(["æ¢¯åº¦", f"{gradients:.2f}", f"{gradients/total_memory*100:.1f}%",
                            "ä¸æ¨¡å‹æƒé‡ç›¸åŒ"])
        if optimizer_states > 0:
            breakdown.append(["ä¼˜åŒ–å™¨çŠ¶æ€", f"{optimizer_states:.2f}", f"{optimizer_states/total_memory*100:.1f}%",
                            "Adam: momentum + variance"])
        if activations > 0:
            breakdown.append(["æ¿€æ´»å€¼", f"{activations:.2f}", f"{activations/total_memory*100:.1f}%",
                            f"Batch {batch_size} Ã— Seq {seq_length}"])
        if kv_cache > 0:
            breakdown.append(["KV Cache", f"{kv_cache:.2f}", f"{kv_cache/total_memory*100:.1f}%",
                            "æ¨ç†æ—¶ç¼“å­˜Key/Value"])
        
        df = pd.DataFrame(breakdown, columns=["ç»„ä»¶", "å¤§å° (GB)", "å æ¯”", "è¯´æ˜"])
        st.dataframe(df, use_container_width=True)
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=1, cols=2,
            specs=[[{"type": "pie"}, {"type": "bar"}]],
            subplot_titles=("æ˜¾å­˜å æ¯”", "å„ç»„ä»¶å¤§å° (GB)")
        )
        
        labels = [item[0] for item in breakdown]
        values = [float(item[1]) for item in breakdown]
        
        fig.add_trace(
            go.Pie(
                labels=labels,
                values=values,
                textinfo='label+percent'
            ),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Bar(
                x=labels,
                y=values,
                text=[f"{v:.1f}GB" for v in values],
                textposition='outside'
            ),
            row=1, col=2
        )
        
        fig.update_layout(height=400, showlegend=False)
        fig.update_yaxes(title_text="æ˜¾å­˜ (GB)", row=1, col=2)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å®é™…æ¡ˆä¾‹å¯¹æ¯”
        st.markdown("### ğŸ¯ å®é™…æ¡ˆä¾‹ï¼šä¸åŒè®­ç»ƒæ–¹å¼å¯¹æ¯”")
        
        cases = []
        
        # å…¨å‚æ•°FP32
        full_fp32 = param_count * (4 + 4 + 8) + activations
        cases.append(["å…¨å‚æ•° FP32 + Adam", f"{full_fp32:.1f}", "âŒ å¤ªå¤§", "4Ã—16GB A100"])
        
        # å…¨å‚æ•°FP16
        full_fp16 = param_count * (2 + 2 + 8) + activations
        cases.append(["å…¨å‚æ•° FP16 + Adam", f"{full_fp16:.1f}", 
                     "âœ…" if full_fp16 < 40 else "âš ï¸", "2Ã—40GB A100"])
        
        # LoRA
        lora_mem = param_count * 2 + param_count * 0.01 * (2 + 8) + activations
        cases.append(["LoRA (1% å‚æ•°)", f"{lora_mem:.1f}", "âœ…", "1Ã—24GB GPU"])
        
        # QLoRA
        qlora_mem = param_count * 0.5 + param_count * 0.01 * (2 + 8) + activations
        cases.append(["QLoRA (4bit + 1%)", f"{qlora_mem:.1f}", "âœ…", "1Ã—16GB GPU"])
        
        # ä»…æ¨ç†
        inference_mem = param_count * 2 + kv_cache
        cases.append(["ä»…æ¨ç† FP16", f"{inference_mem:.1f}", "âœ…", "æ¶ˆè´¹çº§GPU"])
        
        df_cases = pd.DataFrame(cases, columns=["æ¨¡å¼", "æ˜¾å­˜ (GB)", "å¯è¡Œæ€§", "æ¨èç¡¬ä»¶"])
        st.dataframe(df_cases, use_container_width=True)
        
        st.success(f"""
        **{param_count}B æ¨¡å‹è®­ç»ƒå»ºè®®**:
        
        1. **å…¨å‚æ•°å¾®è°ƒ** (Full Fine-tuning):
           - æ˜¾å­˜éœ€æ±‚: ~{full_fp16:.0f} GB
           - éœ€è¦å¤šå¡å¹¶è¡Œ (è‡³å°‘2Ã—A100)
           - é€‚åˆå¤§å‚å’Œç ”ç©¶æœºæ„
        
        2. **LoRAå¾®è°ƒ**:
           - æ˜¾å­˜éœ€æ±‚: ~{lora_mem:.0f} GB
           - å•å¡24GBå¯æå®š
           - æ•ˆæœæ¥è¿‘å…¨å‚æ•°ï¼Œæ¨èï¼
        
        3. **QLoRAå¾®è°ƒ**:
           - æ˜¾å­˜éœ€æ±‚: ~{qlora_mem:.0f} GB
           - æ¶ˆè´¹çº§GPUå¯ç”¨
           - é‡åŒ–æŸå¤±å¾ˆå°
        
        4. **ä»…æ¨ç†**:
           - æ˜¾å­˜éœ€æ±‚: ~{inference_mem:.0f} GB
           - ç¬”è®°æœ¬/æ¶ˆè´¹çº§GPU
           - é…åˆé‡åŒ–æ›´çœ
        """)
    
    @staticmethod
    def _render_precision_quantization():
        """æ··åˆç²¾åº¦ä¸é‡åŒ–"""
        st.markdown("### âš¡ æ··åˆç²¾åº¦ä¸é‡åŒ–")
        
        st.markdown("""
        **æ ¸å¿ƒæ€æƒ³**: ç”¨æ›´å°‘çš„bitè¡¨ç¤ºæ•°å­—ï¼ŒèŠ‚çœæ˜¾å­˜å’Œè®¡ç®—
        """)
        
        # ç²¾åº¦å¯¹æ¯”
        st.markdown("### ğŸ“Š ä¸åŒç²¾åº¦å¯¹æ¯”")
        
        precision_data = {
            "ç²¾åº¦": ["FP32", "FP16", "BF16", "INT8", "INT4"],
            "å­—èŠ‚æ•°": [4, 2, 2, 1, 0.5],
            "æ•°å€¼èŒƒå›´": [
                "Â±3.4e38",
                "Â±65504",
                "Â±3.4e38",
                "0-255",
                "0-15"
            ],
            "ç²¾åº¦": [
                "7ä½æœ‰æ•ˆæ•°å­—",
                "3ä½æœ‰æ•ˆæ•°å­—",
                "7ä½æœ‰æ•ˆæ•°å­—(èŒƒå›´å¤§)",
                "æ•´æ•°",
                "æ•´æ•°"
            ],
            "ç”¨é€”": [
                "ä¼ ç»Ÿè®­ç»ƒ",
                "æ··åˆç²¾åº¦è®­ç»ƒ",
                "ç¨³å®šçš„æ··åˆç²¾åº¦",
                "æ¨ç†åŠ é€Ÿ",
                "æé™å‹ç¼©"
            ],
            "ç›¸å¯¹é€Ÿåº¦": ["1Ã—", "2Ã—", "2Ã—", "4Ã—", "8Ã—"]
        }
        
        df_precision = pd.DataFrame(precision_data)
        st.dataframe(df_precision, use_container_width=True)
        
        # å¯è§†åŒ–æ˜¾å­˜èŠ‚çœ
        st.markdown("### ğŸ’¾ æ˜¾å­˜èŠ‚çœå¯¹æ¯”")
        
        with st.sidebar:
            st.markdown("#### æ¨¡å‹é…ç½®")
            model_params = st.slider("æ¨¡å‹å‚æ•° (B)", 1, 100, 7)
        
        params = model_params * 1e9
        
        mem_fp32 = params * 4 / 1e9
        mem_fp16 = params * 2 / 1e9
        mem_int8 = params * 1 / 1e9
        mem_int4 = params * 0.5 / 1e9
        
        fig = go.Figure()
        
        precisions = ["FP32", "FP16/BF16", "INT8", "INT4"]
        memories = [mem_fp32, mem_fp16, mem_int8, mem_int4]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        
        fig.add_trace(go.Bar(
            x=precisions,
            y=memories,
            text=[f"{m:.1f}GB<br>({m/mem_fp32*100:.0f}%)" for m in memories],
            textposition='outside',
            marker_color=colors
        ))
        
        fig.update_layout(
            title=f"{model_params}B æ¨¡å‹åœ¨ä¸åŒç²¾åº¦ä¸‹çš„æ˜¾å­˜å ç”¨",
            xaxis_title="ç²¾åº¦",
            yaxis_title="æ˜¾å­˜ (GB)",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # é‡åŒ–æ–¹æ³•å¯¹æ¯”
        st.markdown("### ğŸ”§ é‡åŒ–æ–¹æ³•")
        
        quant_methods = {
            "æ–¹æ³•": [
                "æ··åˆç²¾åº¦ (AMP)",
                "è®­ç»ƒåé‡åŒ– (PTQ)",
                "é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)",
                "QLoRA",
                "GPTQ"
            ],
            "ç²¾åº¦": [
                "FP16/BF16",
                "INT8",
                "INT8",
                "4bit NF4",
                "3-4bit"
            ],
            "å‡†ç¡®åº¦æŸå¤±": [
                "< 0.1%",
                "1-2%",
                "< 0.5%",
                "< 1%",
                "1-3%"
            ],
            "å®ç°éš¾åº¦": [
                "ç®€å•",
                "ç®€å•",
                "ä¸­ç­‰",
                "ä¸­ç­‰",
                "å¤æ‚"
            ],
            "é€‚ç”¨åœºæ™¯": [
                "è®­ç»ƒåŠ é€Ÿ",
                "æ¨ç†åŠ é€Ÿ",
                "é«˜ç²¾åº¦æ¨ç†",
                "å¤§æ¨¡å‹å¾®è°ƒ",
                "æé™å‹ç¼©"
            ]
        }
        
        df_quant = pd.DataFrame(quant_methods)
        st.dataframe(df_quant, use_container_width=True)
        
        # å®æˆ˜å»ºè®®
        st.markdown("### ğŸ’¡ å®æˆ˜å»ºè®®")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### è®­ç»ƒé˜¶æ®µ")
            st.info("""
            1. **è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)**
               ```python
               from torch.cuda.amp import autocast
               with autocast():
                   output = model(input)
               ```
               - å¼€ç®±å³ç”¨ï¼Œ2å€åŠ é€Ÿ
               - å‡ ä¹æ— ç²¾åº¦æŸå¤±
            
            2. **æ¢¯åº¦ç´¯ç§¯**
               - å‡å°batch_size
               - ç´¯ç§¯å¤šæ­¥åæ›´æ–°
               - èŠ‚çœæ¿€æ´»å€¼æ˜¾å­˜
            
            3. **æ¢¯åº¦æ£€æŸ¥ç‚¹**
               - é‡è®¡ç®—ä»£æ›¿å­˜å‚¨
               - èŠ‚çœ70%æ¿€æ´»å€¼æ˜¾å­˜
               - é€Ÿåº¦é™ä½20-30%
            """)
        
        with col2:
            st.markdown("#### æ¨ç†é˜¶æ®µ")
            st.info("""
            1. **INT8é‡åŒ–**
               ```python
               model = torch.quantization.quantize_dynamic(
                   model, {torch.nn.Linear}, dtype=torch.qint8
               )
               ```
               - 4å€æ˜¾å­˜èŠ‚çœ
               - 2-4å€é€Ÿåº¦æå‡
            
            2. **æ¨¡å‹è’¸é¦**
               - ç”¨å°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹
               - ä¿ç•™90%æ€§èƒ½
               - 10å€å‚æ•°å‡å°‘
            
            3. **å‰ªæ**
               - å»é™¤ä¸é‡è¦çš„æƒé‡
               - ç»“æ„åŒ–å‰ªææ›´å¿«
               - é…åˆé‡åŒ–æ•ˆæœæ›´å¥½
            """)
        
        st.success(f"""
        **{model_params}B æ¨¡å‹ä¼˜åŒ–è·¯å¾„**:
        
        1. **å¼€å‘é˜¶æ®µ**: FP32 (ç²¾åº¦æœ€é«˜)
        2. **è®­ç»ƒé˜¶æ®µ**: FP16/BF16 AMP (2å€åŠ é€Ÿ)
        3. **å¾®è°ƒé˜¶æ®µ**: LoRA/QLoRA (æ˜¾å­˜èŠ‚çœ)
        4. **éƒ¨ç½²é˜¶æ®µ**: INT8 (4å€æ˜¾å­˜èŠ‚çœ)
        5. **è¾¹ç¼˜è®¾å¤‡**: INT4 + å‰ªæ (æé™ä¼˜åŒ–)
        
        **è®°ä½**: ç²¾åº¦é™ä½æ˜¯èŠ‚çœèµ„æºçš„ä¸»è¦æ‰‹æ®µï¼
        """)


# æ³¨å†Œåˆ°__all__
__all__ = ['InteractiveDimensionsParameters']

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
