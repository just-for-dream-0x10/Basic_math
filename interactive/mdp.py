"""
äº¤äº’å¼é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)å¯è§†åŒ–
ä¸¥æ ¼æŒ‰ç…§ 16.MDP.md ä¸­çš„å…¬å¼å®ç°
"""

import streamlit as st
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.colors import ListedColormap
import warnings
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥commonæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from common.error_handler import safe_render
from common.quiz_system import QuizSystem, QuizTemplates

warnings.filterwarnings('ignore')


class InteractiveMDP:
    """äº¤äº’å¼é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å¯è§†åŒ–"""
    
    @staticmethod
    @safe_render

    def render():
        st.subheader("ğŸ¯ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸è´å°”æ›¼æ–¹ç¨‹")
        st.markdown(r"""
        **æ ¸å¿ƒæ€æƒ³**: ä»é¢„æµ‹åˆ°å†³ç­–çš„èŒƒå¼è½¬ç§»ï¼Œæ™ºèƒ½ä½“é€šè¿‡åŠ¨ä½œæ”¹å˜ç¯å¢ƒå¹¶æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±
        
        å…³é”®æ¦‚å¿µï¼š
        - **äº”å…ƒç»„**: $M = \langle S, A, P, R, \gamma \rangle$
        - **è´å°”æ›¼æ–¹ç¨‹**: $V^*(s) = \max_{a} \sum_{s'} P(s' \mid s, a) [R(s, a, s') + \gamma V^*(s')]$
        - **Q-Learning**: $Q(s,a) \leftarrow Q(s,a) + \alpha[R + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
        - **ç­–ç•¥æ¢¯åº¦**: $\nabla_{\theta} J(\theta) = \mathbb{E}[\sum_{t} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \cdot G_t]$
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ“Š å¯è§†åŒ–é€‰æ‹©")
            viz_type = st.selectbox("é€‰æ‹©å¯è§†åŒ–ç±»å‹", 
                ["MDPåŸºç¡€æ¦‚å¿µ", "ä»·å€¼è¿­ä»£ç®—æ³•", "Q-Learning", "ç­–ç•¥æ¢¯åº¦"])
            
            st.markdown("### ğŸ›ï¸ å‚æ•°è°ƒæ•´")
        
        if viz_type == "MDPåŸºç¡€æ¦‚å¿µ":
            InteractiveMDP._render_mdp_basics()
        elif viz_type == "ä»·å€¼è¿­ä»£ç®—æ³•":
            InteractiveMDP._render_value_iteration()
        elif viz_type == "Q-Learning":
            InteractiveMDP._render_q_learning()
        elif viz_type == "ç­–ç•¥æ¢¯åº¦":
            InteractiveMDP._render_policy_gradient()
    

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        quiz_system = QuizSystem("mdp")
        quizzes = QuizTemplates.get_mdp_quizzes()
        quiz_system.render_quiz(quizzes)
    @staticmethod
    def _render_mdp_basics():
        """MDPåŸºç¡€æ¦‚å¿µæ¼”ç¤º"""
        st.markdown("### ğŸŒ MDPäº”å…ƒç»„ï¼šä¸–ç•Œè§‚çš„æ•°å­¦å»ºæ¨¡")
        
        st.latex(r"""
        M = \langle S, A, P, R, \gamma \rangle
        """)
        
        with st.sidebar:
            grid_size = st.slider("ç½‘æ ¼ä¸–ç•Œå¤§å°", 3, 8, 5, 1)
            gamma = st.slider("æŠ˜æ‰£å› å­ Î³", 0.0, 1.0, 0.9, 0.05)
            show_probabilities = st.checkbox("æ˜¾ç¤ºè½¬ç§»æ¦‚ç‡", value=True)
            show_rewards = st.checkbox("æ˜¾ç¤ºå¥–åŠ±å€¼", value=True)
        
        # åˆ›å»ºç½‘æ ¼ä¸–ç•Œ
        np.random.seed(42)
        grid = np.zeros((grid_size, grid_size))
        
        # éšæœºæ”¾ç½®ç‰¹æ®Šæ ¼å­
        # ç»ˆç‚¹ï¼ˆå¥–åŠ±+10ï¼‰
        grid[grid_size-1, grid_size-1] = 10
        # é™·é˜±ï¼ˆå¥–åŠ±-10ï¼‰
        trap_positions = np.random.choice(grid_size*grid_size-2, 2, replace=False)
        for pos in trap_positions:
            r, c = pos // grid_size, pos % grid_size
            if r != 0 or c != 0:  # ä¸åœ¨èµ·ç‚¹
                grid[r, c] = -10
        # å¢™å£ï¼ˆä¸å¯é€šè¿‡ï¼‰
        wall_positions = np.random.choice(grid_size*grid_size-4, 3, replace=False)
        for pos in wall_positions:
            r, c = pos // grid_size, pos % grid_size
            if r != 0 or c != 0 and (r != grid_size-1 or c != grid_size-1):
                grid[r, c] = -1
        
        # å®šä¹‰åŠ¨ä½œ
        actions = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
        action_deltas = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        
        # è®¡ç®—è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±
        def get_transition_and_reward(state, action):
            r, c = state
            dr, dc = action_deltas[action]
            new_r, new_c = r + dr, c + dc
            
            # æ£€æŸ¥è¾¹ç•Œå’Œå¢™å£
            if (0 <= new_r < grid_size and 0 <= new_c < grid_size and 
                grid[new_r, new_c] != -1):
                return (new_r, new_c), grid[new_r, new_c]
            else:
                # æ’å¢™æˆ–å‡ºç•Œï¼Œç•™åœ¨åŸåœ°
                return (r, c), -1
        
        # å¯è§†åŒ–ç½‘æ ¼ä¸–ç•Œ
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=["ç½‘æ ¼ä¸–ç•Œ", "è½¬ç§»æ¦‚ç‡ç¤ºä¾‹"],
            specs=[[{"type": "heatmap"}, {"type": "bar"}]]
        )
        
        # ç½‘æ ¼ä¸–ç•Œçƒ­åŠ›å›¾
        grid_display = grid.copy()
        grid_display[grid_display == -1] = 0  # å¢™å£æ˜¾ç¤ºä¸ºç°è‰²
        
        fig.add_trace(
            go.Heatmap(
                z=grid_display,
                colorscale='RdYlGn',
                showscale=True,
                colorbar=dict(title="å¥–åŠ±å€¼"),
                text=np.array([['å¢™å£' if x == -1 else f'{x:.0f}' if x != 0 else '' 
                              for x in row] for row in grid]),
                texttemplate="%{text}",
                textfont={"size": 12}
            ),
            row=1, col=1
        )
        
        # è½¬ç§»æ¦‚ç‡ç¤ºä¾‹ï¼ˆä»ä¸­å¿ƒç‚¹ï¼‰
        center_state = (grid_size // 2, grid_size // 2)
        transition_probs = []
        action_labels = []
        
        for i, (action, (dr, dc)) in enumerate(zip(actions, action_deltas)):
            new_state, reward = get_transition_and_reward(center_state, i)
            # ç®€åŒ–çš„è½¬ç§»æ¦‚ç‡ï¼ˆå®é™…åº”è¯¥æ›´å¤æ‚ï¼‰
            prob = 0.8 if grid[new_state[0], new_state[1]] != -1 else 0.0
            
            if show_probabilities:
                transition_probs.append(prob)
                action_labels.append(f'{action}\\nP={prob:.1f}')
        
        if show_probabilities:
            fig.add_trace(
                go.Bar(
                    x=action_labels,
                    y=transition_probs,
                    marker_color='lightblue',
                    name='è½¬ç§»æ¦‚ç‡'
                ),
                row=1, col=2
            )
        
        fig.update_layout(
            title=f"MDPç½‘æ ¼ä¸–ç•Œ (Î³={gamma})",
            height=500,
            showlegend=False
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # é©¬å°”å¯å¤«æ€§è´¨æ¼”ç¤º
        st.markdown("### ğŸ”„ é©¬å°”å¯å¤«æ€§è´¨")
        
        st.markdown("""
        **æ ¸å¿ƒå‡è®¾**: æœªæ¥åªå–å†³äºç°åœ¨ï¼Œä¸è¿‡å»æ— å…³
        
        $P(S_{t+1} | S_t, S_{t-1}, ..., S_0) = P(S_{t+1} | S_t)$
        
        è¿™æ„å‘³ç€çŠ¶æ€ $S_t$ å¿…é¡»åŒ…å«å†³ç­–æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚
        """)
        
        # çŠ¶æ€åºåˆ—æ¼”ç¤º
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨**")
            st.write("âœ… å›½é™…è±¡æ£‹ï¼šå½“å‰æ£‹å±€åŒ…å«æ‰€æœ‰ä¿¡æ¯")
            st.write("âœ… å¯¼èˆªï¼šå½“å‰ä½ç½®å’Œç›®æ ‡ä½ç½®")
            st.write("âœ… é‡‘èï¼šå½“å‰æŠ•èµ„ç»„åˆ")
        
        with col2:
            st.markdown("**ä¸æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨**")
            st.write("âŒ POMDPï¼šéƒ¨åˆ†å¯è§‚æµ‹ç³»ç»Ÿ")
            st.write("âŠ éœ€è¦å†å²ä¿¡æ¯æ‰èƒ½å†³ç­–")
            st.write("âŠ å¡ç‰Œæ¸¸æˆï¼šä¸çŸ¥é“å¯¹æ‰‹æ‰‹ç‰Œ")
        
        # æŠ˜æ‰£å› å­çš„å½±å“
        st.markdown("### â° æŠ˜æ‰£å› å­çš„å½±å“")
        
        gamma_values = [0.0, 0.5, 0.9, 0.99]
        future_values = []
        
        for g in gamma_values:
            # è®¡ç®—æœªæ¥10æ­¥çš„æŠ˜ç°å¥–åŠ±
            discounted_sum = sum([g**i for i in range(10)])
            future_values.append(discounted_sum)
        
        fig_gamma = go.Figure()
        fig_gamma.add_trace(
            go.Scatter(
                x=gamma_values,
                y=future_values,
                mode='lines+markers',
                name='æŠ˜ç°æ€»å’Œ',
                line=dict(width=3),
                marker=dict(size=8)
            )
        )
        
        fig_gamma.update_layout(
            title="æŠ˜æ‰£å› å­å¯¹æœªæ¥å¥–åŠ±çš„å½±å“",
            xaxis_title="æŠ˜æ‰£å› å­ Î³",
            yaxis_title="æœªæ¥10æ­¥æŠ˜ç°å¥–åŠ±æ€»å’Œ",
            height=400
        )
        
        st.plotly_chart(fig_gamma, use_container_width=True)
        
        st.info("""
        **æŠ˜æ‰£å› å­çš„å“²å­¦å«ä¹‰**ï¼š
        - Î³ = 0ï¼šå®Œå…¨çŸ­è§†ï¼Œåªçœ‹å½“å‰å¥–åŠ±
        - Î³ = 0.5ï¼šå¹³è¡¡å½“å‰å’Œæœªæ¥
        - Î³ = 0.9ï¼šé‡è§†é•¿æœŸæ”¶ç›Š
        - Î³ â†’ 1ï¼šæåº¦è¿œè§ï¼Œè€ƒè™‘åƒç§‹ä¸‡ä»£
        """)
    
    @staticmethod
    def _render_value_iteration():
        """ä»·å€¼è¿­ä»£ç®—æ³•æ¼”ç¤º"""
        st.markdown("### ğŸ§  ä»·å€¼è¿­ä»£ï¼šè´å°”æ›¼æ–¹ç¨‹çš„æ•°å€¼æ±‚è§£")
        
        st.latex(r"""
        V_{k+1}(s) = \max_{a} \sum_{s'} P(s' \mid s, a) [R(s, a, s') + \gamma V_k(s')]
        """)
        
        with st.sidebar:
            grid_size = st.slider("ç½‘æ ¼å¤§å°", 4, 8, 5, 1)
            gamma = st.slider("æŠ˜æ‰£å› å­", 0.5, 0.99, 0.9, 0.01)
            max_iterations = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 10, 100, 50, 5)
            convergence_threshold = st.slider("æ”¶æ•›é˜ˆå€¼", 0.001, 0.1, 0.01, 0.001)
            show_convergence = st.checkbox("æ˜¾ç¤ºæ”¶æ•›è¿‡ç¨‹", value=True)
        
        # åˆ›å»ºç½‘æ ¼ä¸–ç•Œ
        grid = np.zeros((grid_size, grid_size))
        grid[grid_size-1, grid_size-1] = 10  # ç»ˆç‚¹
        grid[1, 1] = -10  # é™·é˜±
        grid[2, 2] = -10  # é™·é˜±
        
        # å¢™å£
        walls = [(1, 2), (3, 1)]
        for wall in walls:
            if wall[0] < grid_size and wall[1] < grid_size:
                grid[wall[0], wall[1]] = -1
        
        # ä»·å€¼è¿­ä»£ç®—æ³•
        def value_iteration(grid, gamma, max_iter, threshold):
            rows, cols = grid.shape
            V = np.zeros((rows, cols))
            policy = np.zeros((rows, cols), dtype=int)
            history = []
            
            actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # ä¸Šä¸‹å·¦å³
            action_names = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
            
            for iteration in range(max_iter):
                V_new = np.copy(V)
                max_change = 0
                
                for i in range(rows):
                    for j in range(cols):
                        if grid[i, j] == -1:  # å¢™å£
                            continue
                        
                        if i == rows-1 and j == cols-1:  # ç»ˆç‚¹
                            V_new[i, j] = 0
                            continue
                        
                        best_value = float('-inf')
                        best_action = 0
                        
                        for action_idx, (di, dj) in enumerate(actions):
                            ni, nj = i + di, j + dj
                            
                            # æ£€æŸ¥è¾¹ç•Œå’Œå¢™å£
                            if (0 <= ni < rows and 0 <= nj < cols and 
                                grid[ni, nj] != -1):
                                reward = grid[ni, nj]
                                value = reward + gamma * V[ni, nj]
                            else:
                                # æ’å¢™
                                value = -1 + gamma * V[i, j]
                            
                            if value > best_value:
                                best_value = value
                                best_action = action_idx
                        
                        V_new[i, j] = best_value
                        policy[i, j] = best_action
                        max_change = max(max_change, abs(V_new[i, j] - V[i, j]))
                
                history.append(np.copy(V_new))
                V = V_new
                
                if max_change < threshold:
                    break
            
            return V, policy, history
        
        # è¿è¡Œä»·å€¼è¿­ä»£
        V, policy, history = value_iteration(grid, gamma, max_iterations, convergence_threshold)
        
        # å¯è§†åŒ–ç»“æœ
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                "æœ€ç»ˆä»·å€¼å‡½æ•°", "æœ€ä¼˜ç­–ç•¥", 
                "æ”¶æ•›è¿‡ç¨‹", "ä»·å€¼å‡½æ•°æ¼”åŒ–"
            ],
            specs=[[{"type": "heatmap"}, {"type": "heatmap"}],
                   [{"type": "scatter"}, {"type": "heatmap"}]]
        )
        
        # æœ€ç»ˆä»·å€¼å‡½æ•°
        V_display = V.copy()
        V_display[grid == -1] = np.nan  # å¢™å£æ˜¾ç¤ºä¸ºç©º
        
        fig.add_trace(
            go.Heatmap(
                z=V_display,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="ä»·å€¼"),
                text=np.array([[f'{V[i,j]:.2f}' if not np.isnan(V[i,j]) else '' 
                              for j in range(grid_size)] for i in range(grid_size)]),
                texttemplate="%{text}",
                textfont={"size": 10}
            ),
            row=1, col=1
        )
        
        # æœ€ä¼˜ç­–ç•¥
        policy_display = policy.copy()
        policy_display[grid == -1] = -1  # å¢™å£
        
        action_symbols = ['â†‘', 'â†“', 'â†', 'â†’', 'â–ˆ']
        policy_text = np.array([[action_symbols[policy_display[i,j]] 
                                for j in range(grid_size)] for i in range(grid_size)])
        
        fig.add_trace(
            go.Heatmap(
                z=policy_display,
                colorscale='RdYlBu',
                showscale=False,
                text=policy_text,
                texttemplate="%{text}",
                textfont={"size": 16}
            ),
            row=1, col=2
        )
        
        # æ”¶æ•›è¿‡ç¨‹
        if show_convergence and len(history) > 1:
            changes = [np.max(np.abs(history[i] - history[i-1])) 
                      for i in range(1, len(history))]
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(1, len(changes)+1)),
                    y=changes,
                    mode='lines+markers',
                    name='æœ€å¤§å˜åŒ–',
                    line=dict(width=2)
                ),
                row=2, col=1
            )
            
            # æ·»åŠ æ”¶æ•›çº¿
            fig.add_hline(
                y=convergence_threshold,
                line_dash="dash",
                line_color="red",
                annotation_text=f"é˜ˆå€¼: {convergence_threshold}"
            )
        
        # ä»·å€¼å‡½æ•°æ¼”åŒ–ï¼ˆé€‰æ‹©å‡ ä¸ªå…³é”®çŠ¶æ€ï¼‰
        if len(history) > 1:
            # é€‰æ‹©èµ·ç‚¹(0,0)å’Œç»ˆç‚¹é™„è¿‘çš„çŠ¶æ€
            start_values = [hist[0, 0] for hist in history]
            near_goal_values = [hist[grid_size-2, grid_size-2] for hist in history]
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(start_values))),
                    y=start_values,
                    mode='lines',
                    name='èµ·ç‚¹ (0,0)',
                    line=dict(width=2)
                ),
                row=2, col=2
            )
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(near_goal_values))),
                    y=near_goal_values,
                    mode='lines',
                    name='è¿‘ç»ˆç‚¹',
                    line=dict(width=2)
                ),
                row=2, col=2
            )
        
        fig.update_layout(
            title="ä»·å€¼è¿­ä»£ç®—æ³•åˆ†æ",
            height=600,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ç®—æ³•æ€§èƒ½åˆ†æ
        st.markdown("### ğŸ“Š ç®—æ³•æ€§èƒ½åˆ†æ")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æœ€ç»ˆè¿­ä»£æ¬¡æ•°", len(history))
        with col2:
            final_change = np.max(np.abs(history[-1] - history[-2])) if len(history) > 1 else 0
            st.metric("æœ€ç»ˆå˜åŒ–", f"{final_change:.4f}")
        with col3:
            max_value = np.nanmax(V_display)
            st.metric("æœ€å¤§ä»·å€¼", f"{max_value:.2f}")
        with col4:
            min_value = np.nanmin(V_display)
            st.metric("æœ€å°ä»·å€¼", f"{min_value:.2f}")
        
        st.success("""
        **ä»·å€¼è¿­ä»£çš„æ•°å­¦ä¿è¯**ï¼š
        - **å·´æ‹¿èµ«ä¸åŠ¨ç‚¹å®šç†**ï¼šè´å°”æ›¼ç®—å­æ˜¯å‹ç¼©æ˜ å°„
        - **æ”¶æ•›æ€§**ï¼šÎ³ < 1 æ—¶å¿…ç„¶æ”¶æ•›åˆ°å”¯ä¸€è§£
        - **æœ€ä¼˜æ€§**ï¼šæ”¶æ•›å¾—åˆ°çš„ç­–ç•¥æ˜¯æœ€ä¼˜ç­–ç•¥
        - **è®¡ç®—å¤æ‚åº¦**ï¼šO(|S|Â²|A|)ï¼Œé€‚ç”¨äºä¸­å°è§„æ¨¡é—®é¢˜
        """)
    
    @staticmethod
    def _render_q_learning():
        """Q-Learningç®—æ³•æ¼”ç¤º"""
        st.markdown("### ğŸ® Q-Learningï¼šæ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ")
        
        st.latex(r"""
        Q(s,a) \leftarrow Q(s,a) + \alpha [R + \gamma \max_{a'} Q(s',a') - Q(s,a)]
        """)
        
        with st.sidebar:
            grid_size = st.slider("ç½‘æ ¼å¤§å°", 4, 6, 4, 1)
            alpha = st.slider("å­¦ä¹ ç‡ Î±", 0.1, 1.0, 0.5, 0.1)
            gamma = st.slider("æŠ˜æ‰£å› å­ Î³", 0.5, 0.99, 0.9, 0.05)
            epsilon = st.slider("æ¢ç´¢ç‡ Îµ", 0.1, 1.0, 0.3, 0.1)
            episodes = st.slider("è®­ç»ƒå›åˆæ•°", 100, 2000, 1000, 100)
            show_q_table = st.checkbox("æ˜¾ç¤ºQè¡¨æ¼”åŒ–", value=True)
        
        # åˆ›å»ºç®€å•çš„ç½‘æ ¼ä¸–ç•Œ
        grid = np.zeros((grid_size, grid_size))
        grid[grid_size-1, grid_size-1] = 10  # ç»ˆç‚¹
        grid[1, 1] = -5  # å°é™·é˜±
        
        # Q-Learningå®ç°
        def q_learning(grid, alpha, gamma, epsilon, episodes):
            rows, cols = grid.shape
            Q = np.zeros((rows, cols, 4))  # 4ä¸ªåŠ¨ä½œ
            rewards_history = []
            steps_history = []
            
            actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
            action_names = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
            
            for episode in range(episodes):
                state = (0, 0)  # ä»èµ·ç‚¹å¼€å§‹
                total_reward = 0
                steps = 0
                max_steps = rows * cols * 2  # é˜²æ­¢æ— é™å¾ªç¯
                
                while steps < max_steps:
                    # Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                    if np.random.random() < epsilon:
                        action = np.random.randint(4)
                    else:
                        action = np.argmax(Q[state[0], state[1]])
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    di, dj = actions[action]
                    new_state = (state[0] + di, state[1] + dj)
                    
                    # æ£€æŸ¥è¾¹ç•Œ
                    if (0 <= new_state[0] < rows and 0 <= new_state[1] < cols):
                        reward = grid[new_state[0], new_state[1]]
                    else:
                        new_state = state  # æ’å¢™
                        reward = -1
                    
                    # Q-Learningæ›´æ–°
                    old_q = Q[state[0], state[1], action]
                    next_max_q = np.max(Q[new_state[0], new_state[1]])
                    td_error = reward + gamma * next_max_q - old_q
                    Q[state[0], state[1], action] = old_q + alpha * td_error
                    
                    total_reward += reward
                    steps += 1
                    state = new_state
                    
                    # åˆ°è¾¾ç»ˆç‚¹
                    if state == (rows-1, cols-1):
                        break
                
                rewards_history.append(total_reward)
                steps_history.append(steps)
                
                # è¡°å‡æ¢ç´¢ç‡
                epsilon = max(0.01, epsilon * 0.995)
            
            return Q, rewards_history, steps_history
        
        # è¿è¡ŒQ-Learning
        Q, rewards, steps = q_learning(grid, alpha, gamma, epsilon, episodes)
        
        # å¯è§†åŒ–ç»“æœ
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                "å­¦ä¹ æ›²çº¿", "æ­¥æ•°å˜åŒ–",
                "æœ€ç»ˆQè¡¨", "æœ€ä¼˜ç­–ç•¥"
            ]
        )
        
        # å­¦ä¹ æ›²çº¿ï¼ˆæ»‘åŠ¨å¹³å‡ï¼‰
        window = min(50, episodes // 10)
        if window > 1:
            rewards_smooth = pd.Series(rewards).rolling(window).mean().values
        else:
            rewards_smooth = rewards
        
        fig.add_trace(
            go.Scatter(
                x=list(range(len(rewards_smooth))),
                y=rewards_smooth,
                mode='lines',
                name='å¹³å‡å¥–åŠ±',
                line=dict(width=2)
            ),
            row=1, col=1
        )
        
        # æ­¥æ•°å˜åŒ–
        steps_smooth = pd.Series(steps).rolling(window).mean().values if window > 1 else steps
        fig.add_trace(
            go.Scatter(
                x=list(range(len(steps_smooth))),
                y=steps_smooth,
                mode='lines',
                name='å¹³å‡æ­¥æ•°',
                line=dict(width=2, color='orange')
            ),
            row=1, col=2
        )
        
        # æœ€ç»ˆQè¡¨çƒ­åŠ›å›¾ï¼ˆé€‰æ‹©æœ€ä¼˜åŠ¨ä½œçš„ä»·å€¼ï¼‰
        Q_max = np.max(Q, axis=2)
        fig.add_trace(
            go.Heatmap(
                z=Q_max,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="æœ€å¤§Qå€¼"),
                text=np.array([[f'{Q_max[i,j]:.1f}' for j in range(grid_size)] 
                              for i in range(grid_size)]),
                texttemplate="%{text}",
                textfont={"size": 10}
            ),
            row=2, col=1
        )
        
        # æœ€ä¼˜ç­–ç•¥
        policy = np.argmax(Q, axis=2)
        action_symbols = ['â†‘', 'â†“', 'â†', 'â†’']
        policy_text = np.array([[action_symbols[policy[i,j]] 
                                for j in range(grid_size)] for i in range(grid_size)])
        
        fig.add_trace(
            go.Heatmap(
                z=policy,
                colorscale='RdYlBu',
                showscale=False,
                text=policy_text,
                texttemplate="%{text}",
                textfont={"size": 16}
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title="Q-Learningç®—æ³•åˆ†æ",
            height=600,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ€§èƒ½ç»Ÿè®¡
        st.markdown("### ğŸ“ˆ å­¦ä¹ æ€§èƒ½ç»Ÿè®¡")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            final_reward = np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards)
            st.metric("æœ€ç»ˆå¹³å‡å¥–åŠ±", f"{final_reward:.2f}")
        with col2:
            final_steps = np.mean(steps[-100:]) if len(steps) >= 100 else np.mean(steps)
            st.metric("æœ€ç»ˆå¹³å‡æ­¥æ•°", f"{final_steps:.1f}")
        with col3:
            success_rate = sum(1 for r in rewards if r > 0) / len(rewards)
            st.metric("æˆåŠŸç‡", f"{success_rate:.1%}")
        with col4:
            convergence_episode = next((i for i, r in enumerate(rewards_smooth) 
                                      if r > 0 and i > 100), len(rewards))
            st.metric("æ”¶æ•›å›åˆ", f"{convergence_episode}")
        
        st.success("""
        **Q-Learningçš„æ ¸å¿ƒä¼˜åŠ¿**ï¼š
        - **æ— æ¨¡å‹**ï¼šä¸éœ€è¦çŸ¥é“ç¯å¢ƒè½¬ç§»æ¦‚ç‡
        - **ç¦»ç­–ç•¥**ï¼šå¯ä»¥ä»å†å²ç»éªŒä¸­å­¦ä¹ 
        - **æ”¶æ•›ä¿è¯**ï¼šåœ¨é€‚å½“æ¡ä»¶ä¸‹æ”¶æ•›åˆ°æœ€ä¼˜Qå‡½æ•°
        - **å®ç”¨æ€§å¼º**ï¼šæ˜¯DQNç­‰æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„åŸºç¡€
        """)
    
    @staticmethod
    def _render_policy_gradient():
        """ç­–ç•¥æ¢¯åº¦ç®—æ³•æ¼”ç¤º"""
        st.markdown("### ğŸ¯ ç­–ç•¥æ¢¯åº¦ï¼šç›´æ¥ä¼˜åŒ–ç­–ç•¥")
        
        st.latex(r"""
        \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \cdot G_t\right]
        """)
        
        with st.sidebar:
            num_states = st.slider("çŠ¶æ€æ•°é‡", 3, 10, 5, 1)
            num_actions = st.slider("åŠ¨ä½œæ•°é‡", 2, 5, 3, 1)
            learning_rate = st.slider("å­¦ä¹ ç‡", 0.001, 0.1, 0.01, 0.001)
            episodes = st.slider("è®­ç»ƒå›åˆæ•°", 500, 5000, 2000, 500)
            temperature = st.slider("æ¸©åº¦å‚æ•°", 0.1, 2.0, 1.0, 0.1)
            show_policy_evolution = st.checkbox("æ˜¾ç¤ºç­–ç•¥æ¼”åŒ–", value=True)
        
        # ç®€åŒ–çš„ç­–ç•¥æ¢¯åº¦å®ç°
        class PolicyGradient:
            def __init__(self, num_states, num_actions, learning_rate, temperature):
                self.num_states = num_states
                self.num_actions = num_actions
                self.lr = learning_rate
                self.temperature = temperature
                
                # ç­–ç•¥å‚æ•°
                self.theta = np.random.randn(num_states, num_actions) * 0.1
                
            def policy(self, state):
                """Softmaxç­–ç•¥"""
                logits = self.theta[state] / self.temperature
                exp_logits = np.exp(logits - np.max(logits))
                return exp_logits / np.sum(exp_logits)
            
            def sample_action(self, state):
                """æ ¹æ®ç­–ç•¥é‡‡æ ·åŠ¨ä½œ"""
                probs = self.policy(state)
                return np.random.choice(self.num_actions, p=probs)
            
            def update(self, states, actions, rewards):
                """ç­–ç•¥æ¢¯åº¦æ›´æ–°"""
                for state, action, reward in zip(states, actions, rewards):
                    # è®¡ç®—æ¢¯åº¦
                    probs = self.policy(state)
                    grad = np.zeros(self.num_actions)
                    
                    for a in range(self.num_actions):
                        if a == action:
                            grad[a] = (1 - probs[a]) * reward
                        else:
                            grad[a] = -probs[a] * reward
                    
                    # æ›´æ–°å‚æ•°
                    self.theta[state] += self.lr * grad
        
        # åˆ›å»ºç®€å•çš„ç¯å¢ƒ
        def create_environment(num_states):
            # çº¿æ€§ç¯å¢ƒï¼šä»çŠ¶æ€0åˆ°ç›®æ ‡çŠ¶æ€num_states-1
            rewards = {}
            for s in range(num_states):
                for a in range(3):  # 3ä¸ªåŠ¨ä½œï¼šå‰è¿›ã€åé€€ã€ä¸åŠ¨
                    if a == 0:  # å‰è¿›
                        next_s = min(s + 1, num_states - 1)
                        rewards[(s, a, next_s)] = 1.0 if next_s == num_states - 1 else -0.1
                    elif a == 1:  # åé€€
                        next_s = max(s - 1, 0)
                        rewards[(s, a, next_s)] = -0.1
                    else:  # ä¸åŠ¨
                        rewards[(s, a, s)] = -0.05
            
            return rewards
        
        # è®­ç»ƒç­–ç•¥æ¢¯åº¦
        env_rewards = create_environment(num_states)
        agent = PolicyGradient(num_states, 3, learning_rate, temperature)
        
        episode_rewards = []
        policy_history = []
        
        for episode in range(episodes):
            state = 0
            states, actions, rewards = [], [], []
            total_reward = 0
            
            # ä¸€ä¸ªå›åˆ
            for step in range(num_states * 2):
                action = agent.sample_action(state)
                next_state = min(max(state + action - 1, 0), num_states - 1)
                reward = env_rewards.get((state, action, next_state), -0.1)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                
                total_reward += reward
                state = next_state
                
                if state == num_states - 1:  # åˆ°è¾¾ç›®æ ‡
                    break
            
            # æ›´æ–°ç­–ç•¥
            agent.update(states, actions, rewards)
            episode_rewards.append(total_reward)
            
            # è®°å½•ç­–ç•¥æ¼”åŒ–
            if episode % 100 == 0:
                policy_snapshot = np.array([agent.policy(s) for s in range(num_states)])
                policy_history.append(policy_snapshot)
        
        # å¯è§†åŒ–ç»“æœ
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                "å­¦ä¹ æ›²çº¿", "æœ€ç»ˆç­–ç•¥åˆ†å¸ƒ",
                "ç­–ç•¥æ¼”åŒ–", "åŠ¨ä½œæ¦‚ç‡å˜åŒ–"
            ]
        )
        
        # å­¦ä¹ æ›²çº¿
        window = min(100, episodes // 10)
        if window > 1:
            rewards_smooth = pd.Series(episode_rewards).rolling(window).mean().values
        else:
            rewards_smooth = episode_rewards
        
        fig.add_trace(
            go.Scatter(
                x=list(range(len(rewards_smooth))),
                y=rewards_smooth,
                mode='lines',
                name='å¹³å‡å¥–åŠ±',
                line=dict(width=2)
            ),
            row=1, col=1
        )
        
        # æœ€ç»ˆç­–ç•¥åˆ†å¸ƒ
        final_policy = np.array([agent.policy(s) for s in range(num_states)])
        action_names = ['åé€€', 'å‰è¿›', 'ä¸åŠ¨']
        
        for action in range(3):
            fig.add_trace(
                go.Bar(
                    x=list(range(num_states)),
                    y=final_policy[:, action],
                    name=action_names[action],
                    opacity=0.7
                ),
                row=1, col=2
            )
        
        # ç­–ç•¥æ¼”åŒ–ï¼ˆé€‰æ‹©å‡ ä¸ªçŠ¶æ€ï¼‰
        if show_policy_evolution and policy_history:
            for state_idx in [0, num_states//2, num_states-1]:
                evolution = [policy_hist[state_idx, 1] for policy_hist in policy_history]  # å‰è¿›åŠ¨ä½œ
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(evolution))),
                        y=evolution,
                        mode='lines',
                        name=f'çŠ¶æ€{state_idx}å‰è¿›æ¦‚ç‡',
                        line=dict(width=2)
                    ),
                    row=2, col=1
                )
        
        # åŠ¨ä½œæ¦‚ç‡éšæ—¶é—´å˜åŒ–
        if len(policy_history) > 1:
            start_policy = policy_history[0]
            end_policy = policy_history[-1]
            
            x_pos = np.arange(num_states)
            width = 0.35
            
            for action in range(3):
                fig.add_trace(
                    go.Bar(
                        x=x_pos - width/2,
                        y=start_policy[:, action],
                        name=f'åˆå§‹{action_names[action]}',
                        width=width,
                        opacity=0.7
                    ),
                    row=2, col=2
                )
                
                fig.add_trace(
                    go.Bar(
                        x=x_pos + width/2,
                        y=end_policy[:, action],
                        name=f'æœ€ç»ˆ{action_names[action]}',
                        width=width,
                        opacity=0.7
                    ),
                    row=2, col=2
                )
        
        fig.update_layout(
            title="ç­–ç•¥æ¢¯åº¦ç®—æ³•åˆ†æ",
            height=600,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ç®—æ³•åˆ†æ
        st.markdown("### ğŸ“Š ç®—æ³•åˆ†æ")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            final_performance = np.mean(episode_rewards[-100:])
            st.metric("æœ€ç»ˆæ€§èƒ½", f"{final_performance:.3f}")
        with col2:
            improvement = episode_rewards[-1] - episode_rewards[0]
            st.metric("æ€§èƒ½æå‡", f"{improvement:.3f}")
        with col3:
            convergence_point = next((i for i, r in enumerate(rewards_smooth) 
                                    if r > 0 and i > 100), len(rewards_smooth))
            st.metric("æ”¶æ•›ç‚¹", f"{convergence_point}")
        with col4:
            final_entropy = -np.sum(final_policy * np.log(final_policy + 1e-8), axis=1).mean()
            st.metric("ç­–ç•¥ç†µ", f"{final_entropy:.3f}")
        
        st.info("""
        **ç­–ç•¥æ¢¯åº¦çš„ç‰¹ç‚¹**ï¼š
        - **ç›´æ¥ä¼˜åŒ–**ï¼šç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°ï¼Œä¸éœ€è¦ä»·å€¼å‡½æ•°
        - **è¿ç»­åŠ¨ä½œ**ï¼šå¤©ç„¶æ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´
        - **éšæœºç­–ç•¥**ï¼šå¯ä»¥å­¦ä¹ éšæœºç­–ç•¥ï¼Œé€‚åˆæ¢ç´¢
        - **é«˜æ–¹å·®**ï¼šé€šå¸¸æ¯”ä»·å€¼æ–¹æ³•æ–¹å·®æ›´é«˜ï¼Œéœ€è¦æ›´å¤šæ ·æœ¬
        """)

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
