"""
äº¤äº’å¼æ¦‚ç‡ç¼–ç¨‹ä¸è´å¶æ–¯æ·±åº¦å­¦ä¹ å¯è§†åŒ–
ä¸¥æ ¼æŒ‰ç…§ 17.ProbabilisticProgramming.md ä¸­çš„å…¬å¼å®ç°
"""

import streamlit as st
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from scipy.stats import norm, multivariate_normal
import warnings
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥commonæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from common.error_handler import safe_render
from common.quiz_system import QuizSystem, QuizTemplates

warnings.filterwarnings('ignore')


class InteractiveProbabilisticProgramming:
    """äº¤äº’å¼æ¦‚ç‡ç¼–ç¨‹ä¸è´å¶æ–¯æ·±åº¦å­¦ä¹ å¯è§†åŒ–"""
    
    @staticmethod
    @safe_render

    def render():
        st.subheader("ğŸ² æ¦‚ç‡ç¼–ç¨‹ä¸è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼šæƒé‡çš„ä¸ç¡®å®šæ€§é‡åŒ–")
        st.markdown("""
        **æ ¸å¿ƒæ€æƒ³**: ä»ç‚¹ä¼°è®¡åˆ°åˆ†å¸ƒæ¨æ–­ï¼Œç¥ç»ç½‘ç»œçš„ç¬¬ä¸‰æ¬¡æ•°å­¦é£è·ƒ
        
        å…³é”®æ¦‚å¿µï¼š
        - **è´å¶æ–¯å…¬å¼**: $p(w|D) = \\frac{p(D|w)p(w)}{p(D)}$
        - **å˜åˆ†æ¨æ–­**: $L(\\theta) = \\mathbb{E}_{w \\sim q_\\theta}[\\log P(D \\mid w)] - KL(q_{\\theta}(w) \\mid\\mid P(w))$
        - **é‡å‚æ•°åŒ–**: $z = \\mu + \\sigma \\odot \\epsilon$
        - **ä¸ç¡®å®šæ€§åˆ†ç±»**: è®¤çŸ¥ä¸ç¡®å®šæ€§ vs ä»»æ„ä¸ç¡®å®šæ€§
        """)
        
        with st.sidebar:
            st.markdown("### ğŸ“Š å¯è§†åŒ–é€‰æ‹©")
            viz_type = st.selectbox("é€‰æ‹©å¯è§†åŒ–ç±»å‹", 
                ["è´å¶æ–¯æ¨æ–­åŸºç¡€", "å˜åˆ†æ¨æ–­ vs MCMC", "é‡å‚æ•°åŒ–æŠ€å·§", "ä¸ç¡®å®šæ€§åˆ†æ"])
            
            st.markdown("### ğŸ›ï¸ å‚æ•°è°ƒæ•´")
        
        if viz_type == "è´å¶æ–¯æ¨æ–­åŸºç¡€":
            InteractiveProbabilisticProgramming._render_bayesian_basics()
        elif viz_type == "å˜åˆ†æ¨æ–­ vs MCMC":
            InteractiveProbabilisticProgramming._render_vi_vs_mcmc()
        elif viz_type == "é‡å‚æ•°åŒ–æŠ€å·§":
            InteractiveProbabilisticProgramming._render_reparameterization()
        elif viz_type == "ä¸ç¡®å®šæ€§åˆ†æ":
            InteractiveProbabilisticProgramming._render_uncertainty_analysis()
    

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
        quiz_system = QuizSystem("probabilistic_programming")
        quizzes = QuizTemplates.get_probabilistic_programming_quizzes()
        quiz_system.render_quiz(quizzes)
    @staticmethod
    def _render_bayesian_basics():
        """è´å¶æ–¯æ¨æ–­åŸºç¡€æ¼”ç¤º"""
        st.markdown("### ğŸ§® è´å¶æ–¯æ¨æ–­ï¼šé¢‘ç‡æ´¾ vs è´å¶æ–¯æ´¾")
        
        st.latex(r"""
        p(w|D) = \frac{p(D|w)p(w)}{p(D)} = \frac{p(D|w)p(w)}{\int p(D|w)p(w)dw}
        """)
        
        with st.sidebar:
            # æ¨¡æ‹ŸæŠ›ç¡¬å¸é—®é¢˜
            prior_type = st.selectbox("å…ˆéªŒåˆ†å¸ƒç±»å‹", ["å‡åŒ€å…ˆéªŒ", "é«˜æ–¯å…ˆéªŒ", "Betaå…ˆéªŒ"])
            num_observations = st.slider("è§‚æµ‹æ¬¡æ•°", 1, 50, 10, 1)
            num_heads = st.slider("æ­£é¢æœä¸Šæ¬¡æ•°", 0, 50, 7, 1)
            show_marginal = st.checkbox("æ˜¾ç¤ºè¾¹ç¼˜ä¼¼ç„¶è®¡ç®—", value=False)
        
        # å®šä¹‰å‚æ•°ç©ºé—´
        theta_range = np.linspace(0, 1, 200)
        
        # å…ˆéªŒåˆ†å¸ƒ
        if prior_type == "å‡åŒ€å…ˆéªŒ":
            prior = np.ones_like(theta_range)
            prior_name = "Uniform(0,1)"
        elif prior_type == "é«˜æ–¯å…ˆéªŒ":
            prior = norm.pdf(theta_range, loc=0.5, scale=0.2)
            prior = prior / np.sum(prior)  # å½’ä¸€åŒ–
            prior_name = "Normal(0.5, 0.2)"
        else:  # Betaå…ˆéªŒ
            alpha, beta_param = 2, 2
            prior = theta_range**(alpha-1) * (1-theta_range)**(beta_param-1)
            prior = prior / np.sum(prior)
            prior_name = f"Beta({alpha}, {beta_param})"
        
        # ä¼¼ç„¶å‡½æ•° (äºŒé¡¹åˆ†å¸ƒ)
        likelihood = theta_range**num_heads * (1-theta_range)**(num_observations - num_heads)
        likelihood = likelihood / np.sum(likelihood)
        
        # åéªŒåˆ†å¸ƒ (è´å¶æ–¯æ›´æ–°)
        posterior = likelihood * prior
        posterior = posterior / np.sum(posterior)
        
        # è¾¹ç¼˜ä¼¼ç„¶ (è¯æ®)
        marginal_likelihood = np.sum(likelihood * prior)
        
        # å¯è§†åŒ–
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                "å…ˆéªŒåˆ†å¸ƒ p(w)", "ä¼¼ç„¶å‡½æ•° p(D|w)",
                "åéªŒåˆ†å¸ƒ p(w|D)", "è´å¶æ–¯æ›´æ–°è¿‡ç¨‹"
            ]
        )
        
        # å…ˆéªŒ
        fig.add_trace(
            go.Scatter(
                x=theta_range, y=prior,
                mode='lines',
                name='å…ˆéªŒ',
                line=dict(color='blue', width=2)
            ),
            row=1, col=1
        )
        
        # ä¼¼ç„¶
        fig.add_trace(
            go.Scatter(
                x=theta_range, y=likelihood,
                mode='lines',
                name='ä¼¼ç„¶',
                line=dict(color='red', width=2)
            ),
            row=1, col=2
        )
        
        # åéªŒ
        fig.add_trace(
            go.Scatter(
                x=theta_range, y=posterior,
                mode='lines',
                name='åéªŒ',
                line=dict(color='green', width=2)
            ),
            row=2, col=1
        )
        
        # è´å¶æ–¯æ›´æ–°è¿‡ç¨‹ (åŠ¨ç”»æ•ˆæœ)
        fig.add_trace(
            go.Scatter(
                x=theta_range, y=prior,
                mode='lines',
                name='å…ˆéªŒ',
                line=dict(color='blue', width=2, dash='dash')
            ),
            row=2, col=2
        )
        
        fig.add_trace(
            go.Scatter(
                x=theta_range, y=posterior,
                mode='lines',
                name='åéªŒ',
                line=dict(color='green', width=3)
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title=f"è´å¶æ–¯æ¨æ–­è¿‡ç¨‹ - {prior_name}",
            height=600,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºè®¡ç®—ç»“æœ
        if show_marginal:
            st.markdown("### ğŸ§® è¾¹ç¼˜ä¼¼ç„¶è®¡ç®—")
            st.latex(f"""
            p(D) = \\int_0^1 p(D|w)p(w)dw \\approx {marginal_likelihood:.6f}
            """)
            
            st.markdown("""
            **ç§¯åˆ†è¿‘ä¼¼è¿‡ç¨‹**ï¼š
            - å°†è¿ç»­åŒºé—´[0,1]ç¦»æ•£åŒ–ä¸º200ä¸ªç‚¹
            - ä½¿ç”¨é»æ›¼æ±‚å’Œè¿‘ä¼¼ç§¯åˆ†
            - $p(D) \\approx \\sum_{i=1}^{200} p(D|\\theta_i)p(\\theta_i) \\Delta\\theta$
            """)
        
        # åéªŒåˆ†æ
        st.markdown("### ğŸ“Š åéªŒåˆ†æ")
        
        # è®¡ç®—åéªŒç»Ÿè®¡é‡
        posterior_mean = np.sum(theta_range * posterior)
        posterior_var = np.sum((theta_range - posterior_mean)**2 * posterior)
        posterior_std = np.sqrt(posterior_var)
        
        # æœ€å¤§åéªŒä¼°è®¡ (MAP)
        map_idx = np.argmax(posterior)
        map_estimate = theta_range[map_idx]
        
        # å¯ä¿¡åŒºé—´
        cumsum = np.cumsum(posterior)
        lower_idx = np.argmax(cumsum >= 0.025)
        upper_idx = np.argmax(cumsum >= 0.975)
        credible_interval = (theta_range[lower_idx], theta_range[upper_idx])
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("åéªŒå‡å€¼", f"{posterior_mean:.3f}")
        with col2:
            st.metric("åéªŒæ ‡å‡†å·®", f"{posterior_std:.3f}")
        with col3:
            st.metric("MAPä¼°è®¡", f"{map_estimate:.3f}")
        with col4:
            st.metric("95%å¯ä¿¡åŒºé—´", f"[{credible_interval[0]:.3f}, {credible_interval[1]:.3f}]")
        
        st.info("""
        **å…³é”®æ´å¯Ÿ**ï¼š
        - **å…ˆéªŒ**ï¼šä»£è¡¨æˆ‘ä»¬å¯¹å‚æ•°çš„åˆå§‹ä¿¡å¿µ
        - **ä¼¼ç„¶**ï¼šæ•°æ®å¯¹å‚æ•°çš„è§£é‡ŠåŠ›
        - **åéªŒ**ï¼šç»“åˆå…ˆéªŒå’Œæ•°æ®çš„æ›´æ–°ä¿¡å¿µ
        - **è¾¹ç¼˜ä¼¼ç„¶**ï¼šæ¨¡å‹è¯æ®ï¼Œç”¨äºæ¨¡å‹é€‰æ‹©
        """)
    
    @staticmethod
    def _render_vi_vs_mcmc():
        """å˜åˆ†æ¨æ–­ vs MCMC å¯¹æ¯”æ¼”ç¤º"""
        st.markdown("### âš–ï¸ å˜åˆ†æ¨æ–­ vs MCMCï¼šä¸¤ç§ç»•è¿‡ç§¯åˆ†çš„æ–¹æ³•")
        
        st.markdown("""
        **å˜åˆ†æ¨æ–­ (VI)**ï¼šæŠŠç§¯åˆ†é—®é¢˜è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜
        - ä¼˜ç‚¹ï¼šå¿«ï¼Œå¯æ‰©å±•ï¼Œé€‚åˆå¤§è§„æ¨¡ç¥ç»ç½‘ç»œ
        - ç¼ºç‚¹ï¼šå‡è®¾è¿‘ä¼¼åˆ†å¸ƒæ—ï¼Œå¯èƒ½ä½ä¼°æ–¹å·®
        
        **MCMC**ï¼šæŠŠè®¡ç®—é—®é¢˜è½¬åŒ–ä¸ºé‡‡æ ·é—®é¢˜  
        - ä¼˜ç‚¹ï¼šç†è®ºä¸Šç²¾ç¡®ï¼Œé‡‘æ ‡å‡†
        - ç¼ºç‚¹ï¼šæ…¢ï¼Œè®¡ç®—æˆæœ¬é«˜
        """)
        
        with st.sidebar:
            target_dist = st.selectbox("ç›®æ ‡åˆ†å¸ƒ", ["é«˜æ–¯æ··åˆ", "å¤šå³°åˆ†å¸ƒ", "åæ€åˆ†å¸ƒ"])
            vi_iterations = st.slider("VIè¿­ä»£æ¬¡æ•°", 50, 500, 200, 50)
            mcmc_samples = st.slider("MCMCæ ·æœ¬æ•°", 100, 2000, 1000, 100)
            show_elbo = st.checkbox("æ˜¾ç¤ºELBOæ›²çº¿", value=True)
        
        # å®šä¹‰ç›®æ ‡åˆ†å¸ƒ (çœŸå®çš„åéªŒ)
        def target_distribution(x):
            if target_dist == "é«˜æ–¯æ··åˆ":
                # åŒé«˜æ–¯æ··åˆ
                return 0.6 * norm.pdf(x, loc=-2, scale=1) + 0.4 * norm.pdf(x, loc=2, scale=1.5)
            elif target_dist == "å¤šå³°åˆ†å¸ƒ":
                # ä¸‰å³°åˆ†å¸ƒ
                return 0.4 * norm.pdf(x, loc=-3, scale=0.8) + 0.3 * norm.pdf(x, loc=0, scale=1) + 0.3 * norm.pdf(x, loc=3, scale=1.2)
            else:  # åæ€åˆ†å¸ƒ
                return norm.pdf(x, loc=1, scale=1.5) * (1 + 0.5 * np.tanh(x))
        
        # å˜åˆ†æ¨æ–­ (ä½¿ç”¨é«˜æ–¯è¿‘ä¼¼)
        def variational_inference(target_func, iterations):
            # åˆå§‹åŒ–å˜åˆ†å‚æ•° (é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®)
            mu = 0.0
            log_sigma = 0.0  # ä½¿ç”¨logç¡®ä¿æ–¹å·®ä¸ºæ­£
            
            elbo_history = []
            param_history = []
            
            x_range = np.linspace(-6, 6, 200)
            target_vals = target_func(x_range)
            target_vals = target_vals / np.sum(target_vals)  # å½’ä¸€åŒ–
            
            for i in range(iterations):
                # å½“å‰å˜åˆ†åˆ†å¸ƒ
                sigma = np.exp(log_sigma)
                q_values = norm.pdf(x_range, loc=mu, scale=sigma)
                q_values = q_values / np.sum(q_values)
                
                # è®¡ç®—ELBO
                # é‡æ„é¡¹ï¼šE_q[log p(D|w)]
                reconstruction = np.sum(q_values * np.log(target_vals + 1e-8))
                
                # KLæ•£åº¦é¡¹ï¼šKL(q||p) è¿™é‡Œå‡è®¾å…ˆéªŒæ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒ
                prior_values = norm.pdf(x_range, loc=0, scale=1)
                prior_values = prior_values / np.sum(prior_values)
                kl_divergence = np.sum(q_values * np.log((q_values + 1e-8) / (prior_values + 1e-8)))
                
                elbo = reconstruction - kl_divergence
                elbo_history.append(elbo)
                param_history.append((mu, sigma))
                
                # æ¢¯åº¦æ›´æ–° (ç®€åŒ–çš„æ¢¯åº¦ä¸Šå‡)
                lr = 0.01
                mu += lr * 0.1  # ç®€åŒ–çš„æ¢¯åº¦
                log_sigma += lr * 0.05
                
            return mu, np.exp(log_sigma), elbo_history, param_history
        
        # ç®€åŒ–çš„MCMC (Metropolis-Hastings)
        def metropolis_hastings(target_func, num_samples, burn_in=100):
            samples = []
            current = 0.0
            
            for i in range(num_samples + burn_in):
                # æè®®æ–°çŠ¶æ€
                proposal = current + np.random.normal(0, 1)
                
                # è®¡ç®—æ¥å—æ¦‚ç‡
                current_prob = target_func(current)
                proposal_prob = target_func(proposal)
                
                acceptance_prob = min(1, proposal_prob / current_prob)
                
                # æ¥å—æˆ–æ‹’ç»
                if np.random.random() < acceptance_prob:
                    current = proposal
                
                if i >= burn_in:
                    samples.append(current)
            
            return np.array(samples)
        
        # è¿è¡Œç®—æ³•
        np.random.seed(42)
        
        # å˜åˆ†æ¨æ–­
        vi_mu, vi_sigma, elbo_history, param_history = variational_inference(target_distribution, vi_iterations)
        
        # MCMC
        mcmc_samples = metropolis_hastings(target_distribution, mcmc_samples)
        
        # å¯è§†åŒ–ç»“æœ
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                "ç›®æ ‡åˆ†å¸ƒ vs è¿‘ä¼¼åˆ†å¸ƒ", "ELBOæ”¶æ•›æ›²çº¿",
                "MCMCé‡‡æ ·è½¨è¿¹", "æ–¹æ³•å¯¹æ¯”æ€»ç»“"
            ]
        )
        
        x_range = np.linspace(-6, 6, 200)
        target_vals = target_distribution(x_range)
        target_vals = target_vals / np.sum(target_vals)
        
        # ç›®æ ‡åˆ†å¸ƒ vs VIè¿‘ä¼¼
        fig.add_trace(
            go.Scatter(
                x=x_range, y=target_vals,
                mode='lines',
                name='ç›®æ ‡åˆ†å¸ƒ',
                line=dict(color='black', width=3)
            ),
            row=1, col=1
        )
        
        vi_approx = norm.pdf(x_range, loc=vi_mu, scale=vi_sigma)
        vi_approx = vi_approx / np.sum(vi_approx)
        fig.add_trace(
            go.Scatter(
                x=x_range, y=vi_approx,
                mode='lines',
                name='VIè¿‘ä¼¼',
                line=dict(color='red', width=2, dash='dash')
            ),
            row=1, col=1
        )
        
        # ELBOæ›²çº¿
        if show_elbo:
            fig.add_trace(
                go.Scatter(
                    x=np.arange(len(elbo_history)),
                    y=elbo_history,
                    mode='lines',
                    name='ELBO',
                    line=dict(color='blue', width=2)
                ),
                row=1, col=2
            )
        
        # MCMCé‡‡æ ·è½¨è¿¹
        fig.add_trace(
            go.Scatter(
                x=np.arange(len(mcmc_samples)),
                y=mcmc_samples,
                mode='lines',
                name='MCMCè½¨è¿¹',
                line=dict(color='green', width=1),
                opacity=0.7
            ),
            row=2, col=1
        )
        
        # MCMCç›´æ–¹å›¾
        fig.add_trace(
            go.Histogram(
                x=mcmc_samples,
                nbinsx=50,
                name='MCMCæ ·æœ¬',
                marker_color='lightgreen',
                opacity=0.7,
                yaxis='y4'
            ),
            row=2, col=1
        )
        
        # æ–¹æ³•å¯¹æ¯”
        methods = ['VI', 'MCMC']
        metrics = ['è®¡ç®—é€Ÿåº¦', 'ç²¾åº¦', 'å¯æ‰©å±•æ€§', 'ç†è®ºä¿è¯']
        
        comparison_matrix = [
            [5, 3, 5, 3],  # VI
            [2, 5, 2, 5]   # MCMC
        ]
        
        fig.add_trace(
            go.Bar(
                x=methods,
                y=[comparison_matrix[0][0], comparison_matrix[1][0]],
                name='è®¡ç®—é€Ÿåº¦',
                marker_color='blue'
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title="å˜åˆ†æ¨æ–­ vs MCMC å¯¹æ¯”åˆ†æ",
            height=600,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        st.markdown("### ğŸ“Š æ€§èƒ½å¯¹æ¯”")
        
        # è®¡ç®—MCMCçš„ç»Ÿè®¡é‡
        mcmc_mean = np.mean(mcmc_samples)
        mcmc_std = np.std(mcmc_samples)
        
        # è®¡ç®—VIçš„ç»Ÿè®¡é‡
        vi_mean = vi_mu
        vi_std = vi_sigma
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("VIå‡å€¼", f"{vi_mean:.3f}")
        with col2:
            st.metric("VIæ ‡å‡†å·®", f"{vi_std:.3f}")
        with col3:
            st.metric("MCMCå‡å€¼", f"{mcmc_mean:.3f}")
        with col4:
            st.metric("MCMCæ ‡å‡†å·®", f"{mcmc_std:.3f}")
        
        # è¯¦ç»†å¯¹æ¯”è¡¨
        comparison_data = {
            "ç‰¹æ€§": ["è®¡ç®—å¤æ‚åº¦", "å†…å­˜éœ€æ±‚", "æ”¶æ•›ä¿è¯", "é€‚ç”¨åœºæ™¯"],
            "å˜åˆ†æ¨æ–­": ["O(N)", "ä½", "å±€éƒ¨æœ€ä¼˜", "å¤§è§„æ¨¡ç¥ç»ç½‘ç»œ"],
            "MCMC": ["O(NÂ²)", "é«˜", "ç†è®ºç²¾ç¡®", "å°è§„æ¨¡ç²¾ç¡®æ¨æ–­"]
        }
        
        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison, use_container_width=True)
        
        st.success("""
        **å®è·µå»ºè®®**ï¼š
        - **å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ **ï¼šä½¿ç”¨å˜åˆ†æ¨æ–­ (å¦‚TensorFlow Probability)
        - **å°è§„æ¨¡ç²¾ç¡®æ¨æ–­**ï¼šä½¿ç”¨MCMC (å¦‚PyMC3, Stan)
        - **å®æ—¶åº”ç”¨**ï¼šå˜åˆ†æ¨æ–­ä¼˜åŠ¿æ˜æ˜¾
        - **ç§‘ç ”åˆ†æ**ï¼šMCMCæä¾›æ›´å¯é çš„ä¸ç¡®å®šæ€§é‡åŒ–
        """)
    
    @staticmethod
    def _render_reparameterization():
        """é‡å‚æ•°åŒ–æŠ€å·§æ¼”ç¤º"""
        st.markdown("### ğŸ”„ é‡å‚æ•°åŒ–æŠ€å·§ï¼šè®©æ¢¯åº¦æµåŠ¨çš„é­”æ³•")
        
        st.latex(r"""
        z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
        """)
        
        st.markdown("""
        **æ ¸å¿ƒæ€æƒ³**ï¼šå°†éšæœºæ€§ä»ç½‘ç»œå†…éƒ¨å‰¥ç¦»åˆ°å¤–éƒ¨è¾“å…¥ï¼Œä½¿æ¢¯åº¦å¯ä»¥åå‘ä¼ æ’­
        
        - **é—®é¢˜**ï¼šç›´æ¥é‡‡æ · $z \\sim \\mathcal{N}(\\mu, \\sigma)$ ä¼šåˆ‡æ–­è®¡ç®—å›¾
        - **è§£å†³**ï¼šå¼•å…¥æ ‡å‡†å™ªå£° $\\epsilon$ï¼Œè®© $z$ æˆä¸º $\\mu$ å’Œ $\\sigma$ çš„ç¡®å®šæ€§å‡½æ•°
        """)
        
        with st.sidebar:
            mu_range = st.slider("å‡å€¼ Î¼ èŒƒå›´", -3.0, 3.0, (-1.0, 1.0), 0.1)
            sigma_range = st.slider("æ ‡å‡†å·® Ïƒ èŒƒå›´", 0.1, 2.0, (0.5, 1.5), 0.1)
            num_samples = st.slider("é‡‡æ ·æ•°é‡", 100, 2000, 1000, 100)
            show_gradient = st.checkbox("æ˜¾ç¤ºæ¢¯åº¦æµ", value=True)
        
        # ç”Ÿæˆå‚æ•°ç½‘æ ¼
        mu_values = np.linspace(mu_range[0], mu_range[1], 20)
        sigma_values = np.linspace(sigma_range[0], sigma_range[1], 20)
        
        # é‡å‚æ•°åŒ–é‡‡æ ·
        def reparameterized_sample(mu, sigma, epsilon):
            return mu + sigma * epsilon
        
        # æ ‡å‡†é‡‡æ · (æ— æ³•åå‘ä¼ æ’­)
        def direct_sample(mu, sigma):
            return np.random.normal(mu, sigma)
        
        # å¯è§†åŒ–é‡å‚æ•°åŒ–è¿‡ç¨‹
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                "é‡å‚æ•°åŒ–é‡‡æ ·", "æ ‡å‡†é‡‡æ · (å¯¹æ¯”)",
                "æ¢¯åº¦æµå¯è§†åŒ–", "åˆ†å¸ƒå˜æ¢"
            ]
        )
        
        # ç”Ÿæˆå™ªå£°æ ·æœ¬
        np.random.seed(42)
        epsilon_samples = np.random.normal(0, 1, num_samples)
        
        # é€‰æ‹©ç‰¹å®šçš„muå’Œsigmaè¿›è¡Œæ¼”ç¤º
        mu_demo, sigma_demo = 0.0, 1.0
        
        # é‡å‚æ•°åŒ–é‡‡æ ·
        z_reparam = reparameterized_sample(mu_demo, sigma_demo, epsilon_samples)
        
        # æ ‡å‡†é‡‡æ ·
        z_direct = np.array([direct_sample(mu_demo, sigma_demo) for _ in range(num_samples)])
        
        # é‡å‚æ•°åŒ–é‡‡æ ·åˆ†å¸ƒ
        fig.add_trace(
            go.Histogram(
                x=z_reparam,
                nbinsx=30,
                name='é‡å‚æ•°åŒ–é‡‡æ ·',
                marker_color='blue',
                opacity=0.7
            ),
            row=1, col=1
        )
        
        # æ ‡å‡†é‡‡æ ·åˆ†å¸ƒ
        fig.add_trace(
            go.Histogram(
                x=z_direct,
                nbinsx=30,
                name='æ ‡å‡†é‡‡æ ·',
                marker_color='red',
                opacity=0.7
            ),
            row=1, col=2
        )
        
        # æ¢¯åº¦æµå¯è§†åŒ–
        if show_gradient:
            # æ˜¾ç¤ºæ¢¯åº¦å¦‚ä½•æµåŠ¨
            mu_grad = epsilon_samples  # âˆ‚z/âˆ‚Î¼ = Îµ
            sigma_grad = epsilon_samples  # âˆ‚z/âˆ‚Ïƒ = Îµ
            
            fig.add_trace(
                go.Scatter(
                    x=epsilon_samples[:100],
                    y=mu_grad[:100],
                    mode='markers',
                    name='âˆ‚z/âˆ‚Î¼ = Îµ',
                    marker=dict(color='green', size=4)
                ),
                row=2, col=1
            )
            
            fig.add_trace(
                go.Scatter(
                    x=epsilon_samples[:100],
                    y=sigma_grad[:100],
                    mode='markers',
                    name='âˆ‚z/âˆ‚Ïƒ = Îµ',
                    marker=dict(color='orange', size=4)
                ),
                row=2, col=1
            )
        
        # åˆ†å¸ƒå˜æ¢ï¼šä»æ ‡å‡†æ­£æ€åˆ°ä»»æ„æ­£æ€
        x_range = np.linspace(-4, 4, 200)
        standard_normal = norm.pdf(x_range, loc=0, scale=1)
        transformed_normal = norm.pdf(x_range, loc=mu_demo, scale=sigma_demo)
        
        fig.add_trace(
            go.Scatter(
                x=x_range, y=standard_normal,
                mode='lines',
                name='æ ‡å‡†æ­£æ€ N(0,1)',
                line=dict(color='purple', width=2)
            ),
            row=2, col=2
        )
        
        fig.add_trace(
            go.Scatter(
                x=x_range, y=transformed_normal,
                mode='lines',
                name=f'å˜æ¢å N({mu_demo},{sigma_demo})',
                line=dict(color='brown', width=2, dash='dash')
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title="é‡å‚æ•°åŒ–æŠ€å·§è¯¦ç»†åˆ†æ",
            height=600,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # äº¤äº’å¼å‚æ•°è°ƒèŠ‚
        st.markdown("### ğŸ›ï¸ äº¤äº’å¼å‚æ•°è°ƒèŠ‚")
        
        col1, col2 = st.columns(2)
        
        with col1:
            mu_interactive = st.slider("è°ƒèŠ‚å‡å€¼ Î¼", -3.0, 3.0, 0.0, 0.1)
            sigma_interactive = st.slider("è°ƒèŠ‚æ ‡å‡†å·® Ïƒ", 0.1, 2.0, 1.0, 0.1)
        
        with col2:
            # å®æ—¶æ˜¾ç¤ºé‡‡æ ·ç»“æœ
            z_interactive = reparameterized_sample(mu_interactive, sigma_interactive, epsilon_samples)
            
            fig_interactive = go.Figure()
            fig_interactive.add_trace(
                go.Histogram(
                    x=z_interactive,
                    nbinsx=30,
                    name=f'N({mu_interactive:.1f}, {sigma_interactive:.1f})',
                    marker_color='lightblue'
                )
            )
            
            fig_interactive.update_layout(
                title="å®æ—¶é‡‡æ ·åˆ†å¸ƒ",
                xaxis_title="zå€¼",
                yaxis_title="é¢‘æ¬¡",
                height=300
            )
            
            st.plotly_chart(fig_interactive, use_container_width=True)
        
        # æ•°å­¦æ¨å¯¼
        st.markdown("### ğŸ“ æ•°å­¦æ¨å¯¼")
        
        st.latex(r"""
        \begin{aligned}
        z &\sim \mathcal{N}(\mu, \sigma^2) \\
        \text{é‡å‚æ•°åŒ–:} \quad z &= \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1) \\
        \\
        \frac{\partial z}{\partial \mu} &= 1 \\
        \frac{\partial z}{\partial \sigma} &= \epsilon \\
        \frac{\partial L}{\partial \mu} &= \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial \mu} = \frac{\partial L}{\partial z} \\
        \frac{\partial L}{\partial \sigma} &= \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial \sigma} = \frac{\partial L}{\partial z} \cdot \epsilon
        \end{aligned}
        """)
        
        st.info("""
        **é‡å‚æ•°åŒ–æŠ€å·§çš„é‡è¦æ€§**ï¼š
        - **æ¢¯åº¦æµåŠ¨**ï¼šä½¿å¾—éšæœºå±‚çš„æ¢¯åº¦å¯ä»¥åå‘ä¼ æ’­
        - **GPUåŠ é€Ÿ**ï¼šå¯ä»¥åˆ©ç”¨å¹¶è¡Œè®¡ç®—åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹
        - **å˜åˆ†æ¨æ–­**ï¼šæ˜¯VAEã€è´å¶æ–¯ç¥ç»ç½‘ç»œç­‰æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯
        - **æ¡†æ¶æ”¯æŒ**ï¼šPyTorchã€TensorFlowç­‰ä¸»æµæ¡†æ¶éƒ½å†…ç½®æ”¯æŒ
        """)
    
    @staticmethod
    def _render_uncertainty_analysis():
        """ä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤º"""
        st.markdown("### ğŸ¯ ä¸ç¡®å®šæ€§åˆ†ç±»ï¼šè®¤çŸ¥ vs ä»»æ„")
        
        st.markdown("""
        **è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic)**ï¼š
        - æ¥æºï¼šæ¨¡å‹çŸ¥è¯†çš„ç›²åŒºï¼Œæ²¡è§è¿‡çš„æ•°æ®
        - ç‰¹ç‚¹ï¼šå¯é€šè¿‡å¢åŠ æ•°æ®æ¥æ¶ˆé™¤
        - å»ºæ¨¡ï¼šè´å¶æ–¯ç¥ç»ç½‘ç»œï¼Œæƒé‡åˆ†å¸ƒ
        
        **ä»»æ„ä¸ç¡®å®šæ€§ (Aleatoric)**ï¼š
        - æ¥æºï¼šæ•°æ®æœ¬èº«å›ºæœ‰çš„å™ªå£°
        - ç‰¹ç‚¹ï¼šæ— æ³•é€šè¿‡æ›´å¤šæ•°æ®æ¶ˆé™¤  
        - å»ºæ¨¡ï¼šè¾“å‡ºå±‚å¼•å…¥æ–¹å·®å‚æ•°
        """)
        
        with st.sidebar:
            data_type = st.selectbox("æ•°æ®åœºæ™¯", ["çº¿æ€§å›å½’", "éçº¿æ€§å›å½’", "åˆ†ç±»è¾¹ç•Œ"])
            noise_level = st.slider("æ•°æ®å™ªå£°æ°´å¹³", 0.0, 1.0, 0.2, 0.05)
            num_training_points = st.slider("è®­ç»ƒç‚¹æ•°é‡", 10, 100, 30, 5)
            show_prediction_uncertainty = st.checkbox("æ˜¾ç¤ºé¢„æµ‹ä¸ç¡®å®šæ€§", value=True)
        
        # ç”Ÿæˆåˆæˆæ•°æ®
        np.random.seed(42)
        
        if data_type == "çº¿æ€§å›å½’":
            # çº¿æ€§å›å½’æ•°æ®
            X_train = np.random.uniform(-5, 5, num_training_points)
            y_true = 2 * X_train + 1
            y_train = y_true + np.random.normal(0, noise_level, num_training_points)
            
            X_test = np.linspace(-8, 8, 100)
            y_test_true = 2 * X_test + 1
            
        elif data_type == "éçº¿æ€§å›å½’":
            # éçº¿æ€§å›å½’æ•°æ®
            X_train = np.random.uniform(-3, 3, num_training_points)
            y_true = np.sin(X_train) * 2
            y_train = y_true + np.random.normal(0, noise_level, num_training_points)
            
            X_test = np.linspace(-5, 5, 100)
            y_test_true = np.sin(X_test) * 2
            
        else:  # åˆ†ç±»è¾¹ç•Œ
            # äºŒåˆ†ç±»æ•°æ®
            X_train = np.random.randn(num_training_points, 2)
            # åˆ›å»ºéçº¿æ€§å†³ç­–è¾¹ç•Œ
            y_train = (X_train[:, 0]**2 + X_train[:, 1]**2 > 1).astype(int)
            # æ·»åŠ å™ªå£°æ ‡ç­¾
            flip_indices = np.random.choice(num_training_points, int(noise_level * num_training_points), replace=False)
            y_train[flip_indices] = 1 - y_train[flip_indices]
            
            X_test = np.random.uniform(-3, 3, (1000, 2))
        
        # æ¨¡æ‹Ÿè´å¶æ–¯ç¥ç»ç½‘ç»œé¢„æµ‹ (ä½¿ç”¨MC Dropoutè¿‘ä¼¼)
        def bayesian_nn_predict(X, num_samples=100):
            """æ¨¡æ‹Ÿè´å¶æ–¯ç¥ç»ç½‘ç»œçš„é¢„æµ‹åˆ†å¸ƒ"""
            if data_type == "åˆ†ç±»è¾¹ç•Œ":
                predictions = []
                for _ in range(num_samples):
                    # æ¨¡æ‹Ÿç½‘ç»œæƒé‡çš„ä¸ç¡®å®šæ€§
                    weight_noise = np.random.normal(0, 0.1, (2, 1))
                    bias_noise = np.random.normal(0, 0.1, 1)
                    
                    # ç®€å•çš„éçº¿æ€§å†³ç­–è¾¹ç•Œ
                    logits = X @ weight_noise + bias_noise
                    logits += np.sin(X[:, 0:1]) * np.cos(X[:, 1:2])  # éçº¿æ€§å˜æ¢
                    probs = 1 / (1 + np.exp(-logits))
                    predictions.append(probs.flatten())
                
                return np.array(predictions)
            else:
                predictions = []
                for _ in range(num_samples):
                    # æ¨¡æ‹Ÿæƒé‡ä¸ç¡®å®šæ€§
                    if data_type == "çº¿æ€§å›å½’":
                        weight = np.random.normal(2, 0.3)  # çœŸå®æƒé‡2ï¼Œæœ‰ä¸ç¡®å®šæ€§
                        bias = np.random.normal(1, 0.2)    # çœŸå®åç½®1ï¼Œæœ‰ä¸ç¡®å®šæ€§
                    else:  # éçº¿æ€§
                        weight = np.random.normal(1.5, 0.3)
                        bias = np.random.normal(0, 0.2)
                    
                    y_pred = weight * X + bias
                    # æ·»åŠ ä»»æ„ä¸ç¡®å®šæ€§ï¼ˆæ•°æ®å™ªå£°ï¼‰
                    y_pred += np.random.normal(0, noise_level, len(X))
                    
                    predictions.append(y_pred)
                
                return np.array(predictions)
        
        # è·å–é¢„æµ‹åˆ†å¸ƒ
        if data_type == "åˆ†ç±»è¾¹ç•Œ":
            predictions = bayesian_nn_predict(X_test)
            mean_pred = np.mean(predictions, axis=0)
            std_pred = np.std(predictions, axis=0)
            
            # å¯è§†åŒ–åˆ†ç±»è¾¹ç•Œå’Œä¸ç¡®å®šæ€§
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=["é¢„æµ‹å‡å€¼", "é¢„æµ‹ä¸ç¡®å®šæ€§"],
                specs=[[{"type": "scatter"}, {"type": "scatter"}]]
            )
            
            # è®­ç»ƒæ•°æ®
            colors_train = ['red' if y == 0 else 'blue' for y in y_train]
            fig.add_trace(
                go.Scatter(
                    x=X_train[:, 0], y=X_train[:, 1],
                    mode='markers',
                    name='è®­ç»ƒæ•°æ®',
                    marker=dict(color=colors_train, size=8)
                ),
                row=1, col=1
            )
            
            # é¢„æµ‹è¾¹ç•Œ
            fig.add_trace(
                go.Scatter(
                    x=X_test[:, 0], y=X_test[:, 1],
                    mode='markers',
                    name='é¢„æµ‹æ¦‚ç‡',
                    marker=dict(
                        color=mean_pred,
                        colorscale='RdBu',
                        size=4,
                        opacity=0.6,
                        colorbar=dict(title="æ­£ç±»æ¦‚ç‡", x=0.45)
                    ),
                    showlegend=False
                ),
                row=1, col=1
            )
            
            # ä¸ç¡®å®šæ€§å¯è§†åŒ–
            fig.add_trace(
                go.Scatter(
                    x=X_test[:, 0], y=X_test[:, 1],
                    mode='markers',
                    name='ä¸ç¡®å®šæ€§',
                    marker=dict(
                        color=std_pred,
                        colorscale='Viridis',
                        size=4,
                        opacity=0.6,
                        colorbar=dict(title="æ ‡å‡†å·®", x=1.02)
                    ),
                    showlegend=False
                ),
                row=1, col=2
            )
            
            fig.update_layout(
                title="è´å¶æ–¯åˆ†ç±»ï¼šä¸ç¡®å®šæ€§å¯è§†åŒ–",
                height=500
            )
            
        else:
            predictions = bayesian_nn_predict(X_test)
            mean_pred = np.mean(predictions, axis=0)
            std_pred = np.std(predictions, axis=0)
            
            # å¯è§†åŒ–å›å½’ç»“æœ
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=["é¢„æµ‹å‡å€¼ä¸ç½®ä¿¡åŒºé—´", "ä¸ç¡®å®šæ€§åˆ†è§£"]
            )
            
            # è®­ç»ƒæ•°æ®
            fig.add_trace(
                go.Scatter(
                    x=X_train, y=y_train,
                    mode='markers',
                    name='è®­ç»ƒæ•°æ®',
                    marker=dict(color='blue', size=8),
                    error_y=dict(
                        type='data',
                        array=noise_level * np.ones_like(y_train),
                        visible=True,
                        color='lightblue'
                    )
                ),
                row=1, col=1
            )
            
            # çœŸå®å‡½æ•°
            fig.add_trace(
                go.Scatter(
                    x=X_test, y=y_test_true,
                    mode='lines',
                    name='çœŸå®å‡½æ•°',
                    line=dict(color='black', width=2, dash='dash')
                ),
                row=1, col=1
            )
            
            # é¢„æµ‹å‡å€¼
            fig.add_trace(
                go.Scatter(
                    x=X_test, y=mean_pred,
                    mode='lines',
                    name='é¢„æµ‹å‡å€¼',
                    line=dict(color='red', width=2)
                ),
                row=1, col=1
            )
            
            # ç½®ä¿¡åŒºé—´
            if show_prediction_uncertainty:
                upper_bound = mean_pred + 2 * std_pred
                lower_bound = mean_pred - 2 * std_pred
                
                fig.add_trace(
                    go.Scatter(
                        x=X_test, y=upper_bound,
                        mode='lines',
                        line=dict(width=0),
                        showlegend=False
                    ),
                    row=1, col=1
                )
                
                fig.add_trace(
                    go.Scatter(
                        x=X_test, y=lower_bound,
                        mode='lines',
                        line=dict(width=0),
                        fill='tonexty',
                        fillcolor='rgba(255,0,0,0.2)',
                        name='95%ç½®ä¿¡åŒºé—´'
                    ),
                    row=1, col=1
                )
            
            # ä¸ç¡®å®šæ€§åˆ†è§£
            total_uncertainty = std_pred**2
            aleatoric_uncertainty = noise_level**2 * np.ones_like(total_uncertainty)
            epistemic_uncertainty = total_uncertainty - aleatoric_uncertainty
            
            fig.add_trace(
                go.Scatter(
                    x=X_test, y=epistemic_uncertainty,
                    mode='lines',
                    name='è®¤çŸ¥ä¸ç¡®å®šæ€§',
                    line=dict(color='green', width=2)
                ),
                row=1, col=2
            )
            
            fig.add_trace(
                go.Scatter(
                    x=X_test, y=aleatoric_uncertainty,
                    mode='lines',
                    name='ä»»æ„ä¸ç¡®å®šæ€§',
                    line=dict(color='orange', width=2)
                ),
                row=1, col=2
            )
            
            fig.add_trace(
                go.Scatter(
                    x=X_test, y=total_uncertainty,
                    mode='lines',
                    name='æ€»ä¸ç¡®å®šæ€§',
                    line=dict(color='purple', width=2, dash='dot')
                ),
                row=1, col=2
            )
            
            fig.update_layout(
                title="è´å¶æ–¯å›å½’ï¼šä¸ç¡®å®šæ€§é‡åŒ–",
                height=500
            )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ä¸ç¡®å®šæ€§åˆ†ææ€»ç»“
        st.markdown("### ğŸ“Š ä¸ç¡®å®šæ€§åˆ†ææ€»ç»“")
        
        if data_type != "åˆ†ç±»è¾¹ç•Œ":
            avg_epistemic = np.mean(epistemic_uncertainty)
            avg_aleatoric = np.mean(aleatoric_uncertainty)
            avg_total = np.mean(total_uncertainty)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å¹³å‡è®¤çŸ¥ä¸ç¡®å®šæ€§", f"{avg_epistemic:.4f}")
            with col2:
                st.metric("å¹³å‡ä»»æ„ä¸ç¡®å®šæ€§", f"{avg_aleatoric:.4f}")
            with col3:
                st.metric("å¹³å‡æ€»ä¸ç¡®å®šæ€§", f"{avg_total:.4f}")
        
        st.success("""
        **ä¸ç¡®å®šæ€§é‡åŒ–çš„å®è·µä»·å€¼**ï¼š
        - **è‡ªåŠ¨é©¾é©¶**ï¼šè¯†åˆ«æœªè§è¿‡çš„è·¯å†µï¼Œè§¦å‘å®‰å…¨æœºåˆ¶
        - **åŒ»ç–—è¯Šæ–­**ï¼šé‡åŒ–è¯Šæ–­ç½®ä¿¡åº¦ï¼Œè¾…åŠ©åŒ»ç”Ÿå†³ç­–
        - **é‡‘èé£æ§**ï¼šè¯„ä¼°æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ï¼Œæ§åˆ¶é£é™©
        - **ç§‘å­¦å‘ç°**ï¼šè¯†åˆ«çŸ¥è¯†ç›²åŒºï¼ŒæŒ‡å¯¼æ•°æ®æ”¶é›†
        """)


# ä¸ºäº†å…¼å®¹æ€§ï¼Œæ·»åŠ ç¼ºå°‘çš„å¯¼å…¥
try:
    from scipy.stats import norm, multivariate_normal
except ImportError:
    # å¦‚æœscipyä¸å¯ç”¨ï¼Œä½¿ç”¨numpyå®ç°
    def norm(loc=0, scale=1):
        class NormalDist:
            def pdf(self, x):
                return np.exp(-0.5 * ((x - loc) / scale)**2) / (scale * np.sqrt(2 * np.pi))
        return NormalDist()
    
    def multivariate_normal(mean, cov):
        class MVN:
            def rvs(self, size=1):
                return np.random.multivariate_normal(mean, cov, size)
        return MVN()

        # æ·»åŠ äº¤äº’å¼æµ‹éªŒ
